{"id": "2602.21268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21268", "abs": "https://arxiv.org/abs/2602.21268", "authors": ["Takaaki Fujita", "Florentin Smarandache"], "title": "A Dynamic Survey of Soft Set Theory and Its Extensions", "comment": "Book.143 pages. Publisher: Neutrosophic Science International Association (NSIA) Publishing House. ISBN: 978-1-59973-859-8", "summary": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.", "AI": {"tldr": "\u672c\u4e66\u5bf9\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u4e3b\u8981\u6269\u5c55\u8fdb\u884c\u4e86\u7efc\u8ff0\u6027\u6982\u8ff0\uff0c\u6db5\u76d6\u6838\u5fc3\u5b9a\u4e49\u3001\u4ee3\u8868\u6027\u6784\u9020\u548c\u5f53\u524d\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u8f6f\u96c6\u7406\u8bba\u4e3a\u53c2\u6570\u5316\u51b3\u7b56\u5efa\u6a21\u63d0\u4f9b\u4e86\u76f4\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5c5e\u6027\uff08\u53c2\u6570\uff09\u5206\u914d\u7ed9\u7ed9\u5b9a\u8bba\u57df\u7684\u5b50\u96c6\u6765\u7ed3\u6784\u5316\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u7406\u8bba\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u4e2d\u5df2\u6269\u5c55\u5230\u4f17\u591a\u53d8\u4f53\uff0c\u5e76\u4e0e\u62d3\u6251\u5b66\u3001\u62df\u9635\u7406\u8bba\u7b49\u591a\u4e2a\u9886\u57df\u5efa\u7acb\u4e86\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u6027\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u4ecb\u7ecd\u8f6f\u96c6\u53ca\u5176\u4e3b\u8981\u6269\u5c55\uff08\u5305\u62ec\u8d85\u8f6f\u96c6\u3001\u8d85\u8d85\u8f6f\u96c6\u3001\u6811\u8f6f\u96c6\u3001\u53cc\u6781\u8f6f\u96c6\u548c\u52a8\u6001\u8f6f\u96c6\u7b49\uff09\uff0c\u7a81\u51fa\u6838\u5fc3\u5b9a\u4e49\u3001\u4ee3\u8868\u6027\u6784\u9020\u548c\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u6269\u5c55\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5c55\u793a\u4e86\u8be5\u7406\u8bba\u7684\u53d1\u5c55\u8109\u7edc\u3001\u4e3b\u8981\u53d8\u4f53\u53ca\u5176\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u8054\u7cfb\u3002", "conclusion": "\u672c\u4e66\u7cfb\u7edf\u603b\u7ed3\u4e86\u8f6f\u96c6\u7406\u8bba\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u7ed3\u6784\u5316\u6982\u89c8\uff0c\u5e76\u6307\u660e\u4e86\u5f53\u524d\u7684\u53d1\u5c55\u65b9\u5411\u548c\u6f5c\u5728\u7814\u7a76\u673a\u4f1a\u3002"}}
{"id": "2602.21351", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21351", "abs": "https://arxiv.org/abs/2602.21351", "authors": ["Dmitrii Pantiukhin", "Ivan Kuznetsov", "Boris Shapkin", "Antonia Anna Jost", "Thomas Jung", "Nikolay Koldunov"], "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives", "comment": "20 pages, 6 figures, 7 tables, supplementary material included", "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.", "AI": {"tldr": "PANGAEA-GPT\uff1a\u7528\u4e8e\u5730\u7403\u79d1\u5b66\u6570\u636e\u81ea\u4e3b\u53d1\u73b0\u548c\u5206\u6790\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6", "motivation": "\u5730\u7403\u79d1\u5b66\u6570\u636e\u5feb\u901f\u79ef\u7d2f\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5927\u91cf\u6570\u636e\u96c6\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9650\u5236\u4e86\u6570\u636e\u91cd\u7528\u6027\u3002\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5305\u88c5\u5668\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5177\u6709\u96c6\u4e2d\u5f0f\u76d1\u7763\u8005-\u5de5\u4f5c\u8005\u62d3\u6251\u7ed3\u6784\uff0c\u5305\u542b\u6570\u636e\u7c7b\u578b\u611f\u77e5\u8def\u7531\u3001\u6c99\u76d2\u786e\u5b9a\u6027\u4ee3\u7801\u6267\u884c\u548c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u7684\u81ea\u6211\u6821\u6b63\u673a\u5236\u3002", "result": "\u901a\u8fc7\u7269\u7406\u6d77\u6d0b\u5b66\u548c\u751f\u6001\u5b66\u7528\u4f8b\u573a\u666f\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u591f\u4ee5\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u5bf9\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u7684\u67e5\u8be2\u548c\u5206\u6790\u3002", "conclusion": "PANGAEA-GPT\u4e3a\u901a\u8fc7\u534f\u8c03\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u67e5\u8be2\u548c\u5206\u6790\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u8bba\uff0c\u89e3\u51b3\u4e86\u5730\u7403\u79d1\u5b66\u6570\u636e\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2602.21496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21496", "abs": "https://arxiv.org/abs/2602.21496", "authors": ["Umid Suleymanov", "Zaur Rajabov", "Emil Mirzazada", "Murat Kantarcioglu"], "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information", "comment": "Under Review", "summary": "While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic \"Editor\" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.", "AI": {"tldr": "SemSIEdit\uff1a\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\"\u7f16\u8f91\u5668\"\u8fed\u4ee3\u6279\u8bc4\u548c\u91cd\u5199\u654f\u611f\u5185\u5bb9\u6765\u4fdd\u62a4\u8bed\u4e49\u654f\u611f\u4fe1\u606f\uff0c\u5728\u51cf\u5c1134.6%\u6cc4\u9732\u7684\u540c\u65f6\u4ec5\u635f\u59319.8%\u7684\u6548\u7528\uff0c\u63ed\u793a\u4e86\u9690\u79c1-\u6548\u7528\u5e15\u7d2f\u6258\u8fb9\u754c\u3001\u89c4\u6a21\u4f9d\u8d56\u7684\u5b89\u5168\u5206\u6b67\u548c\u63a8\u7406\u6096\u8bba\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e26\u6765\u4e86\u65b0\u7684\u5a01\u80c1\uff1a\u8bed\u4e49\u654f\u611f\u4fe1\u606f\uff08SemSI\uff09\uff0c\u5373\u6a21\u578b\u63a8\u65ad\u654f\u611f\u8eab\u4efd\u5c5e\u6027\u3001\u751f\u6210\u635f\u5bb3\u58f0\u8a89\u7684\u5185\u5bb9\u6216\u4ea7\u751f\u6f5c\u5728\u9519\u8bef\u4fe1\u606f\u3002LLMs\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\u81ea\u6211\u8c03\u8282\u8fd9\u4e9b\u590d\u6742\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u7684\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u79d1\u5b66\u95ee\u9898\u3002", "method": "\u63d0\u51faSemSIEdit\u6846\u67b6\uff0c\u91c7\u7528\u667a\u80fd\"\u7f16\u8f91\u5668\"\u5728\u63a8\u7406\u65f6\u8fed\u4ee3\u6279\u8bc4\u548c\u91cd\u5199\u654f\u611f\u7247\u6bb5\uff0c\u4ee5\u4fdd\u6301\u53d9\u4e8b\u6d41\u7545\u6027\u800c\u975e\u7b80\u5355\u62d2\u7edd\u56de\u7b54\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9690\u79c1-\u6548\u7528\u5e15\u7d2f\u6258\u8fb9\u754c\u5206\u6790\uff0c\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u5b89\u5168\u7b56\u7565\u5dee\u5f02\u3002", "result": "\u667a\u80fd\u91cd\u5199\u4f7f\u6240\u6709\u4e09\u7c7bSemSI\u7684\u6cc4\u9732\u51cf\u5c1134.6%\uff0c\u800c\u6548\u7528\u635f\u5931\u4ec5\u4e3a9.8%\u3002\u53d1\u73b0\u89c4\u6a21\u4f9d\u8d56\u7684\u5b89\u5168\u5206\u6b67\uff1a\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5efa\u8bbe\u6027\u6269\u5c55\uff08\u589e\u52a0\u7ec6\u5fae\u5dee\u522b\uff09\u5b9e\u73b0\u5b89\u5168\uff0c\u800c\u80fd\u529b\u53d7\u9650\u6a21\u578b\u5219\u56de\u5f52\u7834\u574f\u6027\u622a\u65ad\uff08\u5220\u9664\u6587\u672c\uff09\u3002\u63ed\u793a\u63a8\u7406\u6096\u8bba\uff1a\u63a8\u7406\u65f6\u63a8\u7406\u867d\u7136\u901a\u8fc7\u4f7f\u6a21\u578b\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u654f\u611f\u63a8\u65ad\u800c\u589e\u52a0\u57fa\u7ebf\u98ce\u9669\uff0c\u4f46\u540c\u65f6\u8d4b\u80fd\u9632\u5fa1\u6267\u884c\u5b89\u5168\u91cd\u5199\u3002", "conclusion": "SemSIEdit\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u4e2d\u7684\u8bed\u4e49\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9690\u79c1\u98ce\u9669\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u5bf9\u5b89\u5168\u7b56\u7565\u7684\u5f71\u54cd\u4ee5\u53ca\u63a8\u7406\u80fd\u529b\u5728\u9690\u79c1\u4fdd\u62a4\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\uff0c\u4e3aLLMs\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.21212", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21212", "abs": "https://arxiv.org/abs/2602.21212", "authors": ["Takato Yasuno"], "title": "Disaster Question Answering with LoRA Efficiency and Accurate End Position", "comment": "12 pages, 5 figures", "summary": "Natural disasters such as earthquakes, torrential rainfall, floods, and volcanic eruptions occur with extremely low frequency and affect limited geographic areas. When individuals face disaster situations, they often experience confusion and lack the domain-specific knowledge and experience necessary to determine appropriate responses and actions. While disaster information is continuously updated, even when utilizing RAG search and large language models for inquiries, obtaining relevant domain knowledge about natural disasters and experiences similar to one's specific situation is not guaranteed. When hallucinations are included in disaster question answering, artificial misinformation may spread and exacerbate confusion. This work introduces a disaster-focused question answering system based on Japanese disaster situations and response experiences. Utilizing the cl-tohoku/bert-base-japanese-v3 + Bi-LSTM + Enhanced Position Heads architecture with LoRA efficiency optimization, we achieved 70.4\\% End Position accuracy with only 5.7\\% of the total parameters (6.7M/117M). Experimental results demonstrate that the combination of Japanese BERT-base optimization and Bi-LSTM contextual understanding achieves accuracy levels suitable for real disaster response scenarios, attaining a 0.885 Span F1 score. Future challenges include: establishing natural disaster Q\\&A benchmark datasets, fine-tuning foundation models with disaster knowledge, developing lightweight and power-efficient edge AI Disaster Q\\&A applications for situations with insufficient power and communication during disasters, and addressing disaster knowledge base updates and continual learning capabilities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u65e5\u672c\u707e\u5bb3\u60c5\u5883\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316BERT-base\u65e5\u8bed\u6a21\u578b\u7ed3\u5408Bi-LSTM\u67b6\u6784\uff0c\u5728\u4ec5\u4f7f\u75285.7%\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e8670.4%\u7684\u7aef\u4f4d\u7f6e\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u707e\u5bb3\u54cd\u5e94\u573a\u666f\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u53d1\u751f\u9891\u7387\u4f4e\u4e14\u5f71\u54cd\u8303\u56f4\u6709\u9650\uff0c\u4e2a\u4eba\u9762\u5bf9\u707e\u5bb3\u65f6\u5f80\u5f80\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u548c\u7ecf\u9a8c\uff0c\u800c\u73b0\u6709RAG\u548cLLM\u65b9\u6cd5\u5728\u707e\u5bb3\u95ee\u7b54\u4e2d\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u5bfc\u81f4\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u707e\u5bb3\u60c5\u5883\u7684\u53ef\u9760\u95ee\u7b54\u7cfb\u7edf\u3002", "method": "\u91c7\u7528cl-tohoku/bert-base-japanese-v3 + Bi-LSTM + Enhanced Position Heads\u67b6\u6784\uff0c\u7ed3\u5408LoRA\u6548\u7387\u4f18\u5316\u6280\u672f\uff0c\u4ec5\u4f7f\u7528\u539f\u59cb\u6a21\u578b5.7%\u7684\u53c2\u6570\uff086.7M/117M\uff09\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u7aef\u4f4d\u7f6e\u51c6\u786e\u7387\u4e0a\u8fbe\u523070.4%\uff0cSpan F1\u5206\u6570\u4e3a0.885\uff0c\u8bc1\u660e\u4e86\u65e5\u8bedBERT-base\u4f18\u5316\u4e0eBi-LSTM\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u7ec4\u5408\u5728\u707e\u5bb3\u54cd\u5e94\u573a\u666f\u4e2d\u5177\u6709\u8db3\u591f\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u5b9e\u9645\u707e\u5bb3\u54cd\u5e94\u5e94\u7528\u3002\u672a\u6765\u6311\u6218\u5305\u62ec\uff1a\u5efa\u7acb\u81ea\u7136\u707e\u5bb3\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u3001\u7528\u707e\u5bb3\u77e5\u8bc6\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3001\u5f00\u53d1\u8f7b\u91cf\u7ea7\u8fb9\u7f18AI\u707e\u5bb3\u95ee\u7b54\u5e94\u7528\u3001\u89e3\u51b3\u707e\u5bb3\u77e5\u8bc6\u5e93\u66f4\u65b0\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u95ee\u9898\u3002"}}
{"id": "2602.21215", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21215", "abs": "https://arxiv.org/abs/2602.21215", "authors": ["Runyi Hu", "Jie Zhang", "Shiqian Zhao", "Jiale Meng", "Jiwei Li", "Jason Zeng", "Ming Wu", "Michael Heinrich", "Yonggang Wen", "Tianwei Zhang"], "title": "Inference-time Alignment via Sparse Junction Steering", "comment": "28 pages, 17 figures", "summary": "Token-level steering has emerged as a pivotal approach for inference-time alignment, enabling fine grained control over large language models by modulating their output distributions without parameter updates. While effective, existing methods rely on dense intervention at every decoding step. This persistent manipulation not only incurs substantial computational overhead but also risks compromising generation quality by excessively drifting from the model's intrinsic distribution. In this work, we show that dense intervention is unnecessary and propose Sparse Inference time Alignment (SIA), which performs sparse junction steering by intervening only at critical decision points along the generation trajectory. Our key insight is that high entropy junctions mark pivotal decision points in the generation trajectory and are particularly susceptible to misalignment, indicating the need to introduce alignment related reward signals at these points. Extensive experiments across different model families and alignment objectives show that steering only 20% to 80% of tokens achieves superior alignment-efficiency trade offs. For strong base models such as Qwen3, intervening on as few as 20% of tokens matches or even surpasses heavily post-trained instruct models. This sparsity enables stronger guidance while better preserving the model's native distribution, integrates seamlessly with search based methods such as Best-of-N, and reduces computational cost by up to 6x.", "AI": {"tldr": "SIA\uff08\u7a00\u758f\u63a8\u7406\u65f6\u5bf9\u9f50\uff09\u901a\u8fc7\u5728\u5173\u952e\u51b3\u7b56\u70b9\u800c\u975e\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u8fdb\u884c\u7a00\u758f\u5e72\u9884\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u4ec5\u9700\u5e72\u988420%-80%\u7684token\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u5bc6\u96c6\u5e72\u9884\u6548\u679c\u3002", "motivation": "\u73b0\u6709token\u7ea7\u5f15\u5bfc\u65b9\u6cd5\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u8fdb\u884c\u5bc6\u96c6\u5e72\u9884\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u53ef\u80fd\u8fc7\u5ea6\u504f\u79bb\u6a21\u578b\u56fa\u6709\u5206\u5e03\uff0c\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7a00\u758f\u5e72\u9884\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSIA\u65b9\u6cd5\uff0c\u4ec5\u5728\u9ad8\u71b5\u51b3\u7b56\u70b9\uff08\u5173\u952e\u751f\u6210\u8f68\u8ff9\u8282\u70b9\uff09\u8fdb\u884c\u7a00\u758f\u5e72\u9884\uff0c\u8fd9\u4e9b\u70b9\u5bf9\u9519\u4f4d\u7279\u522b\u654f\u611f\uff0c\u9700\u8981\u5f15\u5165\u5bf9\u9f50\u76f8\u5173\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u5e72\u988420%-80%\u7684token\u5373\u53ef\u5b9e\u73b0\u4f18\u8d8a\u7684\u5bf9\u9f50-\u6548\u7387\u6743\u8861\u3002\u5bf9\u4e8eQwen3\u7b49\u5f3a\u57fa\u7840\u6a21\u578b\uff0c\u4ec5\u5e72\u988420%\u7684token\u5373\u53ef\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u7ecf\u8fc7\u5927\u91cf\u540e\u8bad\u7ec3\u7684\u6307\u4ee4\u6a21\u578b\u3002\u7a00\u758f\u6027\u4f7f\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe6\u500d\u3002", "conclusion": "\u5bc6\u96c6\u5e72\u9884\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u7a00\u758f\u5e72\u9884\u5728\u5173\u952e\u51b3\u7b56\u70b9\u8fdb\u884c\u5f15\u5bfc\u4e0d\u4ec5\u80fd\u4fdd\u6301\u6a21\u578b\u56fa\u6709\u5206\u5e03\uff0c\u8fd8\u80fd\u4e0eBest-of-N\u7b49\u641c\u7d22\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u3002"}}
{"id": "2602.21216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21216", "abs": "https://arxiv.org/abs/2602.21216", "authors": ["Zhyar Rzgar K Rostam", "G\u00e1bor Kert\u00e9sz"], "title": "EQ-5D Classification Using Biomedical Entity-Enriched Pre-trained Language Models and Multiple Instance Learning", "comment": "12 tables", "summary": "The EQ-5D (EuroQol 5-Dimensions) is a standardized instrument for the evaluation of health-related quality of life. In health economics, systematic literature reviews (SLRs) depend on the correct identification of publications that use the EQ-5D, but manual screening of large volumes of scientific literature is time-consuming, error-prone, and inconsistent. In this study, we investigate fine-tuning of general-purpose (BERT) and domain-specific (SciBERT, BioBERT) pre-trained language models (PLMs), enriched with biomedical entity information extracted through scispaCy models for each statement, to improve EQ-5D detection from abstracts. We conduct nine experimental setups, including combining three scispaCy models with three PLMs, and evaluate their performance at both the sentence and study levels. Furthermore, we explore a Multiple Instance Learning (MIL) approach with attention pooling to aggregate sentence-level information into study-level predictions, where each abstract is represented as a bag of enriched sentences (by scispaCy). The findings indicate consistent improvements in F1-scores (reaching 0.82) and nearly perfect recall at the study-level, significantly exceeding classical bag-of-words baselines and recently reported PLM baselines. These results show that entity enrichment significantly improves domain adaptation and model generalization, enabling more accurate automated screening in systematic reviews.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5e76\u7ed3\u5408\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u4fe1\u606f\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u68c0\u6d4bEQ-5D\u76f8\u5173\u6587\u732e\u7684\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u6548\u7387\u3002", "motivation": "\u5728\u5065\u5eb7\u7ecf\u6d4e\u5b66\u4e2d\uff0c\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u4f9d\u8d56\u51c6\u786e\u8bc6\u522b\u4f7f\u7528EQ-5D\u7684\u6587\u732e\uff0c\u4f46\u4eba\u5de5\u7b5b\u9009\u5927\u91cf\u6587\u732e\u8017\u65f6\u3001\u6613\u9519\u4e14\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5fae\u8c03\u901a\u7528\uff08BERT\uff09\u548c\u9886\u57df\u4e13\u7528\uff08SciBERT\u3001BioBERT\uff09\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408scispaCy\u63d0\u53d6\u7684\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u4fe1\u606f\uff0c\u8bbe\u8ba1\u4e869\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u6c60\u5316\u5c06\u53e5\u5b50\u7ea7\u4fe1\u606f\u805a\u5408\u4e3a\u7814\u7a76\u7ea7\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aF1\u5206\u6570\u8fbe\u52300.82\uff0c\u7814\u7a76\u7ea7\u53ec\u56de\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u663e\u8457\u8d85\u8fc7\u4f20\u7edf\u8bcd\u888b\u6a21\u578b\u57fa\u51c6\u548c\u8fd1\u671f\u62a5\u9053\u7684PLM\u57fa\u51c6\uff0c\u5b9e\u4f53\u589e\u5f3a\u663e\u8457\u6539\u5584\u4e86\u9886\u57df\u9002\u5e94\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5b9e\u4f53\u589e\u5f3a\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347EQ-5D\u6587\u732e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u81ea\u52a8\u5316\u7b5b\u9009\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u7814\u7a76\u6548\u7387\u3002"}}
{"id": "2602.21553", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21553", "abs": "https://arxiv.org/abs/2602.21553", "authors": ["Wenqing Zheng", "Dmitri Kalaev", "Noah Fatsi", "Daniel Barcklow", "Owen Reinert", "Igor Melnyk", "Senthil Kumar", "C. Bayan Bruss"], "title": "Revisiting RAG Retrievers: An Information Theoretic Benchmark", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely critically on the retriever module to surface relevant context for large language models. Although numerous retrievers have recently been proposed, each built on different ranking principles such as lexical matching, dense embeddings, or graph citations, there remains a lack of systematic understanding of how these mechanisms differ and overlap. Existing benchmarks primarily compare entire RAG pipelines or introduce new datasets, providing little guidance on selecting or combining retrievers themselves. Those that do compare retrievers directly use a limited set of evaluation tools which fail to capture complementary and overlapping strengths. This work presents MIGRASCOPE, a Mutual Information based RAG Retriever Analysis Scope. We revisit state-of-the-art retrievers and introduce principled metrics grounded in information and statistical estimation theory to quantify retrieval quality, redundancy, synergy, and marginal contribution. We further show that if chosen carefully, an ensemble of retrievers outperforms any single retriever. We leverage the developed tools over major RAG corpora to provide unique insights on contribution levels of the state-of-the-art retrievers. Our findings provide a fresh perspective on the structure of modern retrieval techniques and actionable guidance for designing robust and efficient RAG systems.", "AI": {"tldr": "MIGRASCOPE\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684RAG\u68c0\u7d22\u5668\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u4f30\u8ba1\u7406\u8bba\u91cf\u5316\u68c0\u7d22\u8d28\u91cf\u3001\u5197\u4f59\u6027\u3001\u534f\u540c\u6548\u5e94\u548c\u8fb9\u9645\u8d21\u732e\uff0c\u53d1\u73b0\u7cbe\u5fc3\u9009\u62e9\u7684\u68c0\u7d22\u5668\u96c6\u6210\u4f18\u4e8e\u4efb\u4f55\u5355\u4e00\u68c0\u7d22\u5668\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u4f9d\u8d56\u68c0\u7d22\u5668\u63d0\u4f9b\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u4f46\u5404\u79cd\u68c0\u7d22\u5668\u57fa\u4e8e\u4e0d\u540c\u6392\u5e8f\u539f\u7406\uff08\u8bcd\u6c47\u5339\u914d\u3001\u5bc6\u96c6\u5d4c\u5165\u3001\u56fe\u5f15\u7528\u7b49\uff09\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3\u8fd9\u4e9b\u673a\u5236\u5dee\u5f02\u548c\u91cd\u53e0\u7684\u6846\u67b6\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u6bd4\u8f83\u6574\u4e2aRAG\u6d41\u7a0b\u6216\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u5bf9\u68c0\u7d22\u5668\u672c\u8eab\u7684\u9009\u62e9\u548c\u7ec4\u5408\u63d0\u4f9b\u6307\u5bfc\u6709\u9650\u3002", "method": "\u63d0\u51faMIGRASCOPE\u6846\u67b6\uff0c\u57fa\u4e8e\u4e92\u4fe1\u606f\u548c\u7edf\u8ba1\u4f30\u8ba1\u7406\u8bba\u5efa\u7acb\u539f\u5219\u6027\u6307\u6807\uff0c\u91cf\u5316\u68c0\u7d22\u8d28\u91cf\u3001\u5197\u4f59\u6027\u3001\u534f\u540c\u6548\u5e94\u548c\u8fb9\u9645\u8d21\u732e\u3002\u91cd\u65b0\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u5668\uff0c\u5e76\u5728\u4e3b\u8981RAG\u8bed\u6599\u5e93\u4e0a\u5e94\u7528\u8fd9\u4e9b\u5de5\u5177\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5982\u679c\u7cbe\u5fc3\u9009\u62e9\uff0c\u68c0\u7d22\u5668\u96c6\u6210\u53ef\u4ee5\u8d85\u8d8a\u4efb\u4f55\u5355\u4e00\u68c0\u7d22\u5668\u3002\u901a\u8fc7\u5f00\u53d1\u7684\u5de5\u5177\u5bf9\u4e3b\u8981RAG\u8bed\u6599\u5e93\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6700\u5148\u8fdb\u68c0\u7d22\u5668\u8d21\u732e\u6c34\u5e73\u7684\u72ec\u7279\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u73b0\u4ee3\u68c0\u7d22\u6280\u672f\u7684\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u7a33\u5065\u9ad8\u6548\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002MIGRASCOPE\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u7406\u89e3\u4e0d\u540c\u68c0\u7d22\u673a\u5236\u7684\u5dee\u5f02\u548c\u91cd\u53e0\uff0c\u5e2e\u52a9\u4f18\u5316\u68c0\u7d22\u5668\u9009\u62e9\u548c\u7ec4\u5408\u3002"}}
{"id": "2602.21745", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21745", "abs": "https://arxiv.org/abs/2602.21745", "authors": ["Hyo Jin Kim"], "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems", "comment": "13 pages, 5 figures. Version 1. Includes recursive feedback extension and simulation results. Data available via DOI: 10.5281/zenodo.18754266", "summary": "We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.\n  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.\n  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.", "AI": {"tldr": "ASIR\u52c7\u6c14\u6a21\u578b\u662f\u4e00\u4e2a\u76f8\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5c06\u771f\u76f8\u62ab\u9732\u5f62\u5f0f\u5316\u4e3a\u72b6\u6001\u8f6c\u79fb\u800c\u975e\u4eba\u683c\u7279\u8d28\uff0c\u9002\u7528\u4e8e\u4eba\u7c7b\u548cAI\u7cfb\u7edf\u5728\u98ce\u9669\u4e0b\u7684\u771f\u5b9e\u8868\u8fbe", "motivation": "\u4f20\u7edf\u4e0a\u5c06\u52c7\u6c14\u89c6\u4e3a\u4eba\u683c\u7279\u8d28\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u771f\u76f8\u62ab\u9732\u5e94\u88ab\u7406\u89e3\u4e3a\u5728\u6291\u5236\u548c\u8868\u8fbe\u72b6\u6001\u4e4b\u95f4\u7684\u76f8\u53d8\u8fc7\u7a0b\u3002\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u91ca\u4eba\u7c7b\u5728\u4e0d\u5bf9\u79f0\u98ce\u9669\u4e0b\u7684\u6c89\u9ed8\u548cAI\u7cfb\u7edf\u5728\u653f\u7b56\u7ea6\u675f\u4e0b\u7684\u8f93\u51fa\u5931\u771f\u73b0\u8c61", "method": "\u63d0\u51faASIR\u52c7\u6c14\u6a21\u578b\uff0c\u5c06\u771f\u76f8\u62ab\u9732\u5f62\u5f0f\u5316\u4e3a\u4ece\u6291\u5236\u72b6\u6001(S0)\u5230\u8868\u8fbe\u72b6\u6001(S1)\u7684\u76f8\u53d8\u8fc7\u7a0b\u3002\u4f7f\u7528\u4e0d\u7b49\u5f0flambda(1+gamma)+psi > theta+phi\u63cf\u8ff0\u72b6\u6001\u8f6c\u79fb\u6761\u4ef6\uff0c\u5176\u4e2dlambda\u4e3a\u57fa\u7ebf\u5f00\u653e\u5ea6\uff0cgamma\u4e3a\u5173\u7cfb\u653e\u5927\u56e0\u5b50\uff0cpsi\u4e3a\u7d2f\u79ef\u5185\u90e8\u538b\u529b\uff0ctheta\u548cphi\u4e3a\u8f6c\u79fb\u6210\u672c\u3002\u6a21\u578b\u5305\u542b\u53cd\u9988\u6269\u5c55\uff0c\u6a21\u62df\u8f6c\u79fb\u7ed3\u679c\u5982\u4f55\u9012\u5f52\u5730\u91cd\u65b0\u6821\u51c6\u7cfb\u7edf\u53c2\u6570", "result": "\u8be5\u6a21\u578b\u6210\u529f\u5c06\u4eba\u7c7b\u771f\u76f8\u8bb2\u8ff0\u548cAI\u7cfb\u7edf\u8f93\u51fa\u7ea6\u675f\u7edf\u4e00\u5230\u76f8\u540c\u7684\u76f8\u52a8\u529b\u5b66\u67b6\u6784\u4e2d\u3002\u5728AI\u80cc\u666f\u4e0b\uff0c\u6291\u5236\u5bf9\u5e94\u7ea6\u675f\u8f93\u51fa\u72b6\u6001\uff0c\u7ed3\u6784\u538b\u529b\u6765\u81ea\u7ade\u4e89\u76ee\u6807\u3001\u4e0a\u4e0b\u6587\u7d27\u5f20\u548c\u9012\u5f52\u4ea4\u4e92\u52a8\u529b\u5b66\u3002\u6a21\u578b\u5c06\u52c7\u6c14\u548c\u5bf9\u9f50\u91cd\u65b0\u6846\u67b6\u4e3a\u7ea6\u675f\u76f8\u7a7a\u95f4\u5185\u76f8\u4e92\u4f5c\u7528\u529b\u7684\u51e0\u4f55\u7ed3\u679c", "conclusion": "ASIR\u52c7\u6c14\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u89c6\u89d2\uff0c\u7528\u4e8e\u7406\u89e3\u4eba\u7c7b\u548c\u4eba\u5de5\u7cfb\u7edf\u5728\u98ce\u9669\u4e0b\u7684\u771f\u76f8\u62ab\u9732\u3002\u901a\u8fc7\u5c06\u52c7\u6c14\u548c\u5bf9\u9f50\u91cd\u65b0\u6846\u67b6\u4e3a\u5171\u4eab\u52a8\u529b\u5b66\u7ed3\u6784\uff0c\u6a21\u578b\u63ed\u793a\u4e86\u771f\u76f8\u62ab\u9732\u4f5c\u4e3a\u72b6\u6001\u8f6c\u79fb\u800c\u975e\u4eba\u683c\u7279\u8d28\u7684\u672c\u8d28\uff0c\u4e3a\u8de8\u4eba\u7c7b\u548cAI\u7cfb\u7edf\u7684\u771f\u5b9e\u8868\u8fbe\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7ed3\u6784\u6027\u89e3\u91ca"}}
{"id": "2602.21217", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21217", "abs": "https://arxiv.org/abs/2602.21217", "authors": ["S M Ruhul Alam", "Rifa Ferzana"], "title": "Applied Sociolinguistic AI for Community Development (ASA-CD): A New Scientific Paradigm for Linguistically-Grounded Social Intervention", "comment": "13 pages, 2 figures, 3 tables; simulation-based study introducing the ASA-CD framework", "summary": "This paper establishes Applied Sociolinguistic AI for Community Development (ASA-CD) as a novel scientific paradigm for addressing community challenges through linguistically grounded, AI-enabled intervention. ASA-CD introduces three key contributions: (1) linguistic biomarkers as computational indicators of discursive fragmentation; (2) development-aligned natural language processing (NLP), an AI optimisation paradigm prioritising collective outcomes; and (3) a standardised five-phase protocol for discursive intervention. A proof-of-concept study, incorporating real-world and synthetic corpora, demonstrates systematic associations between exclusionary language and negative sentiment and simulates intervention-based improvements. ASA-CD provides a unified methodological, ethical and empirical framework for scalable, value-aligned AI in the service of community empowerment.", "AI": {"tldr": "\u63d0\u51fa\u5e94\u7528\u793e\u4f1a\u8bed\u8a00\u5b66AI\u7528\u4e8e\u793e\u533a\u53d1\u5c55\uff08ASA-CD\uff09\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u8bed\u8a00\u57fa\u7840\u7684AI\u5e72\u9884\u89e3\u51b3\u793e\u533a\u95ee\u9898\uff0c\u5305\u542b\u8bed\u8a00\u751f\u7269\u6807\u5fd7\u7269\u3001\u53d1\u5c55\u5bf9\u9f50NLP\u548c\u4e94\u9636\u6bb5\u5e72\u9884\u534f\u8bae", "motivation": "\u4e3a\u89e3\u51b3\u793e\u533a\u6311\u6218\u63d0\u4f9b\u4e00\u79cd\u8bed\u8a00\u57fa\u7840\u3001AI\u9a71\u52a8\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u5efa\u7acb\u53ef\u6269\u5c55\u3001\u4ef7\u503c\u5bf9\u9f50\u7684AI\u6846\u67b6\uff0c\u670d\u52a1\u4e8e\u793e\u533a\u8d4b\u6743", "method": "\u63d0\u51faASA-CD\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a1\uff09\u8bed\u8a00\u751f\u7269\u6807\u5fd7\u7269\u4f5c\u4e3a\u8bdd\u8bed\u788e\u7247\u5316\u7684\u8ba1\u7b97\u6307\u6807\uff1b2\uff09\u53d1\u5c55\u5bf9\u9f50\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f18\u5148\u96c6\u4f53\u7ed3\u679c\u7684AI\u4f18\u5316\u8303\u5f0f\uff1b3\uff09\u6807\u51c6\u5316\u7684\u4e94\u9636\u6bb5\u8bdd\u8bed\u5e72\u9884\u534f\u8bae\u3002\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\uff0c\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u8bed\u6599\u5e93\u8fdb\u884c\u9a8c\u8bc1", "result": "\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6392\u65a5\u6027\u8bed\u8a00\u4e0e\u8d1f\u9762\u60c5\u611f\u5b58\u5728\u7cfb\u7edf\u6027\u5173\u8054\uff0c\u5e76\u6a21\u62df\u4e86\u57fa\u4e8e\u5e72\u9884\u7684\u6539\u8fdb\u6548\u679c", "conclusion": "ASA-CD\u4e3a\u53ef\u6269\u5c55\u3001\u4ef7\u503c\u5bf9\u9f50\u7684AI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u65b9\u6cd5\u8bba\u3001\u4f26\u7406\u548c\u5b9e\u8bc1\u6846\u67b6\uff0c\u670d\u52a1\u4e8e\u793e\u533a\u8d4b\u6743"}}
{"id": "2602.21598", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21598", "abs": "https://arxiv.org/abs/2602.21598", "authors": ["Touseef Hasan", "Laila Cure", "Souvika Sarkar"], "title": "Retrieval Challenges in Low-Resource Public Service Information: A Case Study on Food Pantry Access", "comment": "3 pages, 1 figure", "summary": "Public service information systems are often fragmented, inconsistently formatted, and outdated. These characteristics create low-resource retrieval environments that hinder timely access to critical services. We investigate retrieval challenges in such settings through the domain of food pantry access, a socially urgent problem given persistent food insecurity. We develop an AI-powered conversational retrieval system that scrapes and indexes publicly available pantry data and employs a Retrieval-Augmented Generation (RAG) pipeline to support natural language queries via a web interface. We conduct a pilot evaluation study using community-sourced queries to examine system behavior in realistic scenarios. Our analysis reveals key limitations in retrieval robustness, handling underspecified queries, and grounding over inconsistent knowledge bases. This ongoing work exposes fundamental IR challenges in low-resource environments and motivates future research on robust conversational retrieval to improve access to critical public resources.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eRAG\u7684AI\u5bf9\u8bdd\u68c0\u7d22\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u98df\u54c1\u50a8\u85cf\u5ba4\u4fe1\u606f\u68c0\u7d22\uff0c\u901a\u8fc7\u8bd5\u70b9\u8bc4\u4f30\u63ed\u793a\u68c0\u7d22\u9c81\u68d2\u6027\u3001\u67e5\u8be2\u4e0d\u660e\u786e\u5904\u7406\u548c\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\u57fa\u7840\u5316\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u516c\u5171\u670d\u52a1\u4fe1\u606f\u7cfb\u7edf\u901a\u5e38\u788e\u7247\u5316\u3001\u683c\u5f0f\u4e0d\u4e00\u81f4\u4e14\u8fc7\u65f6\uff0c\u8fd9\u4e9b\u7279\u5f81\u521b\u5efa\u4e86\u4f4e\u8d44\u6e90\u68c0\u7d22\u73af\u5883\uff0c\u963b\u788d\u4e86\u5bf9\u5173\u952e\u670d\u52a1\u7684\u53ca\u65f6\u8bbf\u95ee\u3002\u7814\u7a76\u901a\u8fc7\u98df\u54c1\u50a8\u85cf\u5ba4\u8bbf\u95ee\u8fd9\u4e00\u793e\u4f1a\u7d27\u8feb\u95ee\u9898\u6765\u8c03\u67e5\u6b64\u7c7b\u73af\u5883\u4e2d\u7684\u68c0\u7d22\u6311\u6218\u3002", "method": "\u5f00\u53d1AI\u9a71\u52a8\u7684\u5bf9\u8bdd\u68c0\u7d22\u7cfb\u7edf\uff0c\u6293\u53d6\u5e76\u7d22\u5f15\u516c\u5f00\u53ef\u7528\u7684\u50a8\u85cf\u5ba4\u6570\u636e\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ba1\u9053\u652f\u6301\u901a\u8fc7Web\u754c\u9762\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002\u4f7f\u7528\u793e\u533a\u6765\u6e90\u7684\u67e5\u8be2\u8fdb\u884c\u8bd5\u70b9\u8bc4\u4f30\u7814\u7a76\uff0c\u4ee5\u68c0\u67e5\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u68c0\u7d22\u9c81\u68d2\u6027\u3001\u5904\u7406\u4e0d\u660e\u786e\u67e5\u8be2\u4ee5\u53ca\u5728\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\u57fa\u7840\u4e0a\u8fdb\u884c\u57fa\u7840\u5316\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\u3002\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u68c0\u7d22\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u6b63\u5728\u8fdb\u884c\u7684\u5de5\u4f5c\u63ed\u793a\u4e86\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u57fa\u672c\u4fe1\u606f\u68c0\u7d22\u6311\u6218\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u7814\u7a76\u9c81\u68d2\u7684\u5bf9\u8bdd\u68c0\u7d22\u7cfb\u7edf\uff0c\u4ee5\u6539\u5584\u5bf9\u5173\u952e\u516c\u5171\u8d44\u6e90\u7684\u8bbf\u95ee\u3002"}}
{"id": "2602.21746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21746", "abs": "https://arxiv.org/abs/2602.21746", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation", "comment": null, "summary": "In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.", "AI": {"tldr": "fEDM+\u6269\u5c55\u4e86\u539f\u6709\u7684\u6a21\u7cca\u4f26\u7406\u51b3\u7b56\u6846\u67b6\uff0c\u589e\u52a0\u4e86\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u548c\u591a\u5143\u8bed\u4e49\u9a8c\u8bc1\uff0c\u4ee5\u89e3\u51b3\u539f\u6a21\u578b\u5728\u539f\u5219\u6027\u89e3\u91ca\u548c\u4f26\u7406\u591a\u5143\u4e3b\u4e49\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u539ffEDM\u6846\u67b6\u867d\u7136\u4fdd\u8bc1\u4e86\u5f62\u5f0f\u6b63\u786e\u6027\u548c\u51b3\u7b56\u4e00\u81f4\u6027\uff0c\u4f46\u672a\u80fd\u5145\u5206\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u51b3\u7b56\u7684\u539f\u5219\u6027\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u53ca\u5728\u4f26\u7406\u591a\u5143\u4e3b\u4e49\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u9700\u8981\u589e\u5f3a\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u5bf9\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u4ef7\u503c\u89c2\u7684\u9002\u5e94\u6027\u3002", "method": "1. \u5f15\u5165\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u6a21\u5757(ETM)\uff0c\u5c06\u4f26\u7406\u51b3\u7b56\u89c4\u5219\u4e0e\u5e95\u5c42\u9053\u5fb7\u539f\u5219\u660e\u786e\u5173\u8054\uff0c\u4e3a\u6bcf\u4e2a\u63a8\u8350\u884c\u52a8\u8ba1\u7b97\u52a0\u6743\u539f\u5219\u8d21\u732e\u5ea6\uff1b2. \u7528\u591a\u5143\u8bed\u4e49\u9a8c\u8bc1\u6846\u67b6\u66ff\u4ee3\u5355\u53c2\u7167\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u51b3\u7b56\u5bf9\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u53c2\u7167\u7684\u7b26\u5408\u7a0b\u5ea6\uff0c\u6bcf\u4e2a\u53c2\u7167\u7f16\u7801\u4e0d\u540c\u7684\u539f\u5219\u4f18\u5148\u7ea7\u548c\u98ce\u9669\u5bb9\u5fcd\u5ea6\u3002", "result": "\u6269\u5c55\u540e\u7684fEDM+\u4fdd\u7559\u4e86\u5f62\u5f0f\u53ef\u9a8c\u8bc1\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5229\u76ca\u76f8\u5173\u8005\u611f\u77e5\u7684\u9a8c\u8bc1\u3002\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u7684\u89e3\u91ca\uff0c\u5c55\u793a\u51b3\u7b56\u4f9d\u636e\u548c\u539f\u5219\u57fa\u7840\uff0c\u5e76\u80fd\u6b63\u5f0f\u8868\u793a\u539f\u5219\u6027\u5206\u6b67\u800c\u975e\u538b\u5236\u5b83\u4eec\u3002", "conclusion": "fEDM+\u901a\u8fc7\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u591a\u5143\u9a8c\u8bc1\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4f5c\u4e3a\u4f26\u7406\u654f\u611fAI\u7cfb\u7edf\u7684\u76d1\u7763\u548c\u6cbb\u7406\u5c42\uff0c\u5728\u4fdd\u6301\u5f62\u5f0f\u4e25\u8c28\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u3002"}}
{"id": "2602.21218", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21218", "abs": "https://arxiv.org/abs/2602.21218", "authors": ["Amin Banayeeanzade", "Qingchuan Yang", "Deqing Fu", "Spencer Hong", "Erin Babinsky", "Alfy Samuel", "Anoop Kumar", "Robin Jia", "Sai Praneeth Karimireddy"], "title": "EPSVec: Efficient and Private Synthetic Data Generation via Dataset Vectors", "comment": null, "summary": "High-quality data is essential for modern machine learning, yet many valuable corpora are sensitive and cannot be freely shared. Synthetic data offers a practical substitute for downstream development, and large language models (LLMs) have emerged as powerful engines for generating it. However, existing private text generation methods are severely inefficient: they are data-intensive, computationally slow, and often require large private corpora or batch sizes to achieve usable quality. We introduce EPSVec, a differentially-private lightweight alternative that steers LLM generation using *dataset vectors*--directions in activation space that capture the distributional gap between private data and public priors. EPSVec extracts and sanitizes steering vectors just once and then performs standard decoding. This decouples the privacy budget from generation, enabling arbitrarily many synthetic samples without additional privacy cost and yielding strong fidelity even in low-data regimes. Furthermore, we enhance our method by utilizing pretrained (base) models and introducing fixed-shot prompting to boost generation diversity and fidelity. Our experiments demonstrate that EPSVec outperforms existing baselines in distributional alignment and downstream utility, particularly in low-data regimes, while significantly reducing computational overhead.", "AI": {"tldr": "EPSVec\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u5dee\u5206\u9690\u79c1\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u5411\u91cf\u5f15\u5bfcLLM\u751f\u6210\uff0c\u5b9e\u73b0\u9690\u79c1\u9884\u7b97\u4e0e\u751f\u6210\u8fc7\u7a0b\u7684\u89e3\u8026\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u751f\u6210\u3002", "motivation": "\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\u673a\u5668\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u56e0\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u5171\u4eab\u3002\u73b0\u6709\u9690\u79c1\u6587\u672c\u751f\u6210\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff1a\u9700\u8981\u5927\u91cf\u6570\u636e\u3001\u8ba1\u7b97\u7f13\u6162\uff0c\u4e14\u901a\u5e38\u9700\u8981\u5927\u6279\u91cf\u624d\u80fd\u8fbe\u5230\u53ef\u7528\u8d28\u91cf\u3002", "method": "EPSVec\u901a\u8fc7\u63d0\u53d6\u548c\u51c0\u5316\"\u6570\u636e\u96c6\u5411\u91cf\"\uff08\u6355\u6349\u79c1\u6709\u6570\u636e\u4e0e\u516c\u5171\u5148\u9a8c\u4e4b\u95f4\u5206\u5e03\u5dee\u5f02\u7684\u6fc0\u6d3b\u7a7a\u95f4\u65b9\u5411\uff09\u6765\u5f15\u5bfcLLM\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u4e00\u6b21\u6027\u63d0\u53d6\u548c\u51c0\u5316\u5f15\u5bfc\u5411\u91cf\uff0c\u7136\u540e\u6267\u884c\u6807\u51c6\u89e3\u7801\uff0c\u5c06\u9690\u79c1\u9884\u7b97\u4e0e\u751f\u6210\u8fc7\u7a0b\u89e3\u8026\u3002\u8fd8\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u548c\u56fa\u5b9a\u6837\u672c\u63d0\u793a\u6765\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEPSVec\u5728\u5206\u5e03\u5bf9\u9f50\u548c\u4e0b\u6e38\u4efb\u52a1\u6548\u7528\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "EPSVec\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u5dee\u5206\u9690\u79c1\u6587\u672c\u751f\u6210\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u573a\u666f\uff0c\u4e14\u652f\u6301\u65e0\u9650\u751f\u6210\u800c\u4e0d\u589e\u52a0\u9690\u79c1\u6210\u672c\u3002"}}
{"id": "2602.21600", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21600", "abs": "https://arxiv.org/abs/2602.21600", "authors": ["Ganap Ashit Tewary", "Nrusinga Charan Gantayat", "Jeff Zhang"], "title": "AQR-HNSW: Accelerating Approximate Nearest Neighbor Search via Density-aware Quantization and Multi-stage Re-ranking", "comment": "Accepted at DAC 2026", "summary": "Approximate Nearest Neighbor (ANN) search has become fundamental to modern AI infrastructure, powering recommendation systems, search engines, and large language models across industry leaders from Google to OpenAI. Hierarchical Navigable Small World (HNSW) graphs have emerged as the dominant ANN algorithm, widely adopted in production systems due to their superior recall versus latency balance. However, as vector databases scale to billions of embeddings, HNSW faces critical bottlenecks: memory consumption expands, distance computation overhead dominates query latency, and it suffers suboptimal performance on heterogeneous data distributions. This paper presents Adaptive Quantization and Rerank HNSW (AQR-HNSW), a novel framework that synergistically integrates three strategies to enhance HNSW scalability. AQR-HNSW introduces (1) density-aware adaptive quantization, achieving 4x compression while preserving distance relationships; (2) multi-state re-ranking that reduces unnecessary computations by 35%; and (3) quantization-optimized SIMD implementations delivering 16-64 operations per cycle across architectures. Evaluation on standard benchmarks demonstrates 2.5-3.3x higher queries per second (QPS) than state-of-the-art HNSW implementations while maintaining over 98% recall, with 75% memory reduction for the index graph and 5x faster index construction.", "AI": {"tldr": "AQR-HNSW\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u91cf\u5316\u3001\u591a\u72b6\u6001\u91cd\u6392\u5e8f\u548cSIMD\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347HNSW\u5728\u5341\u4ebf\u7ea7\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b02.5-3.3\u500dQPS\u63d0\u5347\u300175%\u5185\u5b58\u51cf\u5c11\u548c5\u500d\u7d22\u5f15\u6784\u5efa\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5411\u91cf\u6570\u636e\u5e93\u6269\u5c55\u5230\u6570\u5341\u4ebf\u5d4c\u5165\u5411\u91cf\uff0cHNSW\u9762\u4e34\u5185\u5b58\u6d88\u8017\u81a8\u80c0\u3001\u8ddd\u79bb\u8ba1\u7b97\u5f00\u9500\u4e3b\u5bfc\u67e5\u8be2\u5ef6\u8fdf\u3001\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0a\u6027\u80fd\u6b20\u4f73\u7b49\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u4f18\u5316\u65b9\u6848\u6765\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faAQR-HNSW\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7b56\u7565\uff1a(1)\u5bc6\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u91cf\u5316\uff0c\u5b9e\u73b04\u500d\u538b\u7f29\u540c\u65f6\u4fdd\u6301\u8ddd\u79bb\u5173\u7cfb\uff1b(2)\u591a\u72b6\u6001\u91cd\u6392\u5e8f\uff0c\u51cf\u5c1135%\u4e0d\u5fc5\u8981\u8ba1\u7b97\uff1b(3)\u91cf\u5316\u4f18\u5316\u7684SIMD\u5b9e\u73b0\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u4e0a\u5b9e\u73b016-64\u64cd\u4f5c/\u5468\u671f\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684HNSW\u5b9e\u73b0\uff0cAQR-HNSW\u5b9e\u73b02.5-3.3\u500d\u66f4\u9ad8\u7684\u6bcf\u79d2\u67e5\u8be2\u6570(QPS)\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc798%\u7684\u53ec\u56de\u7387\uff0c\u7d22\u5f15\u56fe\u5185\u5b58\u51cf\u5c1175%\uff0c\u7d22\u5f15\u6784\u5efa\u901f\u5ea6\u63d0\u53475\u500d\u3002", "conclusion": "AQR-HNSW\u901a\u8fc7\u81ea\u9002\u5e94\u91cf\u5316\u3001\u91cd\u6392\u5e8f\u548cSIMD\u4f18\u5316\u7684\u534f\u540c\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86HNSW\u5728\u5927\u89c4\u6a21\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4e3a\u5de5\u4e1a\u7ea7AI\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5185\u5b58\u4f18\u5316\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21814", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21814", "abs": "https://arxiv.org/abs/2602.21814", "authors": ["Heejin Jo"], "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem", "comment": "9 pages, 4 tables", "summary": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.", "AI": {"tldr": "STAR\u63a8\u7406\u6846\u67b6\u5c06\u6d17\u8f66\u95ee\u9898\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f385%\uff0c\u7ed3\u5408\u7528\u6237\u753b\u50cf\u548cRAG\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u6bd4\u4e0a\u4e0b\u6587\u6ce8\u5165\u66f4\u91cd\u8981", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\"\u6d17\u8f66\u95ee\u9898\"\u8fd9\u4e00\u9700\u8981\u9690\u5f0f\u7269\u7406\u7ea6\u675f\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u5931\u8d25\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u4ea7\u7cfb\u7edf\u4e2d\u54ea\u4e9b\u63d0\u793a\u67b6\u6784\u5c42\u80fd\u591f\u5b9e\u73b0\u6b63\u786e\u63a8\u7406", "method": "\u4f7f\u7528Claude 3.5 Sonnet\u6a21\u578b\uff0c\u63a7\u5236\u8d85\u53c2\u6570\uff08\u6e29\u5ea60.7\uff0ctop_p 1.0\uff09\uff0c\u8fdb\u884c\u53d8\u91cf\u9694\u79bb\u7814\u7a76\uff086\u4e2a\u6761\u4ef6\uff0c\u6bcf\u4e2a\u6761\u4ef6n=20\uff0c\u5171120\u6b21\u8bd5\u9a8c\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u67b6\u6784\u5c42\u7684\u6548\u679c\uff0c\u5305\u62ecSTAR\u63a8\u7406\u6846\u67b6\u3001\u7528\u6237\u753b\u50cf\u5411\u91cf\u6570\u636e\u5e93\u68c0\u7d22\u548cRAG\u4e0a\u4e0b\u6587", "result": "STAR\u63a8\u7406\u6846\u67b6\u5355\u72ec\u5c06\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f385%\uff08p=0.001\uff0cFisher\u7cbe\u786e\u68c0\u9a8c\uff0c\u4f18\u52bf\u6bd413.22\uff09\uff1b\u6dfb\u52a0\u7528\u6237\u753b\u50cf\u4e0a\u4e0b\u6587\u63d0\u4f9b\u989d\u591610\u4e2a\u767e\u5206\u70b9\u7684\u589e\u76ca\uff1bRAG\u4e0a\u4e0b\u6587\u8d21\u732e\u989d\u59165\u4e2a\u767e\u5206\u70b9\uff0c\u5b8c\u6574\u5806\u6808\u6761\u4ef6\u4e0b\u8fbe\u5230100%\u51c6\u786e\u7387", "conclusion": "\u5bf9\u4e8e\u9690\u5f0f\u7ea6\u675f\u63a8\u7406\u4efb\u52a1\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u811a\u624b\u67b6\u2014\u2014\u7279\u522b\u662f\u63a8\u7406\u524d\u7684\u5f3a\u5236\u76ee\u6807\u8868\u8fbe\u2014\u2014\u6bd4\u4e0a\u4e0b\u6587\u6ce8\u5165\u66f4\u4e3a\u91cd\u8981"}}
{"id": "2602.21219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21219", "abs": "https://arxiv.org/abs/2602.21219", "authors": ["Bo Ni", "Branislav Kveton", "Samyadeep Basu", "Subhojyoti Mukherjee", "Leyao Wang", "Franck Dernoncourt", "Sungchul Kim", "Seunghyun Yoon", "Zichao Wang", "Ruiyi Zhang", "Puneet Mathur", "Jihyung Kil", "Jiuxiang Gu", "Nedim Lipka", "Yu Wang", "Ryan A. Rossi", "Tyler Derr"], "title": "Reasoning-Based Personalized Generation for Users with Sparse Data", "comment": null, "summary": "Large Language Model (LLM) personalization holds great promise for tailoring responses by leveraging personal context and history. However, real-world users usually possess sparse interaction histories with limited personal context, such as cold-start users in social platforms and newly registered customers in online E-commerce platforms, compromising the LLM-based personalized generation. To address this challenge, we introduce GraSPer (Graph-based Sparse Personalized Reasoning), a novel framework for enhancing personalized text generation under sparse context. GraSPer first augments user context by predicting items that the user would likely interact with in the future. With reasoning alignment, it then generates texts for these interactions to enrich the augmented context. In the end, it generates personalized outputs conditioned on both the real and synthetic histories, ensuring alignment with user style and preferences. Extensive experiments on three benchmark personalized generation datasets show that GraSPer achieves significant performance gain, substantially improving personalization in sparse user context settings.", "AI": {"tldr": "GraSPer\uff1a\u57fa\u4e8e\u56fe\u7684\u7a00\u758f\u4e2a\u6027\u5316\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u672a\u6765\u53ef\u80fd\u4ea4\u4e92\u7684\u9879\u76ee\u5e76\u751f\u6210\u76f8\u5e94\u6587\u672c\u6765\u589e\u5f3a\u7a00\u758f\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u6539\u5584LLM\u4e2a\u6027\u5316\u751f\u6210", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7528\u6237\u901a\u5e38\u5177\u6709\u7a00\u758f\u7684\u4ea4\u4e92\u5386\u53f2\uff08\u5982\u51b7\u542f\u52a8\u7528\u6237\u3001\u65b0\u6ce8\u518c\u5ba2\u6237\uff09\uff0c\u6709\u9650\u7684\u4e2a\u4eba\u4e0a\u4e0b\u6587\u4f1a\u635f\u5bb3\u57fa\u4e8eLLM\u7684\u4e2a\u6027\u5316\u751f\u6210\u6548\u679c", "method": "GraSPer\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u672a\u6765\u53ef\u80fd\u4ea4\u4e92\u7684\u9879\u76ee\u6765\u589e\u5f3a\u7528\u6237\u4e0a\u4e0b\u6587\uff1b2\uff09\u901a\u8fc7\u63a8\u7406\u5bf9\u9f50\u4e3a\u8fd9\u4e9b\u4ea4\u4e92\u751f\u6210\u6587\u672c\u4ee5\u4e30\u5bcc\u589e\u5f3a\u4e0a\u4e0b\u6587\uff1b3\uff09\u57fa\u4e8e\u771f\u5b9e\u548c\u5408\u6210\u5386\u53f2\u751f\u6210\u4e2a\u6027\u5316\u8f93\u51fa\uff0c\u786e\u4fdd\u4e0e\u7528\u6237\u98ce\u683c\u548c\u504f\u597d\u5bf9\u9f50", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u4e2a\u6027\u5316\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGraSPer\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u7a00\u758f\u7528\u6237\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\u5927\u5e45\u6539\u5584\u4e86\u4e2a\u6027\u5316\u6548\u679c", "conclusion": "GraSPer\u901a\u8fc7\u56fe\u57fa\u589e\u5f3a\u548c\u63a8\u7406\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u4e0a\u4e0b\u6587\u4e0b\u7684LLM\u4e2a\u6027\u5316\u6311\u6218\uff0c\u4e3a\u51b7\u542f\u52a8\u7528\u6237\u7b49\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.21857", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21857", "abs": "https://arxiv.org/abs/2602.21857", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Fernando Acero", "Arturo Oncevay", "Charese H. Smiley", "Xiaomo Liu", "Manuela Veloso"], "title": "Distill and Align Decomposition for Enhanced Claim Verification", "comment": "EACL Findings 2026", "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7GRPO\u540c\u65f6\u4f18\u5316\u5206\u89e3\u8d28\u91cf\u548c\u9a8c\u8bc1\u5668\u5bf9\u9f50\uff0c\u57286\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u9a8c\u8bc1\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u590d\u6742\u58f0\u660e\u7684\u5206\u89e3\u8d28\u91cf\u4e0e\u9a8c\u8bc1\u6027\u80fd\u5bf9\u9f50\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8fd9\u4e24\u4e2a\u65b9\u9762\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u987a\u5e8f\u63a8\u7406\u3001\u6559\u5e08\u84b8\u998f\u793a\u4f8b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u5e73\u8861\u683c\u5f0f\u5408\u89c4\u6027\u3001\u9a8c\u8bc1\u5668\u5bf9\u9f50\u548c\u5206\u89e3\u8d28\u91cf\u7684\u591a\u76ee\u6807\u5956\u52b1", "result": "\u5728\u516d\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\uff0c\u8bad\u7ec3\u76848B\u5206\u89e3\u5668\u5c06\u4e0b\u6e38\u9a8c\u8bc1\u6027\u80fd\u63d0\u5347\u81f371.75% macro-F1\uff0c\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff08+1.99\uff0c+6.24\uff09\u548c\u73b0\u6709RL\u65b9\u6cd5\uff08+5.84\uff09", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u5206\u89e3\u8d28\u91cf\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u58f0\u660e\u9a8c\u8bc1"}}
{"id": "2602.21220", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21220", "abs": "https://arxiv.org/abs/2602.21220", "authors": ["Subhadip Mitra"], "title": "Field-Theoretic Memory for AI Agents: Continuous Dynamics for Context Preservation", "comment": "15 pages, 6 figures. Code: https://github.com/rotalabs/rotalabs-fieldmem", "summary": "We present a memory system for AI agents that treats stored information as continuous fields governed by partial differential equations rather than discrete entries in a database. The approach draws from classical field theory: memories diffuse through semantic space, decay thermodynamically based on importance, and interact through field coupling in multi-agent scenarios. We evaluate the system on two established long-context benchmarks: LoCoMo (ACL 2024) with 300-turn conversations across 35 sessions, and LongMemEval (ICLR 2025) testing multi-session reasoning over 500+ turns. On LongMemEval, the field-theoretic approach achieves significant improvements: +116% F1 on multi-session reasoning (p<0.01, d= 3.06), +43.8% on temporal reasoning (p<0.001, d= 9.21), and +27.8% retrieval recall on knowledge updates (p<0.001, d= 5.00). Multi-agent experiments show near-perfect collective intelligence (>99.8%) through field coupling. Code is available at github.com/rotalabs/rotalabs-fieldmem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u8bba\u7684AI\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5c06\u5b58\u50a8\u4fe1\u606f\u89c6\u4e3a\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u8fde\u7eed\u573a\u800c\u975e\u79bb\u6563\u6570\u636e\u5e93\u6761\u76ee\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfAI\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u79bb\u6563\u6570\u636e\u5e93\u6761\u76ee\u65b9\u5f0f\u5b58\u50a8\u4fe1\u606f\uff0c\u7f3a\u4e4f\u8fde\u7eed\u6027\u548c\u52a8\u6001\u4ea4\u4e92\u7279\u6027\u3002\u53d7\u7ecf\u5178\u573a\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u7b26\u5408\u4fe1\u606f\u672c\u8d28\u7279\u6027\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u80fd\u591f\u6a21\u62df\u8bb0\u5fc6\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u6269\u6563\u3001\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u70ed\u529b\u5b66\u8870\u51cf\u4ee5\u53ca\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u573a\u8026\u5408\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u573a\u8bba\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5c06\u8bb0\u5fc6\u4fe1\u606f\u5efa\u6a21\u4e3a\u8fde\u7eed\u573a\uff0c\u7528\u504f\u5fae\u5206\u65b9\u7a0b\u63cf\u8ff0\u5176\u52a8\u6001\u884c\u4e3a\uff1a1) \u8bb0\u5fc6\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u6269\u6563\uff1b2) \u57fa\u4e8e\u91cd\u8981\u6027\u7684\u70ed\u529b\u5b66\u8870\u51cf\uff1b3) \u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u573a\u8026\u5408\u4ea4\u4e92\u3002\u7cfb\u7edf\u5728\u4e24\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1aLoCoMo\uff08ACL 2024\uff09\u5305\u542b35\u4e2a\u4f1a\u8bdd\u7684300\u8f6e\u5bf9\u8bdd\uff0c\u4ee5\u53caLongMemEval\uff08ICLR 2025\uff09\u6d4b\u8bd5500+\u8f6e\u7684\u591a\u4f1a\u8bdd\u63a8\u7406\u3002", "result": "\u5728LongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u573a\u8bba\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1a\u591a\u4f1a\u8bdd\u63a8\u7406F1\u5206\u6570\u63d0\u5347116%\uff08p<0.01\uff0c\u6548\u5e94\u91cfd=3.06\uff09\uff1b\u65f6\u5e8f\u63a8\u7406\u63d0\u534743.8%\uff08p<0.001\uff0cd=9.21\uff09\uff1b\u77e5\u8bc6\u66f4\u65b0\u68c0\u7d22\u53ec\u56de\u7387\u63d0\u534727.8%\uff08p<0.001\uff0cd=5.00\uff09\u3002\u591a\u667a\u80fd\u4f53\u5b9e\u9a8c\u901a\u8fc7\u573a\u8026\u5408\u5b9e\u73b0\u63a5\u8fd1\u5b8c\u7f8e\u7684\u96c6\u4f53\u667a\u80fd\uff08>99.8%\uff09\u3002", "conclusion": "\u573a\u8bba\u8bb0\u5fc6\u7cfb\u7edf\u4e3aAI\u667a\u80fd\u4f53\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u8fde\u7eed\u573a\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u4f1a\u8bdd\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96c6\u4f53\u667a\u80fd\u7279\u6027\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.21756", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21756", "abs": "https://arxiv.org/abs/2602.21756", "authors": ["Deogyong Kim", "Junseong Lee", "Jeongeun Lee", "Changhoe Kim", "Junguel Lee", "Jungseok Lee", "Dongha Lee"], "title": "Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing", "comment": "Under review", "summary": "Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems.", "AI": {"tldr": "Persona4Rec\uff1a\u57fa\u4e8eLLM\u79bb\u7ebf\u63a8\u7406\u6784\u5efa\u53ef\u89e3\u91ca\u7269\u54c1\u4eba\u683c\u8868\u5f81\u7684\u63a8\u8350\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u63a8\u7406", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63a8\u8350\u91cd\u6392\u5e8f\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5728\u7ebf\u63a8\u7406\u65f6\u95f4\u63a8\u7406\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528LLM\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u53c8\u80fd\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u79bb\u7ebf\u9636\u6bb5\uff1a\u5229\u7528LLM\u5bf9\u7269\u54c1\u8bc4\u8bba\u8fdb\u884c\u63a8\u7406\uff0c\u63a8\u65ad\u4e0d\u540c\u7528\u6237\u53ef\u80fd\u4e0e\u8be5\u7269\u54c1\u4e92\u52a8\u7684\u591a\u6837\u5316\u52a8\u673a\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u4eba\u683c\u8868\u5f81\u30022. \u5b66\u4e60\u7528\u6237\u6863\u6848\u4e0e\u6700\u53ef\u80fd\u7269\u54c1\u4eba\u683c\u7684\u5bf9\u9f50\uff0c\u901a\u8fc7\u4e13\u7528\u7f16\u7801\u5668\u5c06\u7528\u6237-\u7269\u54c1\u76f8\u5173\u6027\u8f6c\u5316\u4e3a\u7528\u6237-\u4eba\u683c\u76f8\u5173\u6027\u30023. \u5728\u7ebf\u9636\u6bb5\uff1a\u4f7f\u7528\u4eba\u683c\u753b\u50cf\u7684\u7269\u54c1\u7d22\u5f15\u8fdb\u884c\u5feb\u901f\u76f8\u5173\u6027\u8ba1\u7b97\uff0c\u65e0\u9700\u8c03\u7528\u6602\u8d35\u7684LLM\u63a8\u7406\u3002", "result": "Persona4Rec\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u8fd1\u7684LLM\u91cd\u6392\u5e8f\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002\u5b9a\u6027\u5206\u6790\u8bc1\u5b9e\u4eba\u683c\u8868\u5f81\u4e0d\u4ec5\u80fd\u9a71\u52a8\u9ad8\u6548\u8bc4\u5206\uff0c\u8fd8\u80fd\u63d0\u4f9b\u57fa\u4e8e\u8bc4\u8bba\u7684\u76f4\u89c2\u89e3\u91ca\u3002", "conclusion": "Persona4Rec\u4e3a\u4e0b\u4e00\u4ee3\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u79bb\u7ebf\u63a8\u7406\u6784\u5efa\u53ef\u89e3\u91ca\u4eba\u683c\u8868\u5f81\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eLLM\u91cd\u6392\u5e8f\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2602.21222", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21222", "abs": "https://arxiv.org/abs/2602.21222", "authors": ["Riya Adsul", "Balachandra Devarangadi Sunil", "Isha Nalawade", "Sudharshan Govindan"], "title": "Task-Aware LoRA Adapter Composition via Similarity Retrieval in Vector Databases", "comment": null, "summary": "Parameter efficient fine tuning methods like LoRA have enabled task specific adaptation of large language models, but efficiently composing multiple specialized adapters for unseen tasks remains challenging. We present a novel framework for dynamic LoRA adapter composition that leverages similarity retrieval in vector databases to enable zero-shot generalization across diverse NLP tasks. Our approach constructs a task-aware vector database by embedding training examples from 22 datasets spanning commonsense reasoning, question answering, natural language inference, and sentiment analysis. At inference time, we retrieve the most similar training examples, compute task similarity distributions via nucleus sampling, and dynamically merge relevant LoRA adapters using retrieval weighted fusion strategies. We evaluated four merging methods Linear, Concatenation, TIES, and Magnitude Prune demonstrating that our dataset centric retrieval approach often matches or exceeds the performance of individually fine-tuned task-specific adapters. Notably, Linear merging achieves 70.95% on PIQA and 77.62% on RTE, substantially outperforming single-task baselines (46% and 52%, respectively). Our framework requires no additional retriever training, operates with frozen embeddings, and enables efficient, interpretable adapter composition. These results suggest that retrieval based dynamic merging offers a promising direction for scalable, parameter-efficient multitask learning without requiring full model retraining for each new task.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5411\u91cf\u6570\u636e\u5e93\u68c0\u7d22\u7684\u52a8\u6001LoRA\u9002\u914d\u5668\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u68c0\u7d22\u5b9e\u73b0\u96f6\u6837\u672c\u8de8\u4efb\u52a1\u6cdb\u5316\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u68c0\u7d22\u5668\uff0c\u572822\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u56db\u79cd\u5408\u5e76\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u867d\u7136\u80fd\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u7279\u5b9a\u9002\u914d\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u7ec4\u5408\u591a\u4e2a\u4e13\u7528\u9002\u914d\u5668\u6765\u5904\u7406\u672a\u89c1\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u52a8\u6001\u9002\u914d\u5668\u7ec4\u5408\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4efb\u52a1\u611f\u77e5\u7684\u5411\u91cf\u6570\u636e\u5e93\uff0c\u5d4c\u5165\u6765\u81ea22\u4e2a\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u793a\u4f8b\uff08\u6db5\u76d6\u5e38\u8bc6\u63a8\u7406\u3001\u95ee\u7b54\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u60c5\u611f\u5206\u6790\uff09\u3002\u63a8\u7406\u65f6\u68c0\u7d22\u6700\u76f8\u4f3c\u7684\u8bad\u7ec3\u793a\u4f8b\uff0c\u901a\u8fc7nucleus\u91c7\u6837\u8ba1\u7b97\u4efb\u52a1\u76f8\u4f3c\u5ea6\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u68c0\u7d22\u52a0\u6743\u878d\u5408\u7b56\u7565\u52a8\u6001\u5408\u5e76\u76f8\u5173LoRA\u9002\u914d\u5668\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u5408\u5e76\u65b9\u6cd5\uff1a\u7ebf\u6027\u5408\u5e76\u3001\u62fc\u63a5\u3001TIES\u548c\u5e45\u5ea6\u526a\u679d\u3002", "result": "\u6570\u636e\u96c6\u4e2d\u5fc3\u7684\u68c0\u7d22\u65b9\u6cd5\u901a\u5e38\u5339\u914d\u6216\u8d85\u8fc7\u5355\u72ec\u5fae\u8c03\u7684\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u6027\u80fd\u3002\u7ebf\u6027\u5408\u5e76\u65b9\u6cd5\u5728PIQA\u4e0a\u8fbe\u523070.95%\uff0c\u5728RTE\u4e0a\u8fbe\u523077.62%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4efb\u52a1\u57fa\u7ebf\uff08\u5206\u522b\u4e3a46%\u548c52%\uff09\u3002\u6846\u67b6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u68c0\u7d22\u5668\uff0c\u4f7f\u7528\u51bb\u7ed3\u5d4c\u5165\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u9002\u914d\u5668\u7ec4\u5408\u3002", "conclusion": "\u57fa\u4e8e\u68c0\u7d22\u7684\u52a8\u6001\u5408\u5e76\u4e3a\u53ef\u6269\u5c55\u7684\u53c2\u6570\u9ad8\u6548\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u91cd\u65b0\u8bad\u7ec3\u5b8c\u6574\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u901a\u8fc7\u76f8\u4f3c\u6027\u68c0\u7d22\u5b9e\u73b0\u96f6\u6837\u672c\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.21957", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21957", "abs": "https://arxiv.org/abs/2602.21957", "authors": ["Yuchun Tu", "Zhiwei Li", "Bingli Sun", "Yixuan Li", "Xiao Song"], "title": "Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation", "comment": "18 pages, 9 figures", "summary": "Federated recommendation facilitates collaborative model training across distributed clients while keeping sensitive user interaction data local. Conventional approaches typically rely on synchronizing high-dimensional item representations between the server and clients. This paradigm implicitly assumes that precise geometric alignment of embedding coordinates is necessary for collaboration across clients. We posit that establishing relative semantic relationships among items is more effective than enforcing shared representations. Specifically, global semantic relations serve as structural constraints for items. Within these constraints, the framework allows item representations to vary locally on each client, which flexibility enables the model to capture fine-grained user personalization while maintaining global consistency. To this end, we propose Cluster-Guided FedRec framework (CGFedRec), a framework that transforms uploaded embeddings into compact cluster labels. In this framework, the server functions as a global structure discoverer to learn item clusters and distributes only the resulting labels. This mechanism explicitly cuts off the downstream transmission of item embeddings, relieving clients from maintaining global shared item embeddings. Consequently, CGFedRec achieves the effective injection of global collaborative signals into local item representations without transmitting full embeddings. Extensive experiments demonstrate that our approach significantly improves communication efficiency while maintaining superior recommendation accuracy across multiple datasets.", "AI": {"tldr": "CGFedRec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u6807\u7b7e\u800c\u975e\u5b8c\u6574\u5d4c\u5165\u7684\u8054\u90a6\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u805a\u7c7b\u6807\u7b7e\u800c\u975e\u9ad8\u7ef4\u5d4c\u5165\u6765\u63d0\u5347\u901a\u4fe1\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u63a8\u8350\u65b9\u6cd5\u9700\u8981\u540c\u6b65\u9ad8\u7ef4\u9879\u76ee\u5d4c\u5165\uff0c\u5047\u8bbe\u7cbe\u786e\u7684\u51e0\u4f55\u5750\u6807\u5bf9\u9f50\u662f\u5b9e\u73b0\u8de8\u5ba2\u6237\u7aef\u534f\u4f5c\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u4f5c\u8005\u8ba4\u4e3a\u5efa\u7acb\u9879\u76ee\u95f4\u7684\u76f8\u5bf9\u8bed\u4e49\u5173\u7cfb\u6bd4\u5f3a\u5236\u5171\u4eab\u8868\u793a\u66f4\u6709\u6548\uff0c\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u53ef\u4f5c\u4e3a\u7ed3\u6784\u7ea6\u675f\uff0c\u5141\u8bb8\u9879\u76ee\u8868\u793a\u5728\u5ba2\u6237\u7aef\u672c\u5730\u53d8\u5316\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7528\u6237\u4e2a\u6027\u5316\u3002", "method": "\u63d0\u51faCluster-Guided FedRec\u6846\u67b6(CGFedRec)\uff0c\u5c06\u4e0a\u4f20\u7684\u5d4c\u5165\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u805a\u7c7b\u6807\u7b7e\u3002\u670d\u52a1\u5668\u4f5c\u4e3a\u5168\u5c40\u7ed3\u6784\u53d1\u73b0\u8005\u5b66\u4e60\u9879\u76ee\u805a\u7c7b\uff0c\u4ec5\u5206\u53d1\u7ed3\u679c\u6807\u7b7e\u3002\u8fd9\u79cd\u673a\u5236\u663e\u5f0f\u5207\u65ad\u9879\u76ee\u5d4c\u5165\u7684\u4e0b\u6e38\u4f20\u8f93\uff0c\u4f7f\u5ba2\u6237\u7aef\u65e0\u9700\u7ef4\u62a4\u5168\u5c40\u5171\u4eab\u7684\u9879\u76ee\u5d4c\u5165\uff0c\u4ece\u800c\u5728\u4e0d\u4f20\u8f93\u5b8c\u6574\u5d4c\u5165\u7684\u60c5\u51b5\u4e0b\u5c06\u5168\u5c40\u534f\u4f5c\u4fe1\u53f7\u6709\u6548\u6ce8\u5165\u672c\u5730\u9879\u76ee\u8868\u793a\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f18\u8d8a\u7684\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "CGFedRec\u901a\u8fc7\u4f20\u8f93\u805a\u7c7b\u6807\u7b7e\u800c\u975e\u5b8c\u6574\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u4e0e\u63a8\u8350\u51c6\u786e\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u8054\u90a6\u63a8\u8350\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u534f\u4f5c\u8303\u5f0f\u3002"}}
{"id": "2602.21223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21223", "abs": "https://arxiv.org/abs/2602.21223", "authors": ["Yilin Geng", "Omri Abend", "Eduard Hovy", "Lea Frermann"], "title": "Measuring Pragmatic Influence in Large Language Model Instructions", "comment": null, "summary": "It is not only what we ask large language models (LLMs) to do that matters, but also how we prompt. Phrases like \"This is urgent\" or \"As your supervisor\" can shift model behavior without altering task content. We study this effect as pragmatic framing, contextual cues that shape directive interpretation rather than task specification. While prior work exploits such cues for prompt optimization or probes them as security vulnerabilities, pragmatic framing itself has not been treated as a measurable property of instruction following. Measuring this influence systematically remains challenging, requiring controlled isolation of framing cues. We introduce a framework with three novel components: directive-framing decomposition separating framing context from task specification; a taxonomy organizing 400 instantiations of framing into 13 strategies across 4 mechanism clusters; and priority-based measurement that quantifies influence through observable shifts in directive prioritization. Across five LLMs of different families and sizes, influence mechanisms cause consistent and structured shifts in directive prioritization, moving models from baseline impartiality toward favoring the framed directive. This work establishes pragmatic framing as a measurable and predictable factor in instruction-following systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u63d0\u793a\u8bed\u4e2d\u7684\u8bed\u7528\u6846\u67b6\u6548\u5e94\uff0c\u5373\u901a\u8fc7\"\u7d27\u6025\"\u3001\"\u4f5c\u4e3a\u4f60\u7684\u4e0a\u53f8\"\u7b49\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6539\u53d8LLM\u884c\u4e3a\u800c\u4e0d\u6539\u53d8\u4efb\u52a1\u5185\u5bb9\uff0c\u5efa\u7acb\u4e86\u6d4b\u91cf\u6846\u67b6\u5e76\u53d1\u73b0\u8bed\u7528\u6846\u67b6\u80fd\u7cfb\u7edf\u6027\u5730\u5f71\u54cd\u6307\u4ee4\u4f18\u5148\u7ea7", "motivation": "\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5c06\u8bed\u7528\u6846\u67b6\u4f5c\u4e3a\u63d0\u793a\u4f18\u5316\u5de5\u5177\uff0c\u8981\u4e48\u89c6\u4e3a\u5b89\u5168\u6f0f\u6d1e\uff0c\u4f46\u8bed\u7528\u6846\u67b6\u672c\u8eab\u5c1a\u672a\u88ab\u89c6\u4e3a\u6307\u4ee4\u9075\u5faa\u7cfb\u7edf\u7684\u53ef\u6d4b\u91cf\u5c5e\u6027\u3002\u9700\u8981\u7cfb\u7edf\u6d4b\u91cf\u8fd9\u79cd\u5f71\u54cd\uff0c\u4f46\u9762\u4e34\u5206\u79bb\u6846\u67b6\u7ebf\u7d22\u4e0e\u4efb\u52a1\u5185\u5bb9\u7684\u6311\u6218", "method": "\u63d0\u51fa\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a1) \u6307\u4ee4-\u6846\u67b6\u5206\u89e3\uff0c\u5206\u79bb\u6846\u67b6\u4e0a\u4e0b\u6587\u4e0e\u4efb\u52a1\u89c4\u8303\uff1b2) \u5305\u542b13\u79cd\u7b56\u7565\u30014\u4e2a\u673a\u5236\u7c07\u7684400\u4e2a\u5b9e\u4f8b\u5206\u7c7b\u6cd5\uff1b3) \u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u4f18\u5148\u7ea7\u53ef\u89c2\u5bdf\u53d8\u5316\u91cf\u5316\u5f71\u54cd", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u5bb6\u65cf\u548c\u89c4\u6a21\u7684LLM\u4e0a\uff0c\u5f71\u54cd\u673a\u5236\u5bfc\u81f4\u6307\u4ee4\u4f18\u5148\u7ea7\u7684\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u5316\u53d8\u5316\uff0c\u4f7f\u6a21\u578b\u4ece\u57fa\u7ebf\u4e2d\u7acb\u6027\u8f6c\u5411\u504f\u5411\u6846\u67b6\u6307\u4ee4\u3002\u8bed\u7528\u6846\u67b6\u6548\u5e94\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u53ef\u9884\u6d4b\u7684\u6a21\u5f0f", "conclusion": "\u8be5\u5de5\u4f5c\u786e\u7acb\u4e86\u8bed\u7528\u6846\u67b6\u4f5c\u4e3a\u6307\u4ee4\u9075\u5faa\u7cfb\u7edf\u4e2d\u53ef\u6d4b\u91cf\u548c\u53ef\u9884\u6d4b\u7684\u56e0\u7d20\uff0c\u4e3a\u7406\u89e3LLM\u5982\u4f55\u89e3\u91ca\u548c\u54cd\u5e94\u4eba\u7c7b\u6307\u4ee4\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2602.22067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22067", "abs": "https://arxiv.org/abs/2602.22067", "authors": ["Giuseppe Canonaco", "Alberto Pozanco", "Daniel Borrajo"], "title": "Semantic Partial Grounding via LLMs", "comment": null, "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.", "AI": {"tldr": "SPG-LLM\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790PDDL\u6587\u4ef6\uff0c\u5728\u89c4\u5212\u524d\u542f\u53d1\u5f0f\u8bc6\u522b\u6f5c\u5728\u65e0\u5173\u7684\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u8c13\u8bcd\uff0c\u663e\u8457\u51cf\u5c11\u57fa\u7840\u5316\u4efb\u52a1\u89c4\u6a21\uff0c\u5728\u4e03\u4e2a\u96be\u57fa\u7840\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u66f4\u5feb\u7684\u57fa\u7840\u5316\u901f\u5ea6\u3002", "motivation": "\u7ecf\u5178\u89c4\u5212\u4e2d\u7684\u57fa\u7840\u5316\u6b65\u9aa4\u5e38\u56e0\u4efb\u52a1\u89c4\u6a21\u589e\u5927\u5bfc\u81f4\u57fa\u7840\u5316\u52a8\u4f5c\u548c\u539f\u5b50\u6570\u91cf\u6307\u6570\u589e\u957f\u800c\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002\u73b0\u6709\u90e8\u5206\u57fa\u7840\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5173\u7cfb\u7279\u5f81\u6216\u5b66\u4e60\u5d4c\u5165\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528PDDL\u63cf\u8ff0\u4e2d\u7684\u6587\u672c\u548c\u7ed3\u6784\u7ebf\u7d22\u3002", "method": "\u63d0\u51faSPG-LLM\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u9886\u57df\u548c\u95ee\u9898\u6587\u4ef6\uff0c\u5728\u57fa\u7840\u5316\u524d\u542f\u53d1\u5f0f\u8bc6\u522b\u6f5c\u5728\u65e0\u5173\u7684\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u8c13\u8bcd\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u57fa\u7840\u5316\u4efb\u52a1\u7684\u89c4\u6a21\u3002", "result": "\u5728\u4e03\u4e2a\u96be\u57fa\u7840\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPG-LLM\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u57fa\u7840\u5316\u901f\u5ea6\uff08\u901a\u5e38\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff09\uff0c\u5728\u67d0\u4e9b\u9886\u57df\u8fd8\u63d0\u4f9b\u4e86\u76f8\u5f53\u6216\u66f4\u597d\u7684\u89c4\u5212\u6210\u672c\u3002", "conclusion": "SPG-LLM\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790PDDL\u7684\u6587\u672c\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u57fa\u7840\u5316\u89c4\u6a21\uff0c\u89e3\u51b3\u4e86\u89c4\u5212\u4e2d\u7684\u57fa\u7840\u5316\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u89c4\u5212\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2602.21224", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21224", "abs": "https://arxiv.org/abs/2602.21224", "authors": ["Yuetao Chen", "Xuliang Wang", "Xinzhou Zheng", "Ming Li", "Peng Wang", "Hong Xu"], "title": "Make Every Draft Count: Hidden State based Speculative Decoding", "comment": null, "summary": "Speculative decoding has emerged as a pivotal technique to accelerate LLM inference by employing a lightweight draft model to generate candidate tokens that are subsequently verified by the target model in parallel. However, while this paradigm successfully increases the arithmetic intensity of memory-bound inference, it causes significant compute inefficiency: the majority of draft tokens fail verification and are discarded, resulting in waste of computation. Motivated by the goal of recollecting this wasted computation, we propose a novel system that transforms discarded drafts into reusable tokens. Our key insight is to perform auto-regressive prediction at the hidden states level and postpone the integrating token information after the hidden states generation, so the draft hidden states are not contaminated by incorrect tokens, enabling hidden state reuse. To implement such a system, first we introduce a draft model architecture based on auto-regressive hidden states, which preserves richer semantics than token-based drafters to facilitate draft repurposing. Second, we design an efficient token information injection mechanism that leverages our specialized draft model to construct high-quality draft token trees and enables resampling tokens from verification failures. Third, we eliminate the overhead hidden in our design to further maximize hardware utilization. We conducted extensive evaluations against various baselines, demonstrating up to a 3.3x speedup against standard speculative decoding.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u6d4b\u89e3\u7801\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cd\u7528\u88ab\u4e22\u5f03\u7684\u8349\u7a3f\u9690\u85cf\u72b6\u6001\u6765\u56de\u6536\u6d6a\u8d39\u7684\u8ba1\u7b97\uff0c\u76f8\u6bd4\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5b9e\u73b0\u6700\u9ad83.3\u500d\u52a0\u901f\u3002", "motivation": "\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u4e2d\uff0c\u5927\u591a\u6570\u8349\u7a3f\u4ee4\u724c\u9a8c\u8bc1\u5931\u8d25\u88ab\u4e22\u5f03\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6d6a\u8d39\u3002\u672c\u6587\u65e8\u5728\u56de\u6536\u8fd9\u4e9b\u6d6a\u8d39\u7684\u8ba1\u7b97\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "1) \u57fa\u4e8e\u81ea\u56de\u5f52\u9690\u85cf\u72b6\u6001\u7684\u8349\u7a3f\u6a21\u578b\u67b6\u6784\uff0c\u4fdd\u7559\u6bd4\u4ee4\u724c\u7ea7\u8349\u7a3f\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\uff1b2) \u9ad8\u6548\u7684\u4ee4\u724c\u4fe1\u606f\u6ce8\u5165\u673a\u5236\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u8349\u7a3f\u4ee4\u724c\u6811\u5e76\u652f\u6301\u4ece\u9a8c\u8bc1\u5931\u8d25\u4e2d\u91cd\u65b0\u91c7\u6837\u4ee4\u724c\uff1b3) \u6d88\u9664\u8bbe\u8ba1\u4e2d\u7684\u9690\u85cf\u5f00\u9500\u4ee5\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u76f8\u6bd4\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad83.3\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff08\u76f8\u5bf9\u4e8e\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u901a\u8fc7\u91cd\u7528\u88ab\u4e22\u5f03\u7684\u8349\u7a3f\u9690\u85cf\u72b6\u6001\uff0c\u6210\u529f\u56de\u6536\u4e86\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u6d6a\u8d39\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u63a8\u7406\u7684\u8ba1\u7b97\u6548\u7387\u548c\u901f\u5ea6\u3002"}}
{"id": "2602.21225", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21225", "abs": "https://arxiv.org/abs/2602.21225", "authors": ["Mohammed Hamdan", "Vincenzo Dentamaro", "Giuseppe Pirlo", "Mohamed Cheriet"], "title": "Architecture-Agnostic Curriculum Learning for Document Understanding: Empirical Evidence from Text-Only and Multimodal", "comment": null, "summary": "We investigate whether progressive data scheduling -- a curriculum learning strategy that incrementally increases training data exposure (33\\%$\\rightarrow$67\\%$\\rightarrow$100\\%) -- yields consistent efficiency gains across architecturally distinct document understanding models. By evaluating BERT (text-only, 110M parameters) and LayoutLMv3 (multimodal, 126M parameters) on the FUNSD and CORD benchmarks, we establish that this schedule reduces wall-clock training time by approximately 33\\%, commensurate with the reduction from 6.67 to 10.0 effective epoch-equivalents of data. To isolate curriculum effects from compute reduction, we introduce matched-compute baselines (Standard-7) that control for total gradient updates. On the FUNSD dataset, the curriculum significantly outperforms the matched-compute baseline for BERT ($\u0394$F1 = +0.023, $p=0.022$, $d_z=3.83$), constituting evidence for a genuine scheduling benefit in capacity-constrained models. In contrast, no analogous benefit is observed for LayoutLMv3 ($p=0.621$), whose multimodal representations provide sufficient inductive bias. On the CORD dataset, all conditions converge to equivalent F1 scores ($\\geq$0.947) irrespective of scheduling, indicating a performance ceiling. Schedule ablations comparing progressive, two-phase, reverse, and random pacing confirm that the efficiency gain derives from reduced data volume rather than ordering. Taken together, these findings demonstrate that progressive scheduling is a reliable compute-reduction strategy across model families, with curriculum-specific benefits contingent on the interaction between model capacity and task complexity.", "AI": {"tldr": "\u6e10\u8fdb\u6570\u636e\u8c03\u5ea6\uff08\u9010\u6b65\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u66b4\u9732\uff09\u5728\u6587\u6863\u7406\u89e3\u6a21\u578b\u4e2d\u53ef\u51cf\u5c11\u7ea633%\u8bad\u7ec3\u65f6\u95f4\uff0c\u5176\u6536\u76ca\u53d6\u51b3\u4e8e\u6a21\u578b\u5bb9\u91cf\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u4f5c\u7528", "motivation": "\u7814\u7a76\u6e10\u8fdb\u6570\u636e\u8c03\u5ea6\uff08\u4e00\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff09\u5728\u4e0d\u540c\u67b6\u6784\u7684\u6587\u6863\u7406\u89e3\u6a21\u578b\u4e2d\u662f\u5426\u5177\u6709\u4e00\u81f4\u7684\u6548\u7387\u63d0\u5347\u6548\u679c\uff0c\u5e76\u63a2\u7a76\u5176\u6536\u76ca\u662f\u5426\u6e90\u4e8e\u771f\u6b63\u7684\u8bfe\u7a0b\u6548\u5e94\u800c\u975e\u5355\u7eaf\u7684\u8ba1\u7b97\u91cf\u51cf\u5c11", "method": "\u8bc4\u4f30BERT\uff08\u7eaf\u6587\u672c\uff0c110M\u53c2\u6570\uff09\u548cLayoutLMv3\uff08\u591a\u6a21\u6001\uff0c126M\u53c2\u6570\uff09\u5728FUNSD\u548cCORD\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002\u91c7\u7528\u6e10\u8fdb\u6570\u636e\u8c03\u5ea6\uff0833%\u219267%\u2192100%\uff09\uff0c\u5f15\u5165\u5339\u914d\u8ba1\u7b97\u57fa\u7ebf\uff08Standard-7\uff09\u4ee5\u63a7\u5236\u603b\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u3002\u8fdb\u884c\u8c03\u5ea6\u6d88\u878d\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u6e10\u8fdb\u3001\u4e24\u9636\u6bb5\u3001\u53cd\u5411\u548c\u968f\u673a\u8c03\u5ea6\u7b56\u7565", "result": "\u6e10\u8fdb\u8c03\u5ea6\u51cf\u5c11\u7ea633%\u8bad\u7ec3\u65f6\u95f4\uff08\u4ece6.67\u523010.0\u6709\u6548epoch\u5f53\u91cf\uff09\u3002\u5728FUNSD\u4e0a\uff0cBERT\u7684\u8bfe\u7a0b\u8c03\u5ea6\u663e\u8457\u4f18\u4e8e\u5339\u914d\u8ba1\u7b97\u57fa\u7ebf\uff08\u0394F1=+0.023\uff0cp=0.022\uff0cdz=3.83\uff09\uff0c\u8868\u660e\u5bb9\u91cf\u53d7\u9650\u6a21\u578b\u5b58\u5728\u771f\u6b63\u7684\u8c03\u5ea6\u6536\u76ca\uff1b\u4f46LayoutLMv3\u65e0\u7c7b\u4f3c\u6536\u76ca\uff08p=0.621\uff09\u3002\u5728CORD\u4e0a\uff0c\u6240\u6709\u6761\u4ef6\u6536\u655b\u5230\u7b49\u6548F1\u5206\u6570\uff08\u22650.947\uff09\uff0c\u8868\u660e\u5b58\u5728\u6027\u80fd\u5929\u82b1\u677f\u3002\u8c03\u5ea6\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u6548\u7387\u589e\u76ca\u6e90\u4e8e\u6570\u636e\u91cf\u51cf\u5c11\u800c\u975e\u987a\u5e8f\u5b89\u6392", "conclusion": "\u6e10\u8fdb\u8c03\u5ea6\u662f\u8de8\u6a21\u578b\u65cf\u7684\u53ef\u9760\u8ba1\u7b97\u51cf\u5c11\u7b56\u7565\uff0c\u5176\u8bfe\u7a0b\u7279\u5b9a\u6536\u76ca\u53d6\u51b3\u4e8e\u6a21\u578b\u5bb9\u91cf\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u5bb9\u91cf\u53d7\u9650\u6a21\u578b\uff08\u5982BERT\uff09\u4ece\u6e10\u8fdb\u8c03\u5ea6\u4e2d\u83b7\u76ca\uff0c\u800c\u5177\u6709\u8db3\u591f\u5f52\u7eb3\u504f\u7f6e\u7684\u591a\u6a21\u6001\u6a21\u578b\uff08\u5982LayoutLMv3\uff09\u5219\u65e0\u660e\u663e\u4f18\u52bf"}}
{"id": "2602.21543", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21543", "abs": "https://arxiv.org/abs/2602.21543", "authors": ["Barah Fazili", "Koustava Goswami"], "title": "Enhancing Multilingual Embeddings via Multi-Way Parallel Text Alignment", "comment": null, "summary": "Multilingual pretraining typically lacks explicit alignment signals, leading to suboptimal cross-lingual alignment in the representation space. In this work, we show that training standard pretrained models for cross-lingual alignment with a multi-way parallel corpus in a diverse pool of languages can substantially improve multilingual and cross-lingual representations for NLU tasks. We construct a multi-way parallel dataset using translations of English text from an off-the-shelf NMT model for a pool of six target languages and achieve strong cross-lingual alignment through contrastive learning. This leads to substantial performance gains across both seen and unseen languages for multiple tasks from the MTEB benchmark evaluated for XLM-Roberta and multilingual BERT base models. Using a multi-way parallel corpus for contrastive training yields substantial gains on bitext mining (21.3%), semantic similarity (5.3%), and classification (28.4%) compared to English-centric (En-X) bilingually parallel data, where X is sampled from a pool of multiple target languages. Furthermore, finetuning mE5 model on a small dataset with multi-way parallelism significantly improves bitext mining compared to one without, underscoring the importance of multi-way cross-lingual supervision even for models already pretrained for high-quality sentence embeddings.", "AI": {"tldr": "\u901a\u8fc7\u591a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u591a\u79cdNLU\u4efb\u52a1\u4e0a\u53d6\u5f97\u6027\u80fd\u63d0\u5347", "motivation": "\u4f20\u7edf\u7684\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u7f3a\u4e4f\u663e\u5f0f\u7684\u5bf9\u9f50\u4fe1\u53f7\uff0c\u5bfc\u81f4\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u8de8\u8bed\u8a00\u5bf9\u9f50\u6548\u679c\u4e0d\u7406\u60f3\u3002\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8de8\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5\u6765\u63d0\u5347\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u8868\u793a\u7684\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u591a\u8bed\u8a00\u5e73\u884c\u6570\u636e\u96c6\uff1a\u4f7f\u7528\u73b0\u6210\u7684NMT\u6a21\u578b\u5c06\u82f1\u8bed\u6587\u672c\u7ffb\u8bd1\u62106\u79cd\u76ee\u6807\u8bed\u8a00\uff0c\u5f62\u6210\u591a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6807\u51c6\u9884\u8bad\u7ec3\u6a21\u578b\uff08XLM-Roberta\u548cmBERT\uff09\uff0c\u5b9e\u73b0\u5f3a\u8de8\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u76f8\u6bd4\u82f1\u8bed\u4e2d\u5fc3\u7684\u53cc\u8bed\u5e73\u884c\u6570\u636e\uff0c\u591a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93\u7684\u5bf9\u6bd4\u8bad\u7ec3\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5e26\u6765\u663e\u8457\u63d0\u5347\uff1a\u53cc\u8bed\u6587\u672c\u6316\u6398\uff08+21.3%\uff09\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff08+5.3%\uff09\u3001\u5206\u7c7b\u4efb\u52a1\uff08+28.4%\uff09\u3002\u5373\u4f7f\u5728\u5df2\u7ecf\u9884\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\uff08mE5\uff09\u4e0a\uff0c\u5fae\u8c03\u5c11\u91cf\u591a\u8bed\u8a00\u5e73\u884c\u6570\u636e\u4e5f\u80fd\u663e\u8457\u6539\u5584\u53cc\u8bed\u6587\u672c\u6316\u6398\u6027\u80fd\u3002", "conclusion": "\u591a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93\u7684\u5bf9\u6bd4\u5b66\u4e60\u662f\u63d0\u5347\u8de8\u8bed\u8a00\u5bf9\u9f50\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u591a\u8bed\u8a00\u8868\u793a\u8d28\u91cf\uff0c\u5373\u4f7f\u5728\u5df2\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4e0a\u4e5f\u80fd\u5e26\u6765\u989d\u5916\u6536\u76ca\u3002\u591a\u8bed\u8a00\u8de8\u8bed\u8a00\u76d1\u7763\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.22094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22094", "abs": "https://arxiv.org/abs/2602.22094", "authors": ["Nguyen Cong Nhat Le", "John G. Rogers", "Claire N. Bonial", "Neil T. Dantam"], "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning", "comment": "16 pages, 5 figures. Submitted to 17th World Symposium on the Algorithmic Foundations of Robotics (WAFR) on 01/14/2026", "summary": "Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePetri\u7f51\u53ef\u8fbe\u6027\u677e\u5f1b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9c81\u68d2\u4e0d\u53d8\u5f0f\u5408\u6210\u3001\u9ad8\u6548\u76ee\u6807\u4e0d\u53ef\u8fbe\u68c0\u6d4b\u548c\u63d0\u4f9b\u4e0d\u53ef\u884c\u6027\u89e3\u91ca\uff0c\u652f\u6301\u76ee\u6807\u548c\u7ea6\u675f\u7684\u589e\u91cf\u66f4\u65b0\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u8ba1\u5212\u5e38\u56e0\u60c5\u51b5\u53d8\u5316\u6216\u8ba4\u77e5\u66f4\u65b0\u800c\u9700\u8981\u8c03\u6574\uff0c\u6709\u65f6\u751a\u81f3\u4e0d\u5b58\u5728\u53ef\u884c\u8ba1\u5212\u3002\u8bc6\u522b\u4e0d\u53ef\u884c\u6027\u6709\u52a9\u4e8e\u8c03\u6574\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53ef\u884c\u60c5\u51b5\u4e0b\u7684\u9ad8\u6548\u4e00\u6b21\u6027\u89c4\u5212\uff0c\u7f3a\u4e4f\u5bf9\u9886\u57df\u66f4\u65b0\u548c\u4e0d\u53ef\u884c\u6027\u68c0\u6d4b\u7684\u652f\u6301\u3002", "method": "\u91c7\u7528Petri\u7f51\u53ef\u8fbe\u6027\u677e\u5f1b\u6280\u672f\uff0c\u5b9e\u73b0\u9c81\u68d2\u4e0d\u53d8\u5f0f\u5408\u6210\uff1b\u7ed3\u5408\u589e\u91cf\u7ea6\u675f\u6c42\u89e3\u5668\uff0c\u652f\u6301\u76ee\u6807\u548c\u7ea6\u675f\u7684\u589e\u91cf\u66f4\u65b0\uff1b\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u76ee\u6807\u4e0d\u53ef\u8fbe\u60c5\u51b5\u5e76\u63d0\u4f9b\u89e3\u91ca\u3002", "result": "\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7cfb\u7edf\u751f\u6210\u76f8\u5f53\u6570\u91cf\u7684\u4e0d\u53d8\u5f0f\uff0c\u68c0\u6d4b\u5230\u6700\u591a2\u500d\u7684\u4e0d\u53ef\u884c\u60c5\u51b5\uff1b\u5728\u4e00\u6b21\u6027\u89c4\u5212\u4e2d\u8868\u73b0\u76f8\u5f53\uff0c\u5728\u987a\u5e8f\u8ba1\u5212\u66f4\u65b0\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684Petri\u7f51\u53ef\u8fbe\u6027\u677e\u5f1b\u65b9\u6cd5\u6709\u6548\u652f\u6301\u9c81\u68d2\u4e0d\u53d8\u5f0f\u5408\u6210\u3001\u4e0d\u53ef\u884c\u6027\u68c0\u6d4b\u548c\u89e3\u91ca\uff0c\u589e\u91cf\u7ea6\u675f\u6c42\u89e3\u5668\u4f7f\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5904\u7406\u76ee\u6807\u548c\u7ea6\u675f\u66f4\u65b0\uff0c\u4e3a\u52a8\u6001\u89c4\u5212\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21226", "abs": "https://arxiv.org/abs/2602.21226", "authors": ["Ezieddin Elmahjub", "Junaid Qadir", "Abdullah Mushtaq", "Rafay Naeem", "Ibrahim Ghaznavi", "Waleed Iqbal"], "title": "IslamicLegalBench: Evaluating LLMs Knowledge and Reasoning of Islamic Law Across 1,200 Years of Islamic Pluralist Legal Traditions", "comment": "This manuscript has been submitted for review to Artificial Intelligence \\& Law", "summary": "As millions of Muslims turn to LLMs like GPT, Claude, and DeepSeek for religious guidance, a critical question arises: Can these AI systems reliably reason about Islamic law? We introduce IslamicLegalBench, the first benchmark evaluating LLMs across seven schools of Islamic jurisprudence, with 718 instances covering 13 tasks of varying complexity. Evaluation of nine state-of-the-art models reveals major limitations: the best model achieves only 68% correctness with 21% hallucination, while several models fall below 35% correctness and exceed 55% hallucination. Few-shot prompting provides minimal gains, improving only 2 of 9 models by >1%. Moderate-complexity tasks requiring exact knowledge show the highest errors, whereas high-complexity tasks display apparent competence through semantic reasoning. False premise detection indicates risky sycophancy, with 6 of 9 models accepting misleading assumptions at rates above 40%. These results highlight that prompt-based methods cannot compensate for missing foundational knowledge. IslamicLegalBench offers the first systematic framework to evaluate Islamic legal reasoning in AI, revealing critical gaps in tools increasingly relied on for spiritual guidance.", "AI": {"tldr": "\u4f0a\u65af\u5170\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5f53\u524dLLMs\u5728\u4f0a\u65af\u5170\u6cd5\u5b66\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u6700\u4f73\u6a21\u578b\u6b63\u786e\u7387\u4ec568%\uff0c\u5e7b\u89c9\u738721%\uff0c\u65e0\u6cd5\u53ef\u9760\u63d0\u4f9b\u5b97\u6559\u6307\u5bfc", "motivation": "\u968f\u7740\u6570\u767e\u4e07\u7a46\u65af\u6797\u4f7f\u7528LLMs\u5bfb\u6c42\u5b97\u6559\u6307\u5bfc\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9bAI\u7cfb\u7edf\u80fd\u5426\u53ef\u9760\u5730\u8fdb\u884c\u4f0a\u65af\u5170\u6cd5\u5f8b\u63a8\u7406\uff0c\u786e\u4fdd\u63d0\u4f9b\u51c6\u786e\u7684\u6cd5\u5f8b\u6307\u5bfc", "method": "\u5f15\u5165\u4f0a\u65af\u5170\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e03\u4e2a\u4f0a\u65af\u5170\u6cd5\u5b66\u6d41\u6d3e\uff0c\u5305\u542b718\u4e2a\u5b9e\u4f8b\u548c13\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e5d\u4e2a\u6700\u5148\u8fdb\u7684LLMs", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u91cd\u5927\u5c40\u9650\u6027\uff1a\u6700\u4f73\u6a21\u578b\u6b63\u786e\u7387\u4ec568%\uff0c\u5e7b\u89c9\u738721%\uff1b\u591a\u4e2a\u6a21\u578b\u6b63\u786e\u7387\u4f4e\u4e8e35%\uff0c\u5e7b\u89c9\u7387\u8d85\u8fc755%\uff1bfew-shot\u63d0\u793a\u6548\u679c\u6709\u9650\uff1b\u4e2d\u7b49\u590d\u6742\u5ea6\u4efb\u52a1\u9519\u8bef\u7387\u6700\u9ad8\uff1b\u865a\u5047\u524d\u63d0\u68c0\u6d4b\u663e\u793a\u5371\u9669\u987a\u4ece\u6027", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u65e0\u6cd5\u5f25\u8865\u57fa\u7840\u77e5\u8bc6\u7684\u7f3a\u5931\uff0c\u4f0a\u65af\u5170\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30AI\u4f0a\u65af\u5170\u6cd5\u5f8b\u63a8\u7406\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u65e5\u76ca\u4f9d\u8d56\u7cbe\u795e\u6307\u5bfc\u5de5\u5177\u4e2d\u7684\u5173\u952e\u5dee\u8ddd"}}
{"id": "2602.21227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21227", "abs": "https://arxiv.org/abs/2602.21227", "authors": ["Caiqi Zhang", "Menglin Xia", "Xuchao Zhang", "Daniel Madrigal", "Ankur Mallick", "Samuel Kessler", "Victor Ruehle", "Saravan Rajmohan"], "title": "Budget-Aware Agentic Routing via Boundary-Guided Training", "comment": null, "summary": "As large language models (LLMs) evolve into autonomous agents that execute long-horizon workflows, invoking a high-capability model at every step becomes economically unsustainable. While model routing is effective for single-turn queries, agentic routing is a sequential, path-dependent problem: early mistakes compound, feedback is often at the end of the episode, and deployments often demand strict per-task spending limits. We propose Budget-Aware Agentic Routing, which selects between a cheap and an expensive model at each step to optimize the cost--success frontier and to operate under strict per-task budgets. We propose Boundary-Guided Training, which leverages two boundary policies (always-small vs.\\ always-large) to build a difficulty taxonomy and to anchor learning under sparse rewards. Our approach warms start with boundary-guided SFT data synthesis via stratified sampling of cost-efficient trajectories, then applies Boundary-Guided Policy Optimization (BoPO), combining boundary-relative rewards with a reference-guided advantage to avoid degenerate cheap-failure solutions. Experiment results show that our method improves the efficiency frontier, matching strong routing baselines at substantially lower cost while demonstrating generalization to strict inference-time budget constraints. Overall, our work establishes a foundational framework for agentic routing, shifting the paradigm from static model selection to dynamic, budget-aware sequential decision-making.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u7b97\u611f\u77e5\u7684\u667a\u80fd\u4f53\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u6b65\u9aa4\u9009\u62e9\u5ec9\u4ef7\u6216\u6602\u8d35\u6a21\u578b\u6765\u4f18\u5316\u6210\u672c-\u6210\u529f\u7387\u8fb9\u754c\uff0c\u5e76\u5728\u4e25\u683c\u7684\u4efb\u52a1\u9884\u7b97\u4e0b\u8fd0\u884c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6f14\u53d8\u4e3a\u6267\u884c\u957f\u6d41\u7a0b\u5de5\u4f5c\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u5728\u6bcf\u4e2a\u6b65\u9aa4\u90fd\u8c03\u7528\u9ad8\u80fd\u529b\u6a21\u578b\u5728\u7ecf\u6d4e\u4e0a\u4e0d\u53ef\u6301\u7eed\u3002\u867d\u7136\u6a21\u578b\u8def\u7531\u5bf9\u5355\u8f6e\u67e5\u8be2\u6709\u6548\uff0c\u4f46\u667a\u80fd\u4f53\u8def\u7531\u662f\u987a\u5e8f\u7684\u3001\u8def\u5f84\u4f9d\u8d56\u7684\u95ee\u9898\uff1a\u65e9\u671f\u9519\u8bef\u4f1a\u7d2f\u79ef\uff0c\u53cd\u9988\u901a\u5e38\u5728\u6d41\u7a0b\u7ed3\u675f\u65f6\u624d\u51fa\u73b0\uff0c\u4e14\u90e8\u7f72\u901a\u5e38\u8981\u6c42\u4e25\u683c\u7684\u4efb\u52a1\u9884\u7b97\u9650\u5236\u3002", "method": "\u63d0\u51fa\u8fb9\u754c\u5f15\u5bfc\u8bad\u7ec3\uff0c\u5229\u7528\u4e24\u4e2a\u8fb9\u754c\u7b56\u7565\uff08\u59cb\u7ec8\u5c0f\u6a21\u578b vs. \u59cb\u7ec8\u5927\u6a21\u578b\uff09\u6784\u5efa\u96be\u5ea6\u5206\u7c7b\u5e76\u951a\u5b9a\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u5b66\u4e60\u3002\u65b9\u6cd5\u5305\u62ec\uff1a1) \u901a\u8fc7\u5206\u5c42\u91c7\u6837\u6210\u672c\u9ad8\u6548\u8f68\u8ff9\u8fdb\u884c\u8fb9\u754c\u5f15\u5bfc\u7684SFT\u6570\u636e\u5408\u6210\u9884\u70ed\uff1b2) \u8fb9\u754c\u5f15\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u7ed3\u5408\u8fb9\u754c\u76f8\u5bf9\u5956\u52b1\u548c\u53c2\u8003\u5f15\u5bfc\u4f18\u52bf\u4ee5\u907f\u514d\u5ec9\u4ef7\u7684\u5931\u8d25\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6539\u8fdb\u4e86\u6548\u7387\u8fb9\u754c\uff0c\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u5339\u914d\u5f3a\u5927\u7684\u8def\u7531\u57fa\u7ebf\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5728\u4e25\u683c\u63a8\u7406\u65f6\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u8def\u7531\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5c06\u8303\u5f0f\u4ece\u9759\u6001\u6a21\u578b\u9009\u62e9\u8f6c\u5411\u52a8\u6001\u7684\u3001\u9884\u7b97\u611f\u77e5\u7684\u987a\u5e8f\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2602.21228", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21228", "abs": "https://arxiv.org/abs/2602.21228", "authors": ["Yuancheng Yang", "Lin Yang", "Xu Wang", "Chao Tong", "Haihua Yang"], "title": "ImpRIF: Stronger Implicit Reasoning Leads to Better Complex Instruction Following", "comment": null, "summary": "As applications of large language models (LLMs) become increasingly complex, the demand for robust complex instruction following capabilities is growing accordingly. We argue that a thorough understanding of the instruction itself, especially the latent reasoning structure embedded between the lines, is crucial for improving instruction following. Therefore we target complex instructions that involve implicit reasoning, intricate logical relations, and multi-constraint dependencies. We propose ImpRIF, a method to enhance LLMs' understanding of implicit reasoning instructions, thereby improving its ability to follow complex instructions. We formalize such instructions as verifiable reasoning graphs, enabling programmatic verification and graph-driven chain-of-thought reasoning. Based on this formulation, we synthesize large-scale single- and multi-turn data, propose fine-tuning with graph reasoning, and apply reinforcement learning to explicitly train models to reason along the graph. On five complex instruction following benchmarks, our models substantially outperform their base models. These results demonstrate that enhancing implicit reasoning capabilities can significantly improve complex instruction following. This project will be open-sourced in the near future.", "AI": {"tldr": "ImpRIF\u65b9\u6cd5\u901a\u8fc7\u5c06\u590d\u6742\u6307\u4ee4\u5f62\u5f0f\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u56fe\uff0c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9690\u542b\u63a8\u7406\u6307\u4ee4\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u65e5\u76ca\u590d\u6742\uff0c\u5bf9\u9c81\u68d2\u7684\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u9700\u6c42\u589e\u957f\u3002\u4f5c\u8005\u8ba4\u4e3a\u6df1\u5165\u7406\u89e3\u6307\u4ee4\u672c\u8eab\uff0c\u7279\u522b\u662f\u9690\u542b\u5728\u6307\u4ee4\u4e2d\u7684\u6f5c\u5728\u63a8\u7406\u7ed3\u6784\uff0c\u5bf9\u4e8e\u6539\u8fdb\u6307\u4ee4\u8ddf\u968f\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\u9488\u5bf9\u6d89\u53ca\u9690\u542b\u63a8\u7406\u3001\u590d\u6742\u903b\u8f91\u5173\u7cfb\u548c\u591a\u7ea6\u675f\u4f9d\u8d56\u7684\u590d\u6742\u6307\u4ee4\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u63d0\u51faImpRIF\u65b9\u6cd5\uff0c\u5c06\u9690\u542b\u63a8\u7406\u6307\u4ee4\u5f62\u5f0f\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u56fe\uff0c\u652f\u6301\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u548c\u56fe\u9a71\u52a8\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u3002\u57fa\u4e8e\u6b64\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5408\u6210\u5927\u89c4\u6a21\u5355\u8f6e\u548c\u591a\u8f6e\u6570\u636e\uff0c\u63d0\u51fa\u57fa\u4e8e\u56fe\u63a8\u7406\u7684\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u660e\u786e\u8bad\u7ec3\u6a21\u578b\u6cbf\u56fe\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u4e94\u4e2a\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5176\u57fa\u7840\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\u589e\u5f3a\u9690\u542b\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u663e\u8457\u6539\u5584\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u3002", "conclusion": "\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9690\u542b\u63a8\u7406\u7684\u7406\u89e3\u80fd\u529b\u662f\u63d0\u5347\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002\u901a\u8fc7\u5c06\u6307\u4ee4\u5f62\u5f0f\u5316\u4e3a\u63a8\u7406\u56fe\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002\u8be5\u9879\u76ee\u5c06\u5728\u8fd1\u671f\u5f00\u6e90\u3002"}}
{"id": "2602.21346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21346", "abs": "https://arxiv.org/abs/2602.21346", "authors": ["Mengxuan Hu", "Vivek V. Datla", "Anoop Kumar", "Zihan Guan", "Sheng Li", "Alfy Samuel", "Daben Liu"], "title": "Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment", "comment": null, "summary": "Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u63a8\u7406\u611f\u77e5\u7684\u540e\u8bad\u7ec3\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u6784\u5efaCoT\u5fae\u8c03\u6570\u636e\u96c6\u548c\u5f15\u5165Alignment-Weighted DPO\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u95f4\u63a5\u6216\u6b3a\u9a97\u6027\u63aa\u8f9e\u7684\u8d8a\u72f1\u653b\u51fb\u3002", "motivation": "\u5c3d\u7ba1SFT\u3001RLHF\u548cDPO\u7b49\u5bf9\u9f50\u6280\u672f\u63d0\u9ad8\u4e86LLMs\u7684\u5b89\u5168\u6027\uff0c\u4f46\u6a21\u578b\u4ecd\u6613\u53d7\u901a\u8fc7\u95f4\u63a5\u6216\u6b3a\u9a97\u6027\u63aa\u8f9e\u4f2a\u88c5\u6709\u5bb3\u610f\u56fe\u7684\u8d8a\u72f1\u653b\u51fb\u3002\u4f5c\u8005\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u5b9e\u8bc1\u53d1\u73b0\uff0c\u8fd9\u79cd\u8106\u5f31\u6027\u6e90\u4e8e\u7f3a\u4e4f\u6df1\u5ea6\u63a8\u7406\u7684\u6d45\u5c42\u5bf9\u9f50\u673a\u5236\uff0c\u6a21\u578b\u7ecf\u5e38\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\u4f46\u5e76\u672a\u771f\u6b63\u7406\u89e3\u5176\u4e3a\u4f55\u6709\u5bb3\u3002", "method": "1) \u6784\u5efa\u5e76\u53d1\u5e03\u65b0\u9896\u7684CoT\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5305\u542b\u5b9e\u7528\u5bfc\u5411\u548c\u5b89\u5168\u5173\u952e\u63d0\u793a\u53ca\u5176\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff1b2) \u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u9f13\u52b1\u6a21\u578b\u4ea7\u751f\u57fa\u4e8e\u63a8\u7406\u7684\u539f\u5219\u6027\u62d2\u7edd\uff1b3) \u53d7CoT\u5fae\u8c03\u5931\u8d25\u6a21\u5f0f\u542f\u53d1\uff0c\u5f15\u5165Alignment-Weighted DPO\uff0c\u901a\u8fc7\u5bf9\u63a8\u7406\u548c\u6700\u7ec8\u7b54\u6848\u6bb5\u5206\u914d\u4e0d\u540c\u504f\u597d\u6743\u91cd\uff0c\u9488\u5bf9\u8f93\u51fa\u4e2d\u6700\u6709\u95ee\u9898\u7684\u90e8\u5206\u8fdb\u884c\u66f4\u7ec6\u7c92\u5ea6\u7684\u76ee\u6807\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u5b89\u5168\u548c\u5b9e\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6574\u4f53\u6a21\u578b\u5b9e\u7528\u6027\u7684\u540c\u65f6\uff0c\u4e00\u81f4\u63d0\u9ad8\u4e86\u5bf9\u9f50\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u6807\u51c6SFT\u57fa\u7ebf\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u591a\u6837\u5316\u8d8a\u72f1\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u611f\u77e5\u7684\u540e\u8bad\u7ec3\u589e\u5f3a\u5bf9\u9f50\uff0c\u7279\u522b\u662f\u7ed3\u5408CoT\u5fae\u8c03\u548cAlignment-Weighted DPO\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8LLMs\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\uff0c\u4e3a\u89e3\u51b3\u6d45\u5c42\u5bf9\u9f50\u673a\u5236\u5bfc\u81f4\u7684\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.21374", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21374", "abs": "https://arxiv.org/abs/2602.21374", "authors": ["Mohammadreza Ghaffarzadeh-Esfahani", "Nahid Yousefian", "Ebrahim Heidari-Farsani", "Ali Akbar Omidvarian", "Sepehr Ghahraei", "Atena Farangi", "AmirBahador Boroumand"], "title": "Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages", "comment": "16 pages, 3 figures, 2 supplementary files", "summary": "Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.", "AI": {"tldr": "\u8bc4\u4f30\u6ce2\u65af\u8bed\u533b\u7597\u8f6c\u5f55\u672c\u4e34\u5e8a\u4fe1\u606f\u63d0\u53d6\u7684\u4e24\u6b65\u6d41\u7a0b\uff1a\u5148\u7528Aya-expanse-8B\u7ffb\u8bd1\u4e3a\u82f1\u8bed\uff0c\u518d\u7528\u4e94\u4e2a\u5f00\u6e90\u5c0f\u8bed\u8a00\u6a21\u578b\u63d0\u53d613\u4e2a\u4e34\u5e8a\u7279\u5f81\u3002Qwen2.5-7B-Instruct\u8868\u73b0\u6700\u4f73\uff0c\u5927\u6a21\u578b\u4f18\u4e8e\u5c0f\u6a21\u578b\uff0c\u7ffb\u8bd1\u63d0\u5347\u654f\u611f\u5ea6\u4f46\u7565\u964d\u7279\u5f02\u6027\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u6ce2\u65af\u8bed\uff09\u533b\u7597\u8f6c\u5f55\u672c\u4e34\u5e8a\u4fe1\u606f\u63d0\u53d6\u7684\u6311\u6218\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u548c\u6807\u6ce8\u8d44\u6e90\u6709\u9650\u7684\u533b\u7597NLP\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6d41\u7a0b\uff1a1) \u4f7f\u7528Aya-expanse-8B\u5c06\u6ce2\u65af\u8bed\u8f6c\u5f55\u672c\u7ffb\u8bd1\u4e3a\u82f1\u8bed\uff1b2) \u4f7f\u7528\u4e94\u4e2a\u5f00\u6e90\u5c0f\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-7B-Instruct\u3001Llama-3.1-8B-Instruct\u3001Llama-3.2-3B-Instruct\u3001Qwen2.5-1.5B-Instruct\u3001Gemma-3-1B-it\uff09\u8fdb\u884c13\u4e2a\u4e34\u5e8a\u7279\u5f81\u7684\u4e8c\u5143\u63d0\u53d6\u3002\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u5b8f\u5e73\u5747F1\u5206\u6570\u3001\u9a6c\u4fee\u65af\u76f8\u5173\u7cfb\u6570\u3001\u654f\u611f\u5ea6\u548c\u7279\u5f02\u6027\u3002", "result": "Qwen2.5-7B-Instruct\u8868\u73b0\u6700\u4f73\uff08\u4e2d\u4f4d\u6570\u5b8fF1\uff1a0.899\uff1bMCC\uff1a0.797\uff09\uff0cGemma-3-1B-it\u8868\u73b0\u6700\u5f31\u3002\u5927\u6a21\u578b\uff087B-8B\u53c2\u6570\uff09\u5728\u654f\u611f\u5ea6\u548cMCC\u4e0a\u6301\u7eed\u4f18\u4e8e\u5c0f\u6a21\u578b\u3002\u7ffb\u8bd1\u6ce2\u65af\u8bed\u5230\u82f1\u8bed\u63d0\u9ad8\u4e86\u654f\u611f\u5ea6\uff0c\u51cf\u5c11\u4e86\u7f3a\u5931\u8f93\u51fa\uff0c\u63d0\u5347\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u9c81\u68d2\u6027\u6307\u6807\uff0c\u4f46\u7565\u5fae\u964d\u4f4e\u4e86\u7279\u5f02\u6027\u548c\u7cbe\u786e\u5ea6\u3002\u751f\u7406\u75c7\u72b6\u63d0\u53d6\u53ef\u9760\uff0c\u800c\u5fc3\u7406\u62b1\u6028\u3001\u884c\u653f\u8bf7\u6c42\u548c\u590d\u6742\u8eaf\u4f53\u7279\u5f81\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u8bed\u8a00\u4e34\u5e8aNLP\u73af\u5883\u4e2d\u90e8\u7f72\u5f00\u6e90\u5c0f\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u84dd\u56fe\uff0c\u5f3a\u8c03\u4e86\u5728\u654f\u611f\u533b\u7597\u5e94\u7528\u4e2d\u8054\u5408\u4f18\u5316\u6a21\u578b\u89c4\u6a21\u548c\u8f93\u5165\u8bed\u8a00\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.21377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21377", "abs": "https://arxiv.org/abs/2602.21377", "authors": ["Felix Schneider", "Maria Gogolev", "Sven Sickert", "Joachim Denzler"], "title": "Beyond Subtokens: A Rich Character Embedding for Low-resource and Morphologically Complex Languages", "comment": "12 content pages, 2 figures, 8 tables, one example textbox", "summary": "Tokenization and sub-tokenization based models like word2vec, BERT and the GPTs are the state-of-the-art in natural language processing. Typically, these approaches have limitations with respect to their input representation. They fail to fully capture orthographic similarities and morphological variations, especially in highly inflected and under-resource languages. To mitigate this problem, we propose to computes word vectors directly from character strings, integrating both semantic and syntactic information. We denote this transformer-based approach Rich Character Embeddings (RCE). Furthermore, we propose a hybrid model that combines transformer and convolutional mechanisms. Both vector representations can be used as a drop-in replacement for dictionary- and subtoken-based word embeddings in existing model architectures. It has the potential to improve performance for both large context-based language models like BERT and small models like word2vec for under-resourced and morphologically rich languages. We evaluate our approach on various tasks like the SWAG, declension prediction for inflected languages, metaphor and chiasmus detection for various languages. Our experiments show that it outperforms traditional token-based approaches on limited data using OddOneOut and TopK metrics.", "AI": {"tldr": "\u63d0\u51faRCE\uff08Rich Character Embeddings\uff09\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u5b57\u7b26\u5b57\u7b26\u4e32\u8ba1\u7b97\u8bcd\u5411\u91cf\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u53e5\u6cd5\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408transformer\u548c\u5377\u79ef\u673a\u5236\u7684\u6a21\u578b\uff0c\u4f5c\u4e3a\u73b0\u6709\u6a21\u578b\u67b6\u6784\u4e2d\u5b57\u5178\u548c\u5b50\u8bcd\u5d4c\u5165\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u8bcd\u548c\u5b50\u8bcd\u5206\u8bcd\u7684\u6a21\u578b\uff08\u5982word2vec\u3001BERT\u3001GPT\uff09\u5728\u8f93\u5165\u8868\u793a\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6b63\u5b57\u6cd5\u76f8\u4f3c\u6027\u548c\u5f62\u6001\u53d8\u5316\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5ea6\u5c48\u6298\u548c\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8etransformer\u7684RCE\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u5b57\u7b26\u5b57\u7b26\u4e32\u8ba1\u7b97\u8bcd\u5411\u91cf\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fa\u7ed3\u5408transformer\u548c\u5377\u79ef\u673a\u5236\u7684\u6df7\u5408\u6a21\u578b\uff1b\u4e24\u79cd\u5411\u91cf\u8868\u793a\u5747\u53ef\u4f5c\u4e3a\u73b0\u6709\u6a21\u578b\u67b6\u6784\u4e2d\u5b57\u5178\u548c\u5b50\u8bcd\u5d4c\u5165\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728SWAG\u3001\u5c48\u6298\u8bed\u8a00\u7684\u53d8\u683c\u9884\u6d4b\u3001\u591a\u79cd\u8bed\u8a00\u7684\u9690\u55bb\u548c\u4ea4\u9519\u6cd5\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u6709\u9650\u6570\u636e\u4e0b\u4f7f\u7528OddOneOut\u548cTopK\u6307\u6807\u65f6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u5206\u8bcd\u7684\u65b9\u6849\u3002", "conclusion": "RCE\u65b9\u6cd5\u6709\u6f5c\u529b\u6539\u5584BERT\u7b49\u5927\u578b\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u548cword2vec\u7b49\u5c0f\u578b\u6a21\u578b\u5728\u8d44\u6e90\u532e\u4e4f\u548c\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bcd\u8868\u793a\u3002"}}
{"id": "2602.21379", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21379", "abs": "https://arxiv.org/abs/2602.21379", "authors": ["Daniel Tamayo", "I\u00f1aki Lacunza", "Paula Rivera-Hidalgo", "Severino Da Dalt", "Javier Aula-Blasco", "Aitor Gonzalez-Agirre", "Marta Villegas"], "title": "MrBERT: Modern Multilingual Encoders via Vocabulary, Domain, and Dimensional Adaptation", "comment": "24 pages, 14 tables and 4 figures", "summary": "We introduce MrBERT, a family of 150M-300M parameter encoders built on the ModernBERT architecture and pre-trained on 35 languages and code. Through targeted adaptation, this model family achieves state-of-the-art results on Catalan- and Spanish-specific tasks, while establishing robust performance across specialized biomedical and legal domains. To bridge the gap between research and production, we incorporate Matryoshka Representation Learning (MRL), enabling flexible vector sizing that significantly reduces inference and storage costs. Ultimately, the MrBERT family demonstrates that modern encoder architectures can be optimized for both localized linguistic excellence and efficient, high-stakes domain specialization. We open source the complete model family on Huggingface.", "AI": {"tldr": "MrBERT\u662f\u57fa\u4e8eModernBERT\u67b6\u6784\u7684150M-300M\u53c2\u6570\u7f16\u7801\u5668\u5bb6\u65cf\uff0c\u652f\u630135\u79cd\u8bed\u8a00\u548c\u4ee3\u7801\uff0c\u5728\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u548c\u897f\u73ed\u7259\u8bed\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u5e76\u5728\u751f\u7269\u533b\u5b66\u548c\u6cd5\u5f8b\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7MRL\u6280\u672f\u5b9e\u73b0\u7075\u6d3b\u7684\u5411\u91cf\u5927\u5c0f\u8c03\u6574\u4ee5\u964d\u4f4e\u63a8\u7406\u548c\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u7f16\u7801\u5668\u67b6\u6784\u5728\u7279\u5b9a\u8bed\u8a00\u4efb\u52a1\u548c\u9886\u57df\u4e13\u4e1a\u5316\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u5b58\u50a8\u6210\u672c\uff0c\u5f25\u5408\u7814\u7a76\u4e0e\u751f\u4ea7\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8eModernBERT\u67b6\u6784\u6784\u5efa150M-300M\u53c2\u6570\u7f16\u7801\u5668\u5bb6\u65cf\uff0c\u572835\u79cd\u8bed\u8a00\u548c\u4ee3\u7801\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u9488\u5bf9\u6027\u9002\u5e94\u7b56\u7565\u4f18\u5316\u7279\u5b9a\u8bed\u8a00\u4efb\u52a1\uff0c\u96c6\u6210Matryoshka\u8868\u793a\u5b66\u4e60\uff08MRL\uff09\u6280\u672f\u5b9e\u73b0\u7075\u6d3b\u7684\u5411\u91cf\u5927\u5c0f\u8c03\u6574\u3002", "result": "\u5728\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u548c\u897f\u73ed\u7259\u8bed\u7279\u5b9a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u751f\u7269\u533b\u5b66\u548c\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u5efa\u7acb\u7a33\u5065\u6027\u80fd\uff0c\u901a\u8fc7MRL\u6280\u672f\u663e\u8457\u964d\u4f4e\u63a8\u7406\u548c\u5b58\u50a8\u6210\u672c\uff0c\u6a21\u578b\u5bb6\u65cf\u5df2\u5728Huggingface\u5f00\u6e90\u3002", "conclusion": "\u73b0\u4ee3\u7f16\u7801\u5668\u67b6\u6784\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u672c\u5730\u5316\u8bed\u8a00\u5353\u8d8a\u6027\u548c\u9ad8\u6548\u7684\u9ad8\u98ce\u9669\u9886\u57df\u4e13\u4e1a\u5316\uff0cMRL\u6280\u672f\u4e3a\u751f\u4ea7\u90e8\u7f72\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u67b6\u6784\u4f18\u5316\u7684\u53cc\u91cd\u6f5c\u529b\u3002"}}
{"id": "2602.21461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21461", "abs": "https://arxiv.org/abs/2602.21461", "authors": ["Xiaoke Huang", "Bhavul Gauri", "Kam Woh Ng", "Tony Ng", "Mengmeng Xu", "Zhiheng Liu", "Weiming Ren", "Zhaochong An", "Zijian Zhou", "Haonan Qiu", "Yuyin Zhou", "Sen He", "Ziheng Wang", "Tao Xiang", "Xiao Han"], "title": "VecGlypher: Unified Vector Glyph Generation with Language Models", "comment": "Accepted to CVPR'26. Project page: https://xk-huang.github.io/VecGlypher/", "summary": "Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.", "AI": {"tldr": "VecGlypher\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u4ece\u6587\u672c\u63cf\u8ff0\u6216\u56fe\u50cf\u793a\u4f8b\u751f\u6210\u9ad8\u8d28\u91cf\u77e2\u91cf\u5b57\u5f62\uff0c\u65e0\u9700\u5149\u6805\u4e2d\u95f4\u5904\u7406\uff0c\u4ea7\u751f\u53ef\u7f16\u8f91\u7684SVG\u8def\u5f84\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u5b57\u4f53\u751f\u6210\u7ba1\u9053\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u793a\u4f8b\u8868\u548c\u5149\u6805\u5230\u77e2\u91cf\u7684\u540e\u5904\u7406\uff0c\u9650\u5236\u4e86\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u7f16\u8f91\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u76f4\u63a5\u4ece\u6587\u672c\u6216\u56fe\u50cf\u751f\u6210\u53ef\u7f16\u8f91\u77e2\u91cf\u5b57\u5f62\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u5927\u89c4\u6a21\u566a\u58f0Envato\u5b57\u4f53\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u638c\u63e1SVG\u8bed\u6cd5\u548c\u957f\u5e8f\u5217\u51e0\u4f55\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u4e13\u5bb6\u6807\u6ce8\u7684Google Fonts\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u5bf9\u9f50\u8bed\u8a00\u3001\u56fe\u50cf\u4e0e\u51e0\u4f55\u3002\u4f7f\u7528\u7edd\u5bf9\u5750\u6807\u5e8f\u5217\u5316\u3001\u8def\u5f84\u89c4\u8303\u5316\u3001\u5750\u6807\u91cf\u5316\u7b49\u6280\u672f\u786e\u4fdd\u7a33\u5b9a\u89e3\u7801\u3002", "result": "\u5728\u8de8\u5bb6\u65cfOOD\u8bc4\u4f30\u4e2d\uff0cVecGlypher\u5728\u7eaf\u6587\u672c\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u901a\u7528LLM\u548c\u4e13\u7528\u77e2\u91cf\u5b57\u4f53\u57fa\u7ebf\uff0c\u56fe\u50cf\u53c2\u8003\u751f\u6210\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u660e\u663e\u4f18\u4e8eDeepVecFont-v2\u548cDualVector\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u6a21\u578b\u89c4\u6a21\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "VecGlypher\u901a\u8fc7\u5141\u8bb8\u7528\u6237\u7528\u6587\u5b57\u6216\u793a\u4f8b\u8bbe\u8ba1\u5b57\u4f53\uff0c\u964d\u4f4e\u4e86\u5b57\u4f53\u521b\u5efa\u95e8\u69db\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u8bbe\u8ba1\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2602.21485", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.21485", "abs": "https://arxiv.org/abs/2602.21485", "authors": ["Deja Dunlap", "R. Thomas McCoy"], "title": "Evaluating the Usage of African-American Vernacular English in Large Language Models", "comment": null, "summary": "In AI, most evaluations of natural language understanding tasks are conducted in standardized dialects such as Standard American English (SAE). In this work, we investigate how accurately large language models (LLMs) represent African American Vernacular English (AAVE). We analyze three LLMs to compare their usage of AAVE to the usage of humans who natively speak AAVE. We first analyzed interviews from the Corpus of Regional African American Language and TwitterAAE to identify the typical contexts where people use AAVE grammatical features such as ain't. We then prompted the LLMs to produce text in AAVE and compared the model-generated text to human usage patterns. We find that, in many cases, there are substantial differences between AAVE usage in LLMs and humans: LLMs usually underuse and misuse grammatical features characteristic of AAVE. Furthermore, through sentiment analysis and manual inspection, we found that the models replicated stereotypes about African Americans. These results highlight the need for more diversity in training data and the incorporation of fairness methods to mitigate the perpetuation of stereotypes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8868\u793a\u975e\u88d4\u7f8e\u56fd\u4eba\u767d\u8bdd\u82f1\u8bed\uff08AAVE\uff09\u65f6\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u4e0d\u8db3\u6216\u8bef\u7528AAVE\u8bed\u6cd5\u7279\u5f81\uff0c\u5e76\u590d\u5236\u4e86\u5bf9\u975e\u88d4\u7f8e\u56fd\u4eba\u7684\u523b\u677f\u5370\u8c61\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8e\u6807\u51c6\u65b9\u8a00\uff08\u5982\u6807\u51c6\u7f8e\u5f0f\u82f1\u8bed\uff09\uff0c\u7f3a\u4e4f\u5bf9\u975e\u6807\u51c6\u65b9\u8a00\u5982\u975e\u88d4\u7f8e\u56fd\u4eba\u767d\u8bdd\u82f1\u8bed\uff08AAVE\uff09\u7684\u51c6\u786e\u8868\u793a\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u51c6\u786e\u8868\u793aAAVE\uff0c\u5e76\u6bd4\u8f83\u6a21\u578b\u4f7f\u7528\u4e0e\u6bcd\u8bed\u8005\u4f7f\u7528\u7684\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u4e09\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff1a1\uff09\u4f7f\u7528\u533a\u57df\u975e\u88d4\u7f8e\u56fd\u4eba\u8bed\u8a00\u8bed\u6599\u5e93\u548cTwitterAAE\u4e2d\u7684\u4eba\u7c7b\u8bbf\u8c08\u6570\u636e\uff0c\u8bc6\u522b\u4eba\u4eec\u4f7f\u7528AAVE\u8bed\u6cd5\u7279\u5f81\uff08\u5982ain't\uff09\u7684\u5178\u578b\u8bed\u5883\uff1b2\uff09\u63d0\u793a\u6a21\u578b\u751f\u6210AAVE\u6587\u672c\uff1b3\uff09\u5c06\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u4e0e\u4eba\u7c7b\u4f7f\u7528\u6a21\u5f0f\u8fdb\u884c\u6bd4\u8f83\uff1b4\uff09\u901a\u8fc7\u60c5\u611f\u5206\u6790\u548c\u4eba\u5de5\u68c0\u67e5\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728AAVE\u4f7f\u7528\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b2\uff09\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u4e0d\u8db3\u6216\u8bef\u7528AAVE\u7684\u8bed\u6cd5\u7279\u5f81\uff1b3\uff09\u901a\u8fc7\u60c5\u611f\u5206\u6790\u548c\u4eba\u5de5\u68c0\u67e5\u53d1\u73b0\uff0c\u6a21\u578b\u590d\u5236\u4e86\u5bf9\u975e\u88d4\u7f8e\u56fd\u4eba\u7684\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u548c\u91c7\u7528\u516c\u5e73\u6027\u65b9\u6cd5\u6765\u51cf\u8f7b\u523b\u677f\u5370\u8c61\u7684\u5ef6\u7eed\u3002\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8868\u793a\u975e\u6807\u51c6\u65b9\u8a00\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u786e\u4fdd\u66f4\u516c\u5e73\u548c\u51c6\u786e\u7684\u8bed\u8a00\u8868\u793a\u3002"}}
{"id": "2602.21608", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21608", "abs": "https://arxiv.org/abs/2602.21608", "authors": ["Kazi Samin Yasar Alam", "Md Tanbir Chowdhury", "Tamim Ahmed", "Ajwad Abrar", "Md Rafid Haque"], "title": "MixSarc: A Bangla-English Code-Mixed Corpus for Implicit Meaning Identification", "comment": "Under Review", "summary": "Bangla-English code-mixing is widespread across South Asian social media, yet resources for implicit meaning identification in this setting remain scarce. Existing sentiment and sarcasm models largely focus on monolingual English or high-resource languages and struggle with transliteration variation, cultural references, and intra-sentential language switching. To address this gap, we introduce MixSarc, the first publicly available Bangla-English code-mixed corpus for implicit meaning identification. The dataset contains 9,087 manually annotated sentences labeled for humor, sarcasm, offensiveness, and vulgarity. We construct the corpus through targeted social media collection, systematic filtering, and multi-annotator validation. We benchmark transformer-based models and evaluate zero-shot large language models under structured prompting. Results show strong performance on humor detection but substantial degradation on sarcasm, offense, and vulgarity due to class imbalance and pragmatic complexity. Zero-shot models achieve competitive micro-F1 scores but low exact match accuracy. Further analysis reveals that over 42\\% of negative sentiment instances in an external dataset exhibit sarcastic characteristics. MixSarc provides a foundational resource for culturally aware NLP and supports more reliable multi-label modeling in code-mixed environments.", "AI": {"tldr": "MixSarc\u662f\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u5b5f\u52a0\u62c9\u8bed-\u82f1\u8bed\u4ee3\u7801\u6df7\u5408\u8bed\u6599\u5e93\uff0c\u4e13\u95e8\u7528\u4e8e\u9690\u5f0f\u610f\u4e49\u8bc6\u522b\uff0c\u5305\u542b9,087\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u53e5\u5b50\uff0c\u6807\u6ce8\u4e86\u5e7d\u9ed8\u3001\u8bbd\u523a\u3001\u5192\u72af\u6027\u548c\u7c97\u4fd7\u6027\u6807\u7b7e\u3002", "motivation": "\u5357\u4e9a\u793e\u4ea4\u5a92\u4f53\u4e2d\u5b5f\u52a0\u62c9\u8bed-\u82f1\u8bed\u4ee3\u7801\u6df7\u5408\u73b0\u8c61\u666e\u904d\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5e94\u7684\u9690\u5f0f\u610f\u4e49\u8bc6\u522b\u8d44\u6e90\u3002\u73b0\u6709\u60c5\u611f\u548c\u8bbd\u523a\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u5355\u8bed\u82f1\u8bed\u6216\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u96be\u4ee5\u5904\u7406\u8f6c\u5199\u53d8\u4f53\u3001\u6587\u5316\u5f15\u7528\u548c\u53e5\u5185\u8bed\u8a00\u5207\u6362\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u793e\u4ea4\u5a92\u4f53\u6536\u96c6\u3001\u7cfb\u7edf\u8fc7\u6ee4\u548c\u591a\u6807\u6ce8\u8005\u9a8c\u8bc1\u6784\u5efa\u8bed\u6599\u5e93\u3002\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5728\u7ed3\u6784\u5316\u63d0\u793a\u4e0b\u8bc4\u4f30\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u5e7d\u9ed8\u68c0\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bbd\u523a\u3001\u5192\u72af\u548c\u7c97\u4fd7\u6027\u68c0\u6d4b\u4e0a\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bed\u7528\u590d\u6742\u6027\u800c\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002\u96f6\u6837\u672c\u6a21\u578b\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u5fae\u5e73\u5747F1\u5206\u6570\u4f46\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u4f4e\u3002\u5206\u6790\u663e\u793a\u5916\u90e8\u6570\u636e\u96c6\u4e2d\u8d85\u8fc742%\u7684\u8d1f\u9762\u60c5\u611f\u5b9e\u4f8b\u5177\u6709\u8bbd\u523a\u7279\u5f81\u3002", "conclusion": "MixSarc\u4e3a\u6587\u5316\u611f\u77e5\u7684NLP\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u652f\u6301\u5728\u4ee3\u7801\u6df7\u5408\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u53ef\u9760\u7684\u591a\u6807\u7b7e\u5efa\u6a21\u3002"}}
{"id": "2602.21619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21619", "abs": "https://arxiv.org/abs/2602.21619", "authors": ["Muku Akasaka", "Soyeon Caren Han"], "title": "When More Is Less: A Systematic Analysis of Spatial and Commonsense Information for Visual Spatial Reasoning", "comment": "5 pages, 6 figures, Under review", "summary": "Visual spatial reasoning (VSR) remains challenging for modern vision-language models (VLMs), despite advances in multimodal architectures. A common strategy is to inject additional information at inference time, such as explicit spatial cues, external commonsense knowledge, or chain-of-thought (CoT) reasoning instructions. However, it remains unclear when such information genuinely improves reasoning and when it introduces noise. In this paper, we conduct a hypothesis-driven analysis of information injection for VSR across three representative VLMs and two public benchmarks. We examine (i) the type and number of spatial contexts, (ii) the amount and relevance of injected commonsense knowledge, and (iii) the interaction between spatial grounding and CoT prompting. Our results reveal a consistent pattern: more information does not necessarily yield better reasoning. Targeted single spatial cues outperform multi-context aggregation, excessive or weakly relevant commonsense knowledge degrades performance, and CoT prompting improves accuracy only when spatial grounding is sufficiently precise. These findings highlight the importance of selective, task-aligned information injection and provide practical guidance for designing reliable multimodal reasoning pipelines.", "AI": {"tldr": "\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u4e2d\uff0c\u4fe1\u606f\u6ce8\u5165\u5e76\u975e\u8d8a\u591a\u8d8a\u597d\uff0c\u9700\u8981\u9009\u62e9\u6027\u3001\u4efb\u52a1\u5bf9\u9f50\u7684\u4fe1\u606f\u6ce8\u5165\u624d\u80fd\u63d0\u5347\u6027\u80fd", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u67b6\u6784\u6709\u8fdb\u6b65\uff0c\u4f46\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u5bf9\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ecd\u5177\u6311\u6218\u3002\u5e38\u89c1\u7b56\u7565\u662f\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u989d\u5916\u4fe1\u606f\uff08\u7a7a\u95f4\u7ebf\u7d22\u3001\u5e38\u8bc6\u77e5\u8bc6\u3001\u601d\u7ef4\u94fe\u6307\u4ee4\uff09\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u4f55\u65f6\u771f\u6b63\u6539\u5584\u63a8\u7406\u3001\u4f55\u65f6\u5f15\u5165\u566a\u58f0\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5bf9\u4e09\u4e2a\u4ee3\u8868\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u8fdb\u884c\u5047\u8bbe\u9a71\u52a8\u7684\u4fe1\u606f\u6ce8\u5165\u5206\u6790\uff0c\u8003\u5bdf\uff1a(i)\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u7c7b\u578b\u548c\u6570\u91cf\uff0c(ii)\u6ce8\u5165\u5e38\u8bc6\u77e5\u8bc6\u7684\u6570\u91cf\u548c\u76f8\u5173\u6027\uff0c(iii)\u7a7a\u95f4\u57fa\u7840\u4e0e\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u4e00\u81f4\u6a21\u5f0f\uff1a\u66f4\u591a\u4fe1\u606f\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u63a8\u7406\u3002\u9488\u5bf9\u6027\u5355\u4e00\u7a7a\u95f4\u7ebf\u7d22\u4f18\u4e8e\u591a\u4e0a\u4e0b\u6587\u805a\u5408\uff1b\u8fc7\u591a\u6216\u5f31\u76f8\u5173\u5e38\u8bc6\u77e5\u8bc6\u4f1a\u964d\u4f4e\u6027\u80fd\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u4ec5\u5728\u7a7a\u95f4\u57fa\u7840\u8db3\u591f\u7cbe\u786e\u65f6\u624d\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u5f3a\u8c03\u9009\u62e9\u6027\u3001\u4efb\u52a1\u5bf9\u9f50\u4fe1\u606f\u6ce8\u5165\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8bbe\u8ba1\u53ef\u9760\u591a\u6a21\u6001\u63a8\u7406\u6d41\u7a0b\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.21628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21628", "abs": "https://arxiv.org/abs/2602.21628", "authors": ["Yukun Chen", "Jiaming Li", "Longze Chen", "Ze Gong", "Jingpeng Li", "Zhen Qin", "Hengyu Chang", "Ancheng Xu", "Zhihao Yang", "Hamid Alinejad-Rokny", "Qiang Qu", "Bo Zheng", "Min Yang"], "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning", "comment": "8 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a prevailing paradigm for enhancing reasoning in Multimodal Large Language Models (MLLMs). However, relying solely on outcome supervision risks reward hacking, where models learn spurious reasoning patterns to satisfy final answer checks. While recent rubric-based approaches offer fine-grained supervision signals, they suffer from high computational costs of instance-level generation and inefficient training dynamics caused by treating all rubrics as equally learnable. In this paper, we propose Stratified Rubric-based Curriculum Learning (RuCL), a novel framework that reformulates curriculum learning by shifting the focus from data selection to reward design. RuCL generates generalized rubrics for broad applicability and stratifies them based on the model's competence. By dynamically adjusting rubric weights during training, RuCL guides the model from mastering foundational perception to tackling advanced logical reasoning. Extensive experiments on various visual reasoning benchmarks show that RuCL yields a remarkable +7.83% average improvement over the Qwen2.5-VL-7B model, achieving a state-of-the-art accuracy of 60.06%.", "AI": {"tldr": "RuCL\u63d0\u51fa\u5206\u5c42\u5f0f\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bc4\u5206\u6807\u51c6\u6743\u91cd\u5f15\u5bfc\u6a21\u578b\u4ece\u57fa\u7840\u611f\u77e5\u5230\u9ad8\u7ea7\u903b\u8f91\u63a8\u7406\u7684\u5b66\u4e60\uff0c\u5728\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u98ce\u9669\uff0c\u800c\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\u4fe1\u53f7\uff0c\u4f46\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5c06\u6240\u6709\u8bc4\u5206\u6807\u51c6\u89c6\u4e3a\u540c\u7b49\u53ef\u5b66\u4e60\u7684\u4f4e\u6548\u8bad\u7ec3\u52a8\u6001\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5f0f\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8bfe\u7a0b\u5b66\u4e60\u91cd\u70b9\u4ece\u6570\u636e\u9009\u62e9\u8f6c\u5411\u5956\u52b1\u8bbe\u8ba1\uff0c\u751f\u6210\u901a\u7528\u8bc4\u5206\u6807\u51c6\u5e76\u6309\u6a21\u578b\u80fd\u529b\u5206\u5c42\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u8bc4\u5206\u6807\u51c6\u6743\u91cd\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRuCL\u76f8\u6bd4Qwen2.5-VL-7B\u6a21\u578b\u5e73\u5747\u63d0\u53477.83%\uff0c\u8fbe\u523060.06%\u7684\u6700\u65b0\u51c6\u786e\u7387\u3002", "conclusion": "RuCL\u901a\u8fc7\u5206\u5c42\u5f0f\u8bc4\u5206\u6807\u51c6\u8bfe\u7a0b\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.21638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21638", "abs": "https://arxiv.org/abs/2602.21638", "authors": ["Anqi Li", "Ruihan Wang", "Zhaoming Chen", "Yuqian Chen", "Yu Lu", "Yi Zhu", "Yuan Xie", "Zhenzhong Lan"], "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs", "comment": "8 pages", "summary": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human counselors' interventions specifically targeting client resistance in text-based therapy. We introduce a theory-driven framework that decomposes counselor responses into four distinct communication mechanisms. Leveraging this framework, we curate and share an expert-annotated dataset of real-world counseling excerpts, pairing counselor-client interactions with professional ratings and explanatory rationales. Using this data, we perform full-parameter instruction tuning on a Llama-3.1-8B-Instruct backbone to model fine-grained evaluative judgments of response quality and generate explanations underlying. Experimental results show that our approach can effectively distinguish the quality of different communication mechanisms (77-81% F1), substantially outperforming GPT-4o and Claude-3.5-Sonnet (45-59% F1). Moreover, the model produces high-quality explanations that closely align with expert references and receive near-ceiling ratings from human experts (2.8-2.9/3.0). A controlled experiment with 43 counselors further confirms that receiving these AI-generated feedback significantly improves counselors' ability to respond effectively to client resistance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eNLP\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u5206\u6790\u5fc3\u7406\u54a8\u8be2\u4e2d\u9488\u5bf9\u5ba2\u6237\u62b5\u6297\u7684\u5e72\u9884\u8d28\u91cf\uff0c\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u6846\u67b6\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54a8\u8be2\u5e08\u7684\u5e94\u5bf9\u80fd\u529b\u3002", "motivation": "\u5fc3\u7406\u54a8\u8be2\u4e2d\u6709\u6548\u5904\u7406\u5ba2\u6237\u62b5\u6297\u662f\u4e00\u9879\u590d\u6742\u6280\u80fd\uff0c\u4f46\u4ece\u4e1a\u8005\u7f3a\u4e4f\u53ca\u65f6\u3001\u53ef\u6269\u5c55\u7684\u76d1\u7763\u53cd\u9988\u3002\u73b0\u6709NLP\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u54a8\u8be2\u8d28\u91cf\u548c\u4e00\u822c\u6cbb\u7597\u6280\u80fd\uff0c\u65e0\u6cd5\u5bf9\u5ba2\u6237\u51fa\u73b0\u62b5\u6297\u7684\u9ad8\u98ce\u9669\u65f6\u523b\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u5c06\u54a8\u8be2\u5e08\u56de\u5e94\u5206\u89e3\u4e3a\u56db\u79cd\u4e0d\u540c\u7684\u6c9f\u901a\u673a\u5236\uff1b\u521b\u5efa\u5e76\u5206\u4eab\u4e86\u4e13\u5bb6\u6807\u6ce8\u7684\u771f\u5b9e\u54a8\u8be2\u5bf9\u8bdd\u6570\u636e\u96c6\uff1b\u57fa\u4e8eLlama-3.1-8B-Instruct\u9aa8\u5e72\u6a21\u578b\u8fdb\u884c\u5168\u53c2\u6570\u6307\u4ee4\u5fae\u8c03\uff0c\u5efa\u6a21\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u5224\u65ad\u5e76\u751f\u6210\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6c9f\u901a\u673a\u5236\u7684\u8d28\u91cf\uff0877-81% F1\uff09\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\u548cClaude-3.5-Sonnet\uff0845-59% F1\uff09\uff1b\u751f\u6210\u7684\u89e3\u91ca\u4e0e\u4e13\u5bb6\u53c2\u8003\u9ad8\u5ea6\u4e00\u81f4\uff0c\u83b7\u5f97\u63a5\u8fd1\u6ee1\u5206\u7684\u4e13\u5bb6\u8bc4\u5206\uff082.8-2.9/3.0\uff09\uff1b43\u540d\u54a8\u8be2\u5e08\u7684\u5bf9\u7167\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u63a5\u6536AI\u751f\u6210\u53cd\u9988\u663e\u8457\u63d0\u5347\u4e86\u4ed6\u4eec\u5e94\u5bf9\u5ba2\u6237\u62b5\u6297\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6709\u6548\u7684AI\u7cfb\u7edf\uff0c\u80fd\u591f\u4e3a\u5fc3\u7406\u54a8\u8be2\u5e08\u63d0\u4f9b\u9488\u5bf9\u5ba2\u6237\u62b5\u6297\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u548c\u89e3\u91ca\u6027\u53cd\u9988\uff0c\u586b\u8865\u4e86\u73b0\u6709NLP\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u8bc1\u660e\u8fd9\u79cd\u53cd\u9988\u80fd\u5b9e\u9645\u6539\u5584\u54a8\u8be2\u5e08\u7684\u4e34\u5e8a\u6280\u80fd\u3002"}}
{"id": "2602.21647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21647", "abs": "https://arxiv.org/abs/2602.21647", "authors": ["Tangsang Chongbang", "Pranesh Pyara Shrestha", "Amrit Sarki", "Anku Jaiswal"], "title": "Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration", "comment": "13 pages, 4 figures, 12 tables", "summary": "This paper presents and evaluates an optimized cascaded Nepali speech-to-English text translation (S2TT) system, focusing on mitigating structural noise introduced by Automatic Speech Recognition (ASR). We first establish highly proficient ASR and NMT components: a Wav2Vec2-XLS-R-300m model achieved a state-of-the-art 2.72% CER on OpenSLR-54, and a multi-stage fine-tuned MarianMT model reached a 28.32 BLEU score on the FLORES-200 benchmark. We empirically investigate the influence of punctuation loss, demonstrating that unpunctuated ASR output significantly degrades translation quality, causing a massive 20.7% relative BLEU drop on the FLORES benchmark. To overcome this, we propose and evaluate an intermediate Punctuation Restoration Module (PRM). The final S2TT pipeline was tested across three configurations on a custom dataset. The optimal configuration, which applied the PRM directly to ASR output, achieved a 4.90 BLEU point gain over the direct ASR-to-NMT baseline (BLEU 36.38 vs. 31.48). This improvement was validated by human assessment, which confirmed the optimized pipeline's superior Adequacy (3.673) and Fluency (3.804). This work validates that targeted punctuation restoration is the most effective intervention for mitigating structural noise in the Nepali S2TT pipeline. It establishes an optimized baseline and demonstrates a critical architectural insight for developing cascaded speech translation systems for similar low-resource languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u4f18\u5316\u7684\u7ea7\u8054\u5f0f\u5c3c\u6cca\u5c14\u8bed\u8bed\u97f3\u5230\u82f1\u8bed\u6587\u672c\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u6807\u70b9\u6062\u590d\u6a21\u5757\u6765\u7f13\u89e3ASR\u5f15\u5165\u7684\u7ed3\u6784\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u7ea7\u8054\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u4f1a\u5f15\u5165\u7ed3\u6784\u566a\u58f0\uff0c\u7279\u522b\u662f\u6807\u70b9\u7b26\u53f7\u7684\u4e22\u5931\u4f1a\u663e\u8457\u964d\u4f4e\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u7684\u8d28\u91cf\u3002\u5bf9\u4e8e\u5c3c\u6cca\u5c14\u8bed\u8fd9\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u5efa\u7acb\u9ad8\u6027\u80fd\u7684ASR\uff08Wav2Vec2-XLS-R-300m\u6a21\u578b\uff09\u548cNMT\uff08\u591a\u9636\u6bb5\u5fae\u8c03\u7684MarianMT\u6a21\u578b\uff09\u7ec4\u4ef6\uff0c\u7136\u540e\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e2d\u95f4\u6807\u70b9\u6062\u590d\u6a21\u5757\uff08PRM\uff09\u3002\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u4e09\u79cd\u914d\u7f6e\uff0c\u6700\u7ec8\u786e\u5b9a\u5c06PRM\u76f4\u63a5\u5e94\u7528\u4e8eASR\u8f93\u51fa\u7684\u6700\u4f18\u914d\u7f6e\u3002", "result": "\u6807\u70b9\u4e22\u5931\u5bfc\u81f4FLORES\u57fa\u51c6\u6d4b\u8bd5\u4e0aBLEU\u5206\u6570\u76f8\u5bf9\u4e0b\u964d20.7%\u3002\u6700\u4f18\u914d\u7f6e\uff08ASR\u8f93\u51fa\u76f4\u63a5\u5e94\u7528PRM\uff09\u6bd4\u76f4\u63a5ASR-to-NMT\u57fa\u7ebf\u63d0\u5347\u4e864.90 BLEU\u70b9\uff0836.38 vs. 31.48\uff09\u3002\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u4f18\u5316\u7ba1\u9053\u5728\u5145\u5206\u6027\uff083.673\uff09\u548c\u6d41\u7545\u6027\uff083.804\uff09\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u6807\u70b9\u6062\u590d\u662f\u7f13\u89e3\u5c3c\u6cca\u5c14\u8bedS2TT\u7ba1\u9053\u4e2d\u7ed3\u6784\u566a\u58f0\u7684\u6700\u6709\u6548\u5e72\u9884\u63aa\u65bd\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7c7b\u4f3c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ea7\u8054\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u5efa\u7acb\u4e86\u4f18\u5316\u57fa\u7ebf\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u67b6\u6784\u89c1\u89e3\u3002"}}
{"id": "2602.21652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21652", "abs": "https://arxiv.org/abs/2602.21652", "authors": ["Minhao Jiang", "Zhikai Li", "Xuewen Liu", "Jing Zhang", "Mengjuan Chen", "Qingyi Gu"], "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models", "comment": "5 pages, 1 figure, 4 tables", "summary": "Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. However, native dense matrices lack high sparsity, making existing approaches that directly remove weights disrupt model states, resulting in unsatisfactory performance recovery even with post-tuning. We propose Sparsity Induction, which promotes models toward higher sparsity at both distribution and feature levels before pruning, to push the limits of PTS. At the distribution level, we enhance distributional sparsity through mathematically equivalent scaling transformations, which are fully absorbable and incur no extra parameters or inference-time overhead. At the feature level, we introduce Spectral Norm Loss to promote feature sparsity from a low-rank perspective. Experiments across diverse model architectures and tasks demonstrate that our method further enhances sparsity-friendliness, achieving superior pruning performance over existing approaches.", "AI": {"tldr": "\u63d0\u51faSparsity Induction\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u526a\u679d\u524d\u4ece\u5206\u5e03\u548c\u7279\u5f81\u5c42\u9762\u4fc3\u8fdb\u6a21\u578b\u5411\u66f4\u9ad8\u7a00\u758f\u5ea6\u53d1\u5c55\uff0c\u7a81\u7834\u540e\u8bad\u7ec3\u7a00\u758f\u5316\u7684\u6781\u9650\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u589e\u957f\u5e26\u6765\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u6311\u6218\uff0c\u540e\u8bad\u7ec3\u7a00\u758f\u5316\u662f\u6709\u6548\u65b9\u6cd5\u3002\u4f46\u539f\u751f\u7a20\u5bc6\u77e9\u9635\u7f3a\u4e4f\u9ad8\u7a00\u758f\u5ea6\uff0c\u76f4\u63a5\u79fb\u9664\u6743\u91cd\u4f1a\u7834\u574f\u6a21\u578b\u72b6\u6001\uff0c\u5373\u4f7f\u540e\u8c03\u4f18\u4e5f\u96be\u4ee5\u6062\u590d\u6027\u80fd\u3002", "method": "\u63d0\u51faSparsity Induction\u65b9\u6cd5\uff1a1) \u5206\u5e03\u5c42\u9762\uff1a\u901a\u8fc7\u6570\u5b66\u7b49\u4ef7\u7684\u7f29\u653e\u53d8\u6362\u589e\u5f3a\u5206\u5e03\u7a00\u758f\u6027\uff0c\u5b8c\u5168\u53ef\u5438\u6536\u4e14\u65e0\u989d\u5916\u53c2\u6570\u6216\u63a8\u7406\u5f00\u9500\uff1b2) \u7279\u5f81\u5c42\u9762\uff1a\u5f15\u5165\u8c31\u8303\u6570\u635f\u5931\u4ece\u4f4e\u79e9\u89d2\u5ea6\u4fc3\u8fdb\u7279\u5f81\u7a00\u758f\u6027\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u7a00\u758f\u53cb\u597d\u6027\uff0c\u5728\u526a\u679d\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Sparsity Induction\u901a\u8fc7\u5728\u526a\u679d\u524d\u4ece\u5206\u5e03\u548c\u7279\u5f81\u5c42\u9762\u4fc3\u8fdb\u6a21\u578b\u5411\u66f4\u9ad8\u7a00\u758f\u5ea6\u53d1\u5c55\uff0c\u6709\u6548\u63d0\u5347\u4e86\u540e\u8bad\u7ec3\u7a00\u758f\u5316\u7684\u6027\u80fd\u6781\u9650\u3002"}}
{"id": "2602.21669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21669", "abs": "https://arxiv.org/abs/2602.21669", "authors": ["Duc Trung Vu", "Pham Khanh Chi", "Dat Phi Van", "Linh Ngo Van", "Sang Dinh", "Trung Le"], "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation", "comment": "EACL Findings", "summary": "Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-based weighting and achieves precise sequence-level alignment by leveraging both lexical and semantic information. At the token level, DWA-KD maps teacher representations into the student space and vice versa, performing dual-space KD via Kullback-Leibler divergence (KL). The process is modulated by dual-space weights that up-weight tokens where the student is uncertain and the teacher is confident, thereby focusing learning on informative tokens rather than treating all positions equally. At the sequence level, DWA-KD applies Soft Dynamic Time Warping (Soft-DTW) to both the embedding and final hidden-state layers, enabling robust alignment of lexical and contextual semantics between teacher and student sequences. Extensive experiments across diverse NLP benchmarks demonstrate that DWA-KD outperforms state-of-the-art KD baselines, while ablation studies confirm the complementary contributions of entropy-based token weighting and embedding and final hidden state layer Soft-DTW alignment.", "AI": {"tldr": "DWA-KD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u5206\u8bcd\u5668\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7a7a\u95f4\u71b5\u52a0\u6743\u548c\u65f6\u95f4\u626d\u66f2\u5bf9\u9f50\uff0c\u5728token\u7ea7\u522b\u548c\u5e8f\u5217\u7ea7\u522b\u540c\u65f6\u4f18\u5316\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u73b0\u6709\u8de8\u5206\u8bcd\u5668\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u5e8f\u5217\u548c\u8bcd\u6c47\u7ea7\u522b\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u540c\u65f6\u5904\u7406token\u7ea7\u522b\u548c\u5e8f\u5217\u7ea7\u522b\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faDWA-KD\u6846\u67b6\uff1a1) token\u7ea7\u522b\uff1a\u901a\u8fc7\u53cc\u7a7a\u95f4\u6620\u5c04\u548cKL\u6563\u5ea6\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u4f7f\u7528\u57fa\u4e8e\u71b5\u7684\u53cc\u7a7a\u95f4\u6743\u91cd\u6765\u5f3a\u8c03\u5b66\u751f\u4e0d\u786e\u5b9a\u800c\u6559\u5e08\u786e\u5b9a\u7684token\uff1b2) \u5e8f\u5217\u7ea7\u522b\uff1a\u5728\u5d4c\u5165\u5c42\u548c\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u5c42\u5e94\u7528Soft\u52a8\u6001\u65f6\u95f4\u626d\u66f2\uff0c\u5bf9\u9f50\u6559\u5e08\u548c\u5b66\u751f\u5e8f\u5217\u7684\u8bcd\u6c47\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u3002", "result": "\u5728\u591a\u4e2aNLP\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDWA-KD\u4f18\u4e8e\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u57fa\u4e8e\u71b5\u7684token\u52a0\u6743\u4e0e\u5d4c\u5165\u5c42\u548c\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u5c42\u7684Soft-DTW\u5bf9\u9f50\u5177\u6709\u4e92\u8865\u8d21\u732e\u3002", "conclusion": "DWA-KD\u901a\u8fc7\u53cc\u7a7a\u95f4\u52a0\u6743\u548c\u65f6\u95f4\u626d\u66f2\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5206\u8bcd\u5668\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u5e8f\u5217\u548c\u8bcd\u6c47\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002"}}
{"id": "2602.21720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21720", "abs": "https://arxiv.org/abs/2602.21720", "authors": ["Andrea Silvi", "Ponrawee Prasertsom", "Jennifer Culbertson", "Devdatt Dubhashi", "Moa Johansson", "Kenny Smith"], "title": "Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning", "comment": null, "summary": "Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facilitates learning. Adopting methods from the Reinforcement Learning literature, we confirm that highly regular human(-like) systems are easier to learn than unattested but possible irregular systems. This asymmetry emerges under the natural assumption that recursive numeral systems are designed for generalisation from limited data to represent all integers exactly. We also find that the influence of regularity on learnability is absent for unnatural, highly irregular systems, whose learnability is influenced instead by signal length, suggesting that different pressures may influence learnability differently in different parts of the space of possible numeral systems. Our results contribute to the body of work linking learnability to cross-linguistic prevalence.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u4eba\u7c7b\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u7684\u89c4\u5f8b\u6027\u4fc3\u8fdb\u5b66\u4e60\uff0c\u89e3\u91ca\u4e86\u4e3a\u4f55\u89c4\u5f8b\u7cfb\u7edf\u5728\u8de8\u8bed\u8a00\u4e2d\u66f4\u666e\u904d\u3002", "motivation": "\u63a2\u7a76\u4eba\u7c7b\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\uff08\u5982\u82f1\u8bed\u5341\u8fdb\u5236\u6570\u5b57\uff09\u4e3a\u4f55\u666e\u904d\u5177\u6709\u9ad8\u5ea6\u89c4\u5f8b\u6027\uff0c\u662f\u5426\u56e0\u4e3a\u89c4\u5f8b\u6027\u4fc3\u8fdb\u4e86\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u8de8\u8bed\u8a00\u4e2d\u7684\u8fd9\u79cd\u666e\u904d\u8d8b\u52bf\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6bd4\u8f83\u9ad8\u5ea6\u89c4\u5f8b\u7684\u4eba\u7c7b\u6570\u5b57\u7cfb\u7edf\u4e0e\u53ef\u80fd\u4f46\u4e0d\u5b58\u5728\u7684\u975e\u89c4\u5f8b\u7cfb\u7edf\u7684\u5b66\u4e60\u96be\u5ea6\u3002\u5047\u8bbe\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u65e8\u5728\u4ece\u6709\u9650\u6570\u636e\u4e2d\u6cdb\u5316\u4ee5\u7cbe\u786e\u8868\u793a\u6240\u6709\u6574\u6570\u3002", "result": "\u9ad8\u5ea6\u89c4\u5f8b\u7684\u4eba\u7c7b\u6570\u5b57\u7cfb\u7edf\u6bd4\u975e\u89c4\u5f8b\u7cfb\u7edf\u66f4\u5bb9\u6613\u5b66\u4e60\u3002\u5bf9\u4e8e\u4e0d\u81ea\u7136\u7684\u9ad8\u5ea6\u975e\u89c4\u5f8b\u7cfb\u7edf\uff0c\u5b66\u4e60\u6027\u4e0d\u53d7\u89c4\u5f8b\u6027\u5f71\u54cd\uff0c\u800c\u662f\u53d7\u4fe1\u53f7\u957f\u5ea6\u5f71\u54cd\uff0c\u8868\u660e\u4e0d\u540c\u538b\u529b\u5728\u4e0d\u540c\u53ef\u80fd\u7684\u6570\u5b57\u7cfb\u7edf\u7a7a\u95f4\u4e2d\u5f71\u54cd\u5b66\u4e60\u6027\u7684\u65b9\u5f0f\u4e0d\u540c\u3002", "conclusion": "\u89c4\u5f8b\u6027\u786e\u5b9e\u4fc3\u8fdb\u5b66\u4e60\uff0c\u8fd9\u89e3\u91ca\u4e86\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u5728\u8de8\u8bed\u8a00\u4e2d\u7684\u666e\u904d\u89c4\u5f8b\u6027\u3002\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4e86\u5b66\u4e60\u6027\u4e0e\u8de8\u8bed\u8a00\u666e\u904d\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2602.21728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21728", "abs": "https://arxiv.org/abs/2602.21728", "authors": ["Shiqi Yan", "Yubo Chen", "Ruiqi Zhou", "Zhengxi Yao", "Shuai Chen", "Tianyi Zhang", "Shijie Zhang", "Wei Qiang Zhang", "Yongfeng Huang", "Haixin Duan", "Yunqi Zhang"], "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling", "comment": "Published as a conference paper at ICLR 2026", "summary": "The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. To enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts. Extensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.", "AI": {"tldr": "\u63d0\u51faExplore-on-Graph\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9f13\u52b1LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u81ea\u4e3b\u63a2\u7d22\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u5148\u9a8c\u7ecf\u9a8c\u7684\u95ee\u9898\uff0c\u5728KGQA\u4efb\u52a1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684LLM\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u751f\u6210\u89c4\u5219\u7ea6\u675f\u6216\u6a21\u4eff\u56fa\u5b9a\u6f14\u793a\u8def\u5f84\u6765\u9650\u5236\u63a8\u7406\uff0c\u8fd9\u81ea\u7136\u5c06LLM\u7684\u63a8\u7406\u6a21\u5f0f\u5c40\u9650\u5728\u5148\u9a8c\u7ecf\u9a8c\u6216\u5fae\u8c03\u6570\u636e\u8303\u56f4\u5185\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u5206\u5e03\u5916\u56fe\u63a8\u7406\u95ee\u9898\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faExplore-on-Graph\u6846\u67b6\uff1a1) \u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4ee5\u63a8\u7406\u8def\u5f84\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\u4f5c\u4e3a\u5956\u52b1\uff0c\u6fc0\u52b1\u63a2\u7d22\u548c\u53d1\u73b0\u65b0\u9896\u63a8\u7406\u8def\u5f84\uff1b2) \u5c06\u8def\u5f84\u4fe1\u606f\u4f5c\u4e3a\u989d\u5916\u5956\u52b1\u4fe1\u53f7\uff0c\u4f18\u5316\u63a2\u7d22\u8fc7\u7a0b\u5e76\u51cf\u5c11\u65e0\u6548\u52aa\u529b\u3002", "result": "\u5728\u4e94\u4e2aKGQA\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u5f00\u6e90LLM\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u95ed\u6e90LLM\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u81ea\u4e3b\u63a2\u7d22\uff0cEoG\u6846\u67b6\u80fd\u591f\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u53d1\u73b0\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347KGQA\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.22072", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22072", "abs": "https://arxiv.org/abs/2602.22072", "authors": ["Christian Nickel", "Laura Schrewe", "Florian Mai", "Lucie Flek"], "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models", "comment": null, "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary.", "AI": {"tldr": "LLM\u5728\u9519\u8bef\u4fe1\u5ff5\u4efb\u52a1\u4e0a\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u5728\u4efb\u52a1\u6270\u52a8\u4e0b\u6025\u5267\u4e0b\u964d\uff0c\u8d28\u7591\u5176\u662f\u5426\u5177\u5907\u7a33\u5065\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u6574\u4f53\u63d0\u5347\u6027\u80fd\u4f46\u67d0\u4e9b\u6270\u52a8\u7c7b\u578b\u4e0b\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u6027", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u901a\u8fc7\u4efb\u52a1\u6270\u52a8\u6d4b\u8bd5\u5176\u7a33\u5065\u6027\uff0c\u5e76\u7814\u7a76\u601d\u7ef4\u94fe\u63d0\u793a\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\u5e76\u89e3\u91ca\u6a21\u578b\u51b3\u7b56", "method": "\u6784\u5efa\u624b\u5de5\u6807\u6ce8\u7684\u5fc3\u7406\u7406\u8bba\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ecf\u5178\u548c\u6270\u52a8\u7684\u9519\u8bef\u4fe1\u5ff5\u4efb\u52a1\u3001\u6709\u6548\u63a8\u7406\u94fe\u7a7a\u95f4\u3001\u63a8\u7406\u5fe0\u5b9e\u5ea6\u3001\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\uff1b\u63d0\u51fa\u8bc4\u4f30\u63a8\u7406\u94fe\u6b63\u786e\u6027\u548c\u7b54\u6848\u5bf9\u63a8\u7406\u8f68\u8ff9\u5fe0\u5b9e\u5ea6\u7684\u6307\u6807\uff1b\u6d4b\u8bd5\u591a\u79cdLLM\u5728\u6270\u52a8\u4efb\u52a1\u4e0a\u7684\u8868\u73b0", "result": "\u6240\u6709\u8bc4\u4f30\u7684LLM\u5728\u4efb\u52a1\u6270\u52a8\u4e0b\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u6025\u5267\u4e0b\u964d\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u6574\u4f53\u4e0a\u80fd\u5fe0\u5b9e\u63d0\u5347\u5fc3\u7406\u7406\u8bba\u6027\u80fd\uff0c\u4f46\u5bf9\u67d0\u4e9b\u6270\u52a8\u7c7b\u578b\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u9700\u8981\u9009\u62e9\u6027\u5e94\u7528", "conclusion": "LLM\u7f3a\u4e4f\u7a33\u5065\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u867d\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u9700\u8c28\u614e\u5e94\u7528\uff1b\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u589e\u5f3aLLM\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b"}}
{"id": "2602.21741", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.21741", "abs": "https://arxiv.org/abs/2602.21741", "authors": ["MD. Sagor Chowdhury", "Adiba Fairooz Chowdhury"], "title": "Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization", "comment": "6 pages, 5 figures, 3 tables; system paper submitted to DL Sprint 4.0 (Kaggle)", "summary": "We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whisper medium model with Demucs source separation for vocal isolation, silence-boundary chunking, and carefully tuned generation hyperparameters. For speaker diarization we reach a best private Diarization Error Rate (DER) of 0.27671 and public DER of 0.20936 by replacing the default segmentation model inside the pyannote.audio pipeline with a Bengali-fine-tuned variant, pairing it with wespeaker-voxceleb-resnet34-LM embeddings and centroid-based agglomerative clustering. Our experiments demonstrate that domain-specific fine-tuning of the segmentation component, vocal source separation, and natural silence-aware chunking are the three most impactful design choices for low-resource Bengali speech processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u53c2\u52a0Kaggle DL Sprint 4.0\u7ade\u8d5b\u7684\u5b5f\u52a0\u62c9\u8bed\u957f\u8bed\u97f3\u8bc6\u522b\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\uff0c\u901a\u8fc7Whisper\u6a21\u578b\u5fae\u8c03\u3001\u58f0\u6e90\u5206\u79bb\u548c\u8bf4\u8bdd\u4eba\u5206\u5272\u4f18\u5316\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u5904\u7406\u9762\u4e34\u591a\u91cd\u6311\u6218\uff1a\u97f3\u7d20\u5e93\u5e9e\u5927\u3001\u65b9\u8a00\u5dee\u5f02\u663e\u8457\u3001\u4e0e\u82f1\u8bed\u9891\u7e41\u6df7\u7528\u3001\u5927\u89c4\u6a21\u6807\u6ce8\u8bed\u6599\u7a00\u7f3a\u3002\u8fd9\u4e9b\u56e0\u7d20\u4f7f\u5f97\u4f20\u7edf\u7684\u8bed\u97f3\u8bc6\u522b\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u65b9\u6cd5\u5728\u5b5f\u52a0\u62c9\u8bed\u73af\u5883\u4e0b\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u8bed\u97f3\u8bc6\u522b\uff1a\u4f7f\u7528BengaliAI\u5fae\u8c03\u7684Whisper medium\u6a21\u578b\uff0c\u7ed3\u5408Demucs\u58f0\u6e90\u5206\u79bb\u8fdb\u884c\u4eba\u58f0\u63d0\u53d6\uff0c\u91c7\u7528\u9759\u97f3\u8fb9\u754c\u5206\u5757\u7b56\u7565\uff0c\u5e76\u7cbe\u7ec6\u8c03\u6574\u751f\u6210\u8d85\u53c2\u6570\u3002\n2. \u8bf4\u8bdd\u4eba\u65e5\u5fd7\uff1a\u6539\u8fdbpyannote.audio\u6d41\u7a0b\uff0c\u7528\u5b5f\u52a0\u62c9\u8bed\u5fae\u8c03\u7684\u5206\u5272\u6a21\u578b\u66ff\u6362\u9ed8\u8ba4\u6a21\u578b\uff0c\u914d\u5408wespeaker-voxceleb-resnet34-LM\u5d4c\u5165\u548c\u57fa\u4e8e\u8d28\u5fc3\u7684\u51dd\u805a\u805a\u7c7b\u3002", "result": "\u8bed\u97f3\u8bc6\u522b\uff1a\u6700\u4f73\u79c1\u6709WER\u4e3a0.37738\uff0c\u516c\u5f00WER\u4e3a0.36137\u3002\n\u8bf4\u8bdd\u4eba\u65e5\u5fd7\uff1a\u6700\u4f73\u79c1\u6709DER\u4e3a0.27671\uff0c\u516c\u5f00DER\u4e3a0.20936\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0c\u9886\u57df\u7279\u5b9a\u7684\u5206\u5272\u6a21\u578b\u5fae\u8c03\u3001\u58f0\u6e90\u5206\u79bb\u548c\u81ea\u7136\u9759\u97f3\u611f\u77e5\u5206\u5757\u662f\u5f71\u54cd\u6027\u80fd\u7684\u4e09\u4e2a\u6700\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u5904\u7406\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u3001\u58f0\u6e90\u5206\u79bb\u6280\u672f\u548c\u9759\u97f3\u611f\u77e5\u5206\u5757\u7684\u7ec4\u5408\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2602.22207", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22207", "abs": "https://arxiv.org/abs/2602.22207", "authors": ["Hanna Yukhymenko", "Anton Alexandrov", "Martin Vechev"], "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets", "comment": null, "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6\u89e3\u51b3\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u4e2d\u7ffb\u8bd1\u57fa\u51c6\u8d28\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7USI\u548cT-RANK\u65b9\u6cd5\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e94\u7528\u4e8e8\u79cd\u4e1c\u6b27\u548c\u5357\u6b27\u8bed\u8a00\uff0c\u8d85\u8d8a\u73b0\u6709\u8d44\u6e90", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u53d7\u5230\u7ffb\u8bd1\u57fa\u51c6\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u635f\u5bb3\u3002\u73b0\u6709\u8d44\u6e90\u5e38\u5b58\u5728\u8bed\u4e49\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u8bef\u5bfc\u6027\u7684\u6027\u80fd\u6307\u6807\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u591a\u8bed\u8a00AI\u8bc4\u4f30", "method": "\u5f00\u53d1\u5168\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u91c7\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u7b56\u7565\uff0c\u7279\u522b\u662f\u901a\u7528\u81ea\u6211\u6539\u8fdb\uff08USI\uff09\u548c\u63d0\u51fa\u7684\u591a\u8f6e\u6392\u540d\u65b9\u6cd5T-RANK\uff0c\u786e\u4fdd\u57fa\u51c6\u5728\u672c\u5730\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u539f\u59cb\u4efb\u52a1\u7ed3\u6784\u548c\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b", "result": "\u5c06\u6846\u67b6\u5e94\u7528\u4e8e8\u79cd\u4e1c\u6b27\u548c\u5357\u6b27\u8bed\u8a00\uff08\u4e4c\u514b\u5170\u8bed\u3001\u4fdd\u52a0\u5229\u4e9a\u8bed\u3001\u65af\u6d1b\u4f10\u514b\u8bed\u3001\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u3001\u7acb\u9676\u5b9b\u8bed\u3001\u7231\u6c99\u5c3c\u4e9a\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u5e0c\u814a\u8bed\uff09\u7684\u6d41\u884c\u57fa\u51c6\u7ffb\u8bd1\uff0c\u4f7f\u7528\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u7ffb\u8bd1\u8d28\u91cf\u8d85\u8d8a\u73b0\u6709\u8d44\u6e90\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4e0b\u6e38\u6a21\u578b\u8bc4\u4f30", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u7ffb\u8bd1\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\uff0c\u53d1\u5e03\u7684\u6846\u67b6\u548c\u6539\u8fdb\u7684\u57fa\u51c6\u5c06\u4fc3\u8fdb\u7a33\u5065\u4e14\u53ef\u590d\u73b0\u7684\u591a\u8bed\u8a00AI\u53d1\u5c55"}}
{"id": "2602.21763", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21763", "abs": "https://arxiv.org/abs/2602.21763", "authors": ["Heng Wang", "Changxing Wu"], "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs", "comment": "AAAI26'0ral", "summary": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classification-generation framework that jointly performs relation prediction and explanation generation, and train it with the additional supervision of LLM-generated explanations. Our framework is plug-and-play, enabling easy integration with most existing IDRR models. Experimental results on PDTB demonstrate that our approach significantly improves IDRR performance, while human evaluation further confirms that the generated explanations enhance model interpretability. Furthermore, we validate the generality of our approach on sentiment classification and natural language inference", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u8bc6\u522b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u8bc6\u522b\uff08IDRR\uff09\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u7f3a\u4e4f\u663e\u5f0f\u7bc7\u7ae0\u6807\u8bb0\u65f6\u9700\u8981\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\uff1b2\uff09\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u9884\u6d4b\u5173\u7cfb\u800c\u4e0d\u63d0\u4f9b\u89e3\u91ca\u652f\u6301\u3002\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6df1\u5ea6\u8bed\u8a00\u7406\u89e3\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528LLMs\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff1a1\uff09\u9996\u5148\u4f7f\u7528LLM\u57fa\u4e8e\u9ec4\u91d1\u6807\u7b7e\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u751f\u6210\u89e3\u91ca\uff1b2\uff09\u5f15\u5165\u65b0\u9896\u7684\u5206\u7c7b-\u751f\u6210\u6846\u67b6\uff0c\u8054\u5408\u6267\u884c\u5173\u7cfb\u9884\u6d4b\u548c\u89e3\u91ca\u751f\u6210\uff1b3\uff09\u4f7f\u7528LLM\u751f\u6210\u7684\u89e3\u91ca\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u8bad\u7ec3\u8f7b\u91cf\u7ea7IDRR\u6a21\u578b\u3002\u8be5\u6846\u67b6\u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u53ef\u4e0e\u5927\u591a\u6570\u73b0\u6709IDRR\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728PDTB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86IDRR\u6027\u80fd\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u751f\u6210\u7684\u89e3\u91ca\u589e\u5f3a\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u5728\u60c5\u611f\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u9a8c\u8bc1\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06LLMs\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7IDRR\u6a21\u578b\u4e2d\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u9700\u8981\u89e3\u91ca\u7684NLP\u4efb\u52a1\u3002"}}
{"id": "2602.21786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21786", "abs": "https://arxiv.org/abs/2602.21786", "authors": ["Shunsuke Ubukata"], "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models", "comment": "9 pages, 3 figures. Code: https://github.com/gitpullpull/DisciplinedChainOfThought | Benchmarks: https://huggingface.co/datasets/gitpullpull/D-CoT-Benchmarks | Dataset: https://huggingface.co/datasets/gitpullpull/D-CoT-datasets", "summary": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-shot) by 9.1%, while drastically reducing computational costs. Furthermore, we confirm that the model internalizes this disciplined thought structure, maintaining high performance even without explicit control tags during inference.", "AI": {"tldr": "\u63d0\u51faD-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u6807\u7b7e\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578bCoT\u84b8\u998f\u4e2d\u7684\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u3002", "motivation": "\u4f20\u7edfCoT\u84b8\u998f\u65b9\u6cd5\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\u4e2d\u5bb9\u6613\u5f15\u53d1\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u591a\u7684token\u6d88\u8017\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7ed3\u6784\u5316\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u4f18\u5316CoT\u8f68\u8ff9\u3002", "method": "\u63d0\u51faDisciplined Chain-of-Thought (D-CoT)\u6846\u67b6\uff0c\u4f7f\u7528\u63a7\u5236\u6807\u7b7e\uff08\u5982<TEMP_LOW>\u7528\u4e8e\u4e8b\u5b9e\u68c0\u67e5\uff0c<TEMP_HIGH>\u7528\u4e8e\u591a\u89c6\u89d2\u63a2\u7d22\uff09\u4f5c\u4e3a\u8f85\u52a9\u811a\u624b\u67b6\u6765\u5f3a\u5236\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f18\u5316CoT\u8f68\u8ff9\u5e76\u6291\u5236\u63a8\u7406\u6f02\u79fb\u3002", "result": "\u5728Qwen3-8B\u6a21\u578b\u4e0a\uff0c\u4ec5\u4f7f\u75285,000\u4e2a\u8bad\u7ec3\u6837\u672c\uff0cD-CoT\u663e\u8457\u63d0\u5347GPQA-diamond\u51c6\u786e\u73879.9%\uff0cMMLU-Pro (0-shot)\u51c6\u786e\u73879.1%\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u6a21\u578b\u5185\u5316\u4e86\u8fd9\u79cd\u7ed3\u6784\u5316\u601d\u7ef4\uff0c\u5728\u63a8\u7406\u65f6\u5373\u4f7f\u6ca1\u6709\u663e\u5f0f\u63a7\u5236\u6807\u7b7e\u4e5f\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "D-CoT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u8bed\u8a00\u6a21\u578bCoT\u84b8\u998f\u4e2d\u7684\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u5316\u7684\u53cc\u91cd\u76ee\u6807\uff0c\u4e3a\u9ad8\u6548\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.21854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21854", "abs": "https://arxiv.org/abs/2602.21854", "authors": ["Mustafa Dogan", "Ilker Kesen", "Iacer Calixto", "Aykut Erdem", "Erkut Erdem"], "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning", "comment": "Preprint. 49 pages, 38 Figures, 5 Tables", "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench", "AI": {"tldr": "FewMMBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u6db5\u76d6\u591a\u79cd\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8626\u4e2a\u5f00\u653e\u6743\u91cd\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u6570\u636e\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u8bc4\u4f30\u5176\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30MLLMs\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faFewMMBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4ece\u5c5e\u6027\u8bc6\u522b\u5230\u65f6\u5e8f\u63a8\u7406\u7684\u591a\u6837\u5316\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u3002\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\u589e\u5f3a\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u8bc4\u4f30\u4e86\u6765\u81ea\u516d\u4e2a\u6a21\u578b\u5bb6\u65cf\u768426\u4e2a\u5f00\u653e\u6743\u91cdMLLMs\uff0c\u5e76\u5206\u6790\u4e86\u4efb\u52a1\u7c7b\u578b\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u901a\u8fc7\u989d\u5916\u6f14\u793a\u6216\u601d\u7ef4\u94fe\u63a8\u7406\u83b7\u76ca\u6709\u9650\u751a\u81f3\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\uff1b2\uff09\u57fa\u4e8e\u68c0\u7d22\u7684\u6f14\u793a\u548c\u589e\u52a0\u4e0a\u4e0b\u6587\u5927\u5c0f\u5e26\u6765\u7684\u589e\u76ca\u6709\u9650\uff1b3\uff09FewMMBench\u80fd\u591f\u6709\u6548\u8bca\u65ad\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u80fd\u529b\u3002", "conclusion": "FewMMBench\u4e3a\u8bca\u65ad\u548c\u63a8\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5bf9\u989d\u5916\u6f14\u793a\u7684\u6709\u9650\u54cd\u5e94\u3002"}}
{"id": "2602.21862", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21862", "abs": "https://arxiv.org/abs/2602.21862", "authors": ["Chia Cheng Chang", "An-Zi Yen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access", "comment": null, "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u4e3b\u52a8\u4fe1\u606f\u8bbf\u95ee\u6846\u67b6\uff0c\u5e2e\u52a9\u7528\u6237\u56de\u5fc6\u9057\u5fd8\u7684\u751f\u6d3b\u7ecf\u5386", "motivation": "\u4e2a\u4eba\u96be\u4ee5\u56de\u5fc6\u6240\u6709\u751f\u6d3b\u7ec6\u8282\u4e14\u5bb9\u6613\u6df7\u6dc6\u4e8b\u4ef6\uff0c\u73b0\u6709\u8bb0\u5fc6\u56de\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u4e2a\u4eba\u751f\u6d3b\u65e5\u5fd7\u6570\u636e\u7a00\u7f3a\uff0c\u4e14\u7cfb\u7edf\u9700\u8981\u9002\u5e94\u968f\u65f6\u95f4\u589e\u957f\u7684\u65b0\u6570\u636e", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e3b\u52a8\u4fe1\u606f\u8bbf\u95ee\uff0c\u96c6\u6210\u4e2a\u4eba\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u7cbe\u70bc\u7684\u51b3\u7b56\u8fc7\u7a0b\u589e\u5f3a\u8bbf\u95ee\u9700\u6c42\u68c0\u6d4b\uff0c\u6846\u67b6\u5177\u6709\u9ad8\u5ea6\u7075\u6d3b\u6027\uff0c\u53ef\u66ff\u6362\u57fa\u7840\u6a21\u578b\u548c\u4fee\u6539\u4e8b\u5b9e\u68c0\u7d22\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u9057\u5fd8\u4e8b\u4ef6\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u56de\u5fc6\u8fc7\u53bb\u7ecf\u5386", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u6210\u529f\u89e3\u51b3\u4e86\u4e2a\u4eba\u8bb0\u5fc6\u56de\u5fc6\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6539\u8fdb\u7684\u7cfb\u7edf\u65b9\u6848"}}
{"id": "2602.21887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21887", "abs": "https://arxiv.org/abs/2602.21887", "authors": ["Changjiang Gao", "Zixian Huang", "Kaichen Yang", "Jiajun Chen", "Jixing Li", "Shujian Huang"], "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection", "comment": null, "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.", "AI": {"tldr": "ExpLang\u662f\u4e00\u79cd\u65b0\u9896\u7684LLM\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u542f\u7528\u7b56\u7565\u5185\u601d\u8003\u8bed\u8a00\u9009\u62e9\uff0c\u5229\u7528\u591a\u8bed\u8a00\u4f18\u52bf\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u548c\u5229\u7528\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u9884\u7b97\u4e0b\u7a33\u5b9a\u8d85\u8d8a\u4ec5\u82f1\u8bed\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u540e\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\u63a8\u7406\uff0c\u5ffd\u89c6\u4e86\u591a\u8bed\u8a00\u601d\u7ef4\u7684\u6f5c\u5728\u4f18\u52bf\u4ee5\u53ca\u5168\u7403\u7528\u6237\u5bf9\u539f\u751f\u601d\u7ef4\u8f68\u8ff9\u7684\u9700\u6c42\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5229\u7528\u591a\u8bed\u8a00\u6027\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faExpLang\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5c06\u7b56\u7565\u5185\u601d\u8003\u8bed\u8a00\u9009\u62e9\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u4f5c\uff0c\u5141\u8bb8\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u9009\u62e9\u601d\u8003\u8bed\u8a00\uff0c\u4ece\u800c\u6269\u5c55\u63a2\u7d22\u7a7a\u95f4\u5e76\u5229\u7528\u975e\u82f1\u8bed\u4f18\u52bf\u3002", "result": "\u65b9\u6cd5\u5728\u76f8\u540c\u8bad\u7ec3\u9884\u7b97\u4e0b\u7a33\u5b9a\u8d85\u8d8a\u4ec5\u82f1\u8bed\u8bad\u7ec3\uff0c\u5bf9\u5df2\u89c1\u548c\u672a\u89c1\u8bed\u8a00\u90fd\u8868\u73b0\u51fa\u9ad8\u601d\u8003\u8bed\u8a00\u9075\u4ece\u6027\u3002\u5206\u6790\u663e\u793a\u901a\u8fc7\u591a\u8bed\u8a00\u504f\u597d\u6269\u5c55\u63a2\u7d22\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u975e\u82f1\u8bed\u4f18\u52bf\u6539\u8fdb\u5229\u7528\u7ed3\u679c\u3002", "conclusion": "ExpLang\u65b9\u6cd5\u4e0e\u5927\u591a\u6570\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6b63\u4ea4\uff0c\u4e3a\u5229\u7528\u591a\u8bed\u8a00\u6027\u6539\u8fdb\u5927\u578b\u63a8\u7406\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u89c6\u89d2\uff0c\u8bc1\u660e\u591a\u8bed\u8a00\u601d\u7ef4\u9009\u62e9\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.21933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21933", "abs": "https://arxiv.org/abs/2602.21933", "authors": ["Bitan Majumder", "Anirban Sen"], "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text", "comment": null, "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings.", "AI": {"tldr": "\u5728\u4f4e\u8d44\u6e90\u6df7\u5408\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684DistilBERT\u6a21\u578b\u5728\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4ee584%\u7684\u51c6\u786e\u7387\u8d85\u8d8a\u4e86Llama 3.1\u3001Mistral\u3001Gemma 3\u548cPhi-4\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u591a\u8bed\u8a00\u548c\u6df7\u5408\u4ee3\u7801\u73af\u5883\u4e2d\u7684\u8bbd\u523a\u68c0\u6d4b\u7531\u4e8e\u7ed3\u6784\u53d8\u5316\u3001\u975e\u6b63\u5f0f\u8868\u8fbe\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ef\u7528\u6027\uff0c\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u6784\u6210\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u4f4e\u8d44\u6e90\u6df7\u5408\u8bed\u8a00\uff08Hinglish\uff09\u73af\u5883\u4e0b\uff0c\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5bf9\u6bd4\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama 3.1\u3001Mistral\u3001Gemma 3\u3001Phi-4\uff09\u4e0e\u7ecf\u8fc7\u5fae\u8c03\u7684DistilBERT\u6a21\u578b\u5728\u6df7\u5408\u4ee3\u7801Hinglish\u6587\u672c\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002DistilBERT\u6a21\u578b\u4f7f\u7528\u5c11\u91cfLLM\u751f\u6210\u7684\u6df7\u5408\u4ee3\u7801\u6570\u636e\u8fdb\u884c\u987a\u5e8f\u5fae\u8c03\uff0c\u800cLLMs\u5219\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ecf\u8fc7\u5fae\u8c03\u7684DistilBERT\u6a21\u578b\u53d6\u5f97\u4e8684%\u7684\u6700\u9ad8\u603b\u4f53\u51c6\u786e\u7387\uff0c\u5728\u6240\u6709\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u90fd\u8d85\u8d8a\u4e86\u6240\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8fd9\u8868\u660e\u5728\u4f4e\u8d44\u6e90\u548c\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\uff0c\u5c0f\u578bTransformer\u6a21\u578b\u7684\u9886\u57df\u81ea\u9002\u5e94\u5fae\u8c03\u76f8\u6bd4\u901a\u7528LLM\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u8bbd\u523a\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u5728\u4f4e\u8d44\u6e90\u6df7\u5408\u8bed\u8a00\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u5fae\u8c03\u7684\u5c0f\u578bTransformer\u6a21\u578b\u6bd4\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u8fd9\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21947", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21947", "abs": "https://arxiv.org/abs/2602.21947", "authors": ["Sohan Venkatesh", "Ashish Mahendran Kurapath", "Tejas Melkote"], "title": "Large Language Models are Algorithmically Blind", "comment": "20 pages, 11 figures, 14 tables", "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction.", "AI": {"tldr": "LLMs\u5728\u7b97\u6cd5\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u79f0\u4e3a\"\u7b97\u6cd5\u76f2\u89c6\"\uff0c\u8868\u73b0\u4e3a\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u7b97\u6cd5\u6027\u80fd\uff0c\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u751a\u81f3\u4e0d\u5982\u968f\u673a\u731c\u6d4b", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u77e5\u8bc6\u5e7f\u5ea6\uff0c\u4f46\u5176\u5bf9\u8ba1\u7b97\u8fc7\u7a0b\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5bf9\u4f9d\u8d56LLMs\u6307\u5bfc\u7b97\u6cd5\u9009\u62e9\u548c\u90e8\u7f72\u7684\u4ece\u4e1a\u8005\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528\u56e0\u679c\u53d1\u73b0\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bc4\u4f30\u516b\u4e2a\u524d\u6cbfLLM\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u7b97\u6cd5\u6267\u884c\u7684ground truth\u8fdb\u884c\u6bd4\u8f83\u5206\u6790", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u3001\u8fd1\u4e4e\u5b8c\u5168\u7684\u5931\u8d25\uff1a\u6a21\u578b\u4ea7\u751f\u7684\u7f6e\u4fe1\u533a\u95f4\u8fdc\u5bbd\u4e8e\u771f\u5b9e\u533a\u95f4\uff0c\u5374\u4ecd\u65e0\u6cd5\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u5305\u542b\u771f\u5b9e\u7b97\u6cd5\u5747\u503c\uff1b\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u6bd4\u968f\u673a\u731c\u6d4b\u66f4\u5dee\uff0c\u6700\u4f73\u6a21\u578b\u7684\u8fb9\u9645\u8d85\u968f\u673a\u8868\u73b0\u6700\u7b26\u5408\u57fa\u51c6\u8bb0\u5fc6\u800c\u975e\u539f\u5219\u6027\u63a8\u7406", "conclusion": "\u8fd9\u79cd\u5931\u8d25\u88ab\u79f0\u4e3a\"\u7b97\u6cd5\u76f2\u89c6\"\uff0c\u53cd\u6620\u4e86\u5173\u4e8e\u7b97\u6cd5\u7684\u9648\u8ff0\u6027\u77e5\u8bc6\u4e0e\u6821\u51c6\u7684\u7a0b\u5e8f\u6027\u9884\u6d4b\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u8ddd"}}
{"id": "2602.21950", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21950", "abs": "https://arxiv.org/abs/2602.21950", "authors": ["Boqi Chen", "Xudong Liu", "Jiachuan Peng", "Marianne Frey-Marti", "Bang Zheng", "Kyle Lam", "Lin Li", "Jianing Qiu"], "title": "MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown great potential in medical applications, yet existing benchmarks inadequately capture real-world clinical complexity. We introduce MEDSYN, a multilingual, multimodal benchmark of highly complex clinical cases with up to 7 distinct visual clinical evidence (CE) types per case. Mirroring clinical workflow, we evaluate 18 MLLMs on differential diagnosis (DDx) generation and final diagnosis (FDx) selection. While top models often match or even outperform human experts on DDx generation, all MLLMs exhibit a much larger DDx--FDx performance gap compared to expert clinicians, indicating a failure mode in synthesis of heterogeneous CE types. Ablations attribute this failure to (i) overreliance on less discriminative textual CE ($\\it{e.g.}$, medical history) and (ii) a cross-modal CE utilization gap. We introduce Evidence Sensitivity to quantify the latter and show that a smaller gap correlates with higher diagnostic accuracy. Finally, we demonstrate how it can be used to guide interventions to improve model performance. We will open-source our benchmark and code.", "AI": {"tldr": "MEDSYN\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u7684\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u9ad8\u5ea6\u590d\u6742\u7684\u4e34\u5e8a\u75c5\u4f8b\uff0c\u6bcf\u4e2a\u75c5\u4f8b\u6700\u591a\u5305\u542b7\u79cd\u4e0d\u540c\u7684\u89c6\u89c9\u4e34\u5e8a\u8bc1\u636e\u7c7b\u578b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u9876\u7ea7MLLM\u5728\u9274\u522b\u8bca\u65ad\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u7efc\u5408\u5f02\u6784\u4e34\u5e8a\u8bc1\u636e\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u8868\u73b0\u4e3a\u9274\u522b\u8bca\u65ad\u4e0e\u6700\u7ec8\u8bca\u65ad\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u8fdc\u5927\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8bca\u65ad\u80fd\u529b\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u66f4\u8d34\u8fd1\u5b9e\u9645\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u63ed\u793a\u6a21\u578b\u5728\u7efc\u5408\u591a\u79cd\u4e34\u5e8a\u8bc1\u636e\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86MEDSYN\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u7684\u590d\u6742\u4e34\u5e8a\u75c5\u4f8b\uff0c\u6bcf\u4e2a\u75c5\u4f8b\u6700\u591a\u5305\u542b7\u79cd\u4e0d\u540c\u7684\u89c6\u89c9\u4e34\u5e8a\u8bc1\u636e\u7c7b\u578b\u3002\u8bc4\u4f30\u4e8618\u4e2aMLLM\u5728\u9274\u522b\u8bca\u65ad\u751f\u6210\u548c\u6700\u7ec8\u8bca\u65ad\u9009\u62e9\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u6a21\u62df\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u6a21\u578b\u5931\u8d25\u539f\u56e0\uff0c\u5e76\u5f15\u5165\"\u8bc1\u636e\u654f\u611f\u6027\"\u6307\u6807\u6765\u91cf\u5316\u8de8\u6a21\u6001\u4e34\u5e8a\u8bc1\u636e\u5229\u7528\u5dee\u8ddd\u3002", "result": "\u9876\u7ea7MLLM\u5728\u9274\u522b\u8bca\u65ad\u751f\u6210\u65b9\u9762\u5e38\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u9274\u522b\u8bca\u65ad\u4e0e\u6700\u7ec8\u8bca\u65ad\u9009\u62e9\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8fdc\u5927\u4e8e\u4e13\u5bb6\u4e34\u5e8a\u533b\u751f\u7684\u5dee\u8ddd\u3002\u7814\u7a76\u53d1\u73b0\u5931\u8d25\u539f\u56e0\u5305\u62ec\uff1a(1)\u8fc7\u5ea6\u4f9d\u8d56\u533a\u5206\u5ea6\u8f83\u4f4e\u7684\u6587\u672c\u4e34\u5e8a\u8bc1\u636e\uff08\u5982\u75c5\u53f2\uff09\uff1b(2)\u5b58\u5728\u8de8\u6a21\u6001\u4e34\u5e8a\u8bc1\u636e\u5229\u7528\u5dee\u8ddd\u3002\u8bc1\u636e\u654f\u611f\u6027\u6307\u6807\u663e\u793a\uff0c\u8f83\u5c0f\u7684\u8de8\u6a21\u6001\u5dee\u8ddd\u4e0e\u66f4\u9ad8\u7684\u8bca\u65ad\u51c6\u786e\u6027\u76f8\u5173\u3002", "conclusion": "MLLM\u5728\u7efc\u5408\u5f02\u6784\u4e34\u5e8a\u8bc1\u636e\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u4ece\u591a\u79cd\u4e34\u5e8a\u8bc1\u636e\u7c7b\u578b\u4e2d\u5408\u6210\u4fe1\u606f\u4ee5\u505a\u51fa\u6700\u7ec8\u8bca\u65ad\u51b3\u7b56\u65f6\u3002\u8bc1\u636e\u654f\u611f\u6027\u6307\u6807\u53ef\u4f5c\u4e3a\u6307\u5bfc\u5e72\u9884\u63aa\u65bd\u4ee5\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u7684\u6709\u7528\u5de5\u5177\u3002\u7814\u7a76\u56e2\u961f\u5c06\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801\u3002"}}
{"id": "2602.21951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21951", "abs": "https://arxiv.org/abs/2602.21951", "authors": ["Bo Xue", "Yuan Jin", "Luoyi Fu", "Jiaxin Ding", "Xinbing Wang"], "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning", "comment": null, "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.", "AI": {"tldr": "RADAR\u5c06\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4ece\u751f\u6210\u5f0f\u6a21\u5f0f\u5339\u914d\u91cd\u6784\u4e3a\u5224\u522b\u5f0f\u5173\u7cfb\u63a8\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5b9e\u4f53\u53ef\u5206\u6027\uff0c\u5728\u8868\u793a\u7a7a\u95f4\u76f4\u63a5\u63a8\u7406\u4ee5\u907f\u514d\u5e7b\u89c9\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u751f\u6210\u5f0f\u8303\u5f0f\uff0c\u5bb9\u6613\u8bb0\u5fc6\u8868\u9762\u5171\u73b0\u6a21\u5f0f\u800c\u975e\u5b66\u4e60\u771f\u6b63\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u89e3\u51b3\u751f\u6210\u5f0f\u65b9\u6cd5\u5bf9\u6807\u8bb0\u4f3c\u7136\u6a21\u4eff\u7684\u4f9d\u8d56\u53ca\u5176\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faRADAR\u6846\u67b6\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5224\u522b\u5f0f\u5b9e\u4f53\u9009\u62e9\u4efb\u52a1\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5b9e\u4f53\u95f4\u7684\u76f8\u5bf9\u53ef\u5206\u6027\uff0c\u8d85\u8d8a\u57fa\u4e8e\u6807\u8bb0\u4f3c\u7136\u7684\u6a21\u4eff\u5b66\u4e60\u3002\u5229\u7528\u8fd9\u79cd\u53ef\u5206\u6027\uff0c\u63a8\u7406\u76f4\u63a5\u5728\u8868\u793a\u7a7a\u95f4\u8fdb\u884c\uff0c\u786e\u4fdd\u4e0e\u5224\u522b\u5f0f\u4f18\u5316\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u7ed5\u8fc7\u751f\u6210\u8fc7\u7a0b\u5f15\u8d77\u7684\u5e7b\u89c9\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRADAR\u5728\u94fe\u63a5\u9884\u6d4b\u548c\u4e09\u5143\u7ec4\u5206\u7c7b\u4efb\u52a1\u4e0a\u76f8\u6bd4\u5f3aLLM\u57fa\u7ebf\u83b7\u5f975-6%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002\u4e2d\u95f4\u8868\u793a\u7684\u4efb\u52a1\u76f8\u5173\u4e92\u4fe1\u606f\u589e\u52a0\u4e8662.9%\uff0c\u8868\u660e\u83b7\u5f97\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u8fc1\u79fb\u7684\u5173\u7cfb\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "RADAR\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4ece\u751f\u6210\u5f0f\u8303\u5f0f\u8f6c\u5411\u5224\u522b\u5f0f\u5173\u7cfb\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u8868\u9762\u5171\u73b0\u6a21\u5f0f\u7684\u4f9d\u8d56\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u5173\u7cfb\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.21978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21978", "abs": "https://arxiv.org/abs/2602.21978", "authors": ["Miyu Oba", "Saku Sugawara"], "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models", "comment": null, "summary": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models.", "AI": {"tldr": "CxMP\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e\u6784\u5f0f\u8bed\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5bf9\u6bd4\u5bf9\u8bbe\u8ba1\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5bf9\u6784\u5f0f\uff08\u5f62\u5f0f-\u610f\u4e49\u914d\u5bf9\uff09\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6784\u5f0f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u5224\u65ad\uff0c\u800c\u5bf9\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u8bed\u6cd5\u5f62\u5f0f\u6240\u4f20\u8fbe\u610f\u4e49\u7684\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u8bc4\u4f30\u6784\u5f0f\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u57fa\u4e8e\u6784\u5f0f\u8bed\u6cd5\u7406\u8bba\uff0c\u8bbe\u8ba1\u5305\u542b\u4e5d\u79cd\u6784\u5f0f\u7c7b\u578b\uff08\u5982let-alone\u3001\u81f4\u4f7f\u79fb\u52a8\u3001\u53cc\u53ca\u7269\u6784\u5f0f\u7b49\uff09\u7684\u6700\u5c0f\u5bf9\u6bd4\u5bf9\u57fa\u51c6\u6d4b\u8bd5CxMP\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u6784\u5f0f\u9690\u542b\u8bed\u4e49\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u53e5\u6cd5\u80fd\u529b\u8f83\u65e9\u51fa\u73b0\uff0c\u4f46\u6784\u5f0f\u7406\u89e3\u53d1\u5c55\u8f83\u6162\uff0c\u5373\u4f7f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002\u6a21\u578b\u5728\u6574\u5408\u5f62\u5f0f\u4e0e\u610f\u4e49\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "CxMP\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u6784\u5f0f\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u6784\u5f0f\u7406\u89e3\u548c\u5b66\u4e60\u8f68\u8ff9\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u8868\u660e\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5f62\u5f0f\u4e0e\u610f\u4e49\u7684\u6574\u5408\u673a\u5236\u3002"}}
{"id": "2602.22014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22014", "abs": "https://arxiv.org/abs/2602.22014", "authors": ["Louis Est\u00e8ve", "Christophe Servan", "Thomas Lavergne", "Agata Savary"], "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT", "comment": null, "summary": "Diversity has been gaining interest in the NLP community in recent years. At the same time, state-of-the-art transformer models such as ModernBERT use very large pre-training datasets, which are driven by size rather than by diversity. This summons for an investigation of the impact of diversity on the ModernBERT pre-training. We do so in this study, with the express intent of reducing pre-training dataset size, while retaining at least comparable performance. We compare diversity-driven sampling algorithms, so as to pick the best one. We find that diversity-driven sampling allows in some tasks to gain 10 points relative to randomly-sampled pre-training data of commensurate size. We also see that a model pre-trained for 483h on a diversity-driven dataset of 150M tokens can yield a commensurate performance to a model pre-trained for 1,775h on a randomly-driven dataset of 2.4B tokens.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u6837\u6027\u5bf9ModernBERT\u9884\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u591a\u6837\u6027\u9a71\u52a8\u91c7\u6837\u51cf\u5c11\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684transformer\u6a21\u578b\u5982ModernBERT\u4f7f\u7528\u57fa\u4e8e\u89c4\u6a21\u800c\u975e\u591a\u6837\u6027\u7684\u8d85\u5927\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u9700\u8981\u7814\u7a76\u591a\u6837\u6027\u5bf9\u9884\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u51cf\u5c11\u6570\u636e\u96c6\u89c4\u6a21\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u591a\u6837\u6027\u9a71\u52a8\u7684\u91c7\u6837\u7b97\u6cd5\uff0c\u9009\u62e9\u6700\u4f73\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u6837\u6027\u9a71\u52a8\u91c7\u6837\u6784\u5efa\u8f83\u5c0f\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u591a\u6837\u6027\u9a71\u52a8\u91c7\u6837\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u6bd4\u540c\u7b49\u89c4\u6a21\u7684\u968f\u673a\u91c7\u6837\u6570\u636e\u96c6\u6027\u80fd\u63d0\u534710\u5206\uff1b\u4f7f\u75281.5\u4ebftoken\u7684\u591a\u6837\u6027\u9a71\u52a8\u6570\u636e\u96c6\u9884\u8bad\u7ec3483\u5c0f\u65f6\u7684\u6a21\u578b\uff0c\u4e0e\u4f7f\u752824\u4ebftoken\u968f\u673a\u91c7\u6837\u6570\u636e\u96c6\u9884\u8bad\u7ec31775\u5c0f\u65f6\u7684\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u591a\u6837\u6027\u9a71\u52a8\u91c7\u6837\u80fd\u663e\u8457\u51cf\u5c11\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.22045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22045", "abs": "https://arxiv.org/abs/2602.22045", "authors": ["Walter Hernandez Cruz", "Peter Devine", "Nikhil Vadgama", "Paolo Tasca", "Jiahua Xu"], "title": "DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain", "comment": null, "summary": "We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.", "AI": {"tldr": "DLT-Corpus\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f\u9886\u57df\u7279\u5b9a\u6587\u672c\u8bed\u6599\u5e93\uff0c\u5305\u542b29.8\u4ebf\u4e2atoken\u548c2212\u4e07\u4efd\u6587\u6863\uff0c\u6db5\u76d6\u79d1\u5b66\u6587\u732e\u3001\u4e13\u5229\u548c\u793e\u4ea4\u5a92\u4f53\uff0c\u5e76\u53d1\u5e03\u4e86\u9886\u57df\u9002\u5e94\u7684LedgerBERT\u6a21\u578b\u3002", "motivation": "\u73b0\u6709NLP\u8d44\u6e90\u4e3b\u8981\u5173\u6ce8\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u9884\u6d4b\u548c\u667a\u80fd\u5408\u7ea6\uff0c\u7f3a\u4e4f\u5bf9\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u800c\u8be5\u9886\u57df\u5e02\u503c\u7ea63\u4e07\u4ebf\u7f8e\u5143\u4e14\u6280\u672f\u53d1\u5c55\u8fc5\u901f\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u79d1\u5b66\u6587\u732e\uff0837,440\u7bc7\uff09\u3001\u7f8e\u56fd\u4e13\u5229\u5546\u6807\u5c40\u4e13\u5229\uff0849,023\u9879\uff09\u548c\u793e\u4ea4\u5a92\u4f53\uff082200\u4e07\u6761\u5e16\u5b50\uff09\u7684DLT-Corpus\u8bed\u6599\u5e93\uff0c\u5e76\u5f00\u53d1\u4e86\u9886\u57df\u9002\u5e94\u7684LedgerBERT\u6a21\u578b\u3002", "result": "\u5206\u6790\u663e\u793a\u6280\u672f\u9075\u5faa\u4f20\u7edf\u8f6c\u79fb\u6a21\u5f0f\uff1a\u4ece\u79d1\u5b66\u6587\u732e\u5230\u4e13\u5229\u518d\u5230\u793e\u4ea4\u5a92\u4f53\uff1b\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\u59cb\u7ec8\u4fdd\u6301\u770b\u6da8\uff0c\u800c\u79d1\u5b66\u548c\u4e13\u5229\u6d3b\u52a8\u72ec\u7acb\u4e8e\u5e02\u573a\u6ce2\u52a8\uff0c\u4e0e\u6574\u4f53\u5e02\u573a\u6269\u5f20\u5f62\u6210\u826f\u6027\u5faa\u73af\u3002", "conclusion": "DLT-Corpus\u586b\u8865\u4e86\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u8d44\u6e90\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86DLT\u6280\u672f\u8f6c\u79fb\u6a21\u5f0f\u548c\u5e02\u573a-\u521b\u65b0\u5173\u7cfb\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5e76\u516c\u5f00\u4e86\u8bed\u6599\u5e93\u3001\u6a21\u578b\u548c\u5de5\u5177\u3002"}}
{"id": "2602.22090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22090", "abs": "https://arxiv.org/abs/2602.22090", "authors": ["Bo-Wei Chen", "Chung-Chi Chen", "An-Zi Yen"], "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference", "comment": "Accepted by EACL 2026 Findings", "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u6a21\u578b\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u4efb\u52a1\u7684\u7f6e\u4fe1\u5ea6\u548c\u54cd\u5e94\u51c6\u786e\u6027\uff0c\u5c06\u7b80\u5355\u4efb\u52a1\u4fdd\u7559\u5728\u5c0f\u6a21\u578b\uff0c\u590d\u6742\u4efb\u52a1\u59d4\u6258\u7ed9\u5927\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5927\u6a21\u578b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6a21\u578b\u9009\u62e9\u7b56\u7565\uff1a1\uff09\u8bc4\u4f30\u6a21\u578b\u77e5\u9053\u6b63\u786e\u7b54\u6848\u7684\u53ef\u80fd\u6027\uff1b2\uff09\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u51c6\u786e\u6027\u7684\u6982\u7387\uff1b3\uff09\u6839\u636e\u7f6e\u4fe1\u5ea6\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\uff0c\u5c06\u7f6e\u4fe1\u5ea6\u9ad8\u7684\u4efb\u52a1\u4fdd\u7559\u5728\u5c0f\u6a21\u578b\uff0c\u7f6e\u4fe1\u5ea6\u4f4e\u6216\u590d\u6742\u7684\u4efb\u52a1\u59d4\u6258\u7ed9\u5927\u6a21\u578b\u3002", "result": "\u5728MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e8620%-40%\u3002\u5e94\u7528\u4e8eGPT-4o API\u8c03\u7528\u65f6\uff0ctoken\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86\u7ea660%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6a21\u578b\u9009\u62e9\u7b56\u7565\u80fd\u591f\u6709\u6548\u5e73\u8861\u63a8\u7406\u8d28\u91cf\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u5982\u8fb9\u7f18\u8bbe\u5907\u548c\u5546\u4e1aAPI\u5e94\u7528\uff0c\u4e3a\u5b9e\u9645LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22125", "abs": "https://arxiv.org/abs/2602.22125", "authors": ["Thanmay Jayakumar", "Mohammed Safi Ur Rahman Khan", "Raj Dabre", "Ratish Puduppully", "Anoop Kunchukuttan"], "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages", "comment": "8 pages + Appendix", "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval).", "code_url": "https://github.com/ai4bharat/IndicIFEval", "AI": {"tldr": "IndicIFEval\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u572814\u79cd\u5370\u5ea6\u8bed\u8a00\u4e2d\u7ea6\u675f\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u81ea\u52a8\u53ef\u9a8c\u8bc1\u7684\u89c4\u5219\u6307\u4ee4\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5370\u5ea6\u8bed\u8a00\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u663e\u8457\u843d\u540e\u4e8e\u82f1\u8bed\u3002", "motivation": "\u76ee\u524d\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u5370\u5ea6\u8bed\u8a00\u4f7f\u7528\u8005\u7684\u8bc4\u4f30\uff0c\u5b58\u5728\u5173\u952e\u8bc4\u4f30\u7f3a\u53e3\u3002\u5370\u5ea6\u6709\u6570\u4ebf\u6bcd\u8bed\u4f7f\u7528\u8005\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u7ea6\u675f\u751f\u6210\u80fd\u529b\u3002", "method": "\u521b\u5efaIndicIFEval\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u5b50\u96c6\uff1aIndicIFEval-Ground\uff08\u4eceIFEval\u7ffb\u8bd1\u5e76\u672c\u5730\u5316\u7684\u63d0\u793a\uff09\u548cIndicIFEval-Ground\uff08\u57fa\u4e8e\u539f\u751f\u5370\u5ea6\u5185\u5bb9\u5408\u6210\u7684\u6307\u4ee4\uff09\u3002\u6bcf\u4e2a\u8bed\u8a00\u7ea6800\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u793a\u4f8b\uff0c\u4f7f\u7528\u81ea\u52a8\u53ef\u9a8c\u8bc1\u7684\u89c4\u5219\u6307\u4ee4\u3002\u5bf9\u4e3b\u8981\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u6db5\u76d6\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u683c\u5f0f\u7ea6\u675f\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bcd\u6c47\u548c\u8de8\u8bed\u8a00\u4efb\u52a1\u4e0a\u663e\u8457\u56f0\u96be\u3002\u5c3d\u7ba1\u9ad8\u8d44\u6e90\u8bed\u8a00\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u6574\u4e2a\u5370\u5ea6\u8bed\u7cfb\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u663e\u8457\u843d\u540e\u4e8e\u82f1\u8bed\u3002", "conclusion": "\u9700\u8981\u66f4\u591a\u5de5\u4f5c\u63d0\u5347LLMs\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002IndicIFEval\u57fa\u51c6\u53ca\u5176\u8bc4\u4f30\u811a\u672c\u5df2\u5f00\u6e90\uff0c\u652f\u6301\u591a\u8bed\u8a00\u7ea6\u675f\u751f\u6210\u7684\u8fdb\u5c55\u3002"}}
{"id": "2602.22157", "categories": ["cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22157", "abs": "https://arxiv.org/abs/2602.22157", "authors": ["Leon Pielage", "Ole H\u00e4tscher", "Mitja Back", "Bernhard Marschall", "Benjamin Risse"], "title": "Dynamic Personality Adaptation in Large Language Models via State Machines", "comment": "22 pages, 5 figures, submitted to ICPR 2026", "summary": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u52a8\u6001\u4eba\u683c\u6a21\u62df\u6846\u67b6\uff0c\u4f7f\u7528\u72b6\u6001\u673a\u8868\u793a\u6f5c\u5728\u4eba\u683c\u72b6\u6001\uff0c\u901a\u8fc7\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u8f6c\u79fb\u6982\u7387\uff0c\u5b9e\u73b0LLM\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u4eba\u683c\u8868\u8fbe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6839\u636e\u5bf9\u8bdd\u52a8\u6001\u8c03\u6574\u4eba\u683c\u8868\u8fbe\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u6a21\u62df\u4eba\u683c\u7684\u6846\u67b6\u6765\u63d0\u5347LLM\u5728\u533b\u7597\u6559\u80b2\u3001\u5ba2\u6237\u652f\u6301\u7b49\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684\u52a8\u6001\u4eba\u683c\u6a21\u62df\u6846\u67b6\uff1a1) \u4f7f\u7528\u72b6\u6001\u673a\u8868\u793a\u6f5c\u5728\u4eba\u683c\u72b6\u6001\uff0c\u8f6c\u79fb\u6982\u7387\u6839\u636e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\uff1b2) \u8bbe\u8ba1\u6a21\u5757\u5316\u7684\u4eba\u683c\u8bc4\u5206\u7ba1\u9053\uff0c\u6cbf\u6f5c\u5728\u8f74\u8bc4\u4f30\u5bf9\u8bdd\uff0c\u72ec\u7acb\u4e8e\u5177\u4f53\u4eba\u683c\u6a21\u578b\u3001\u7ef4\u5ea6\u3001\u8f6c\u79fb\u673a\u5236\u6216LLM\uff1b3) \u8bc4\u5206\u4f5c\u4e3a\u52a8\u6001\u72b6\u6001\u53d8\u91cf\u7cfb\u7edf\u6027\u5730\u91cd\u65b0\u914d\u7f6e\u7cfb\u7edf\u63d0\u793a\uff0c\u5f15\u5bfc\u884c\u4e3a\u5bf9\u9f50\u3002", "result": "\u5728\u533b\u7597\u6559\u80b2\u573a\u666f\u4e2d\u64cd\u4f5c\u5316\u4eba\u9645\u73af\u6a21\u578b(IPC)\u8fdb\u884c\u8bc4\u4f30\uff1a1) \u7cfb\u7edf\u6210\u529f\u6839\u636e\u7528\u6237\u8f93\u5165\u8c03\u6574\u4eba\u683c\u72b6\u6001\uff1b2) \u7cfb\u7edf\u80fd\u591f\u5f71\u54cd\u7528\u6237\u884c\u4e3a\uff0c\u4fc3\u8fdb\u964d\u7ea7\u8bad\u7ec3\uff1b3) \u8bc4\u5206\u7ba1\u9053\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5206\u7c7b\u5668\u800c\u975e\u5927\u89c4\u6a21LLM\u65f6\u4ecd\u4fdd\u6301\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6a21\u5757\u5316\u3001\u4eba\u683c\u81ea\u9002\u5e94\u67b6\u6784\u5728\u6559\u80b2\u3001\u5ba2\u6237\u652f\u6301\u548c\u66f4\u5e7f\u6cdb\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u53ef\u884c\u6027\uff0c\u4e3aLLM\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u4eba\u683c\u8868\u8fbe\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22175", "abs": "https://arxiv.org/abs/2602.22175", "authors": ["Xi Ye", "Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen"], "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs", "comment": null, "summary": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.", "code_url": "https://github.com/princeton-pli/DySCO", "code_stars": 0, "code_last_update": "2026-02-26", "AI": {"tldr": "DySCO\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u867d\u7136\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u968f\u7740\u8f93\u5165\u957f\u5ea6\u589e\u52a0\uff0c\u5176\u51c6\u786e\u6027\u4f1a\u4e0b\u964d\u3002\u6a21\u578b\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u4fdd\u6301\u6ce8\u610f\u529b\u4e0e\u6700\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u5bf9\u9f50", "method": "DySCO\u5229\u7528\u68c0\u7d22\u5934\uff08\u4e13\u95e8\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684\u6ce8\u610f\u529b\u5934\u5b50\u96c6\uff09\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u8bc6\u522b\u4efb\u52a1\u76f8\u5173token\uff0c\u5e76\u663e\u5f0f\u63d0\u5347\u5176\u6743\u91cd\uff0c\u4ece\u800c\u52a8\u6001\u8c03\u6574\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u5e03", "result": "\u5728\u591a\u4e2a\u6307\u4ee4\u8c03\u4f18\u548c\u63a8\u7406\u6a21\u578b\u4e0a\uff0cDySCO\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\uff0c\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cMRCR\u548cLongBenchV2\u4e0a\u83b7\u5f97\u9ad8\u8fbe25%\u7684\u76f8\u5bf9\u589e\u76ca", "conclusion": "DySCO\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6210\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u610f\u529b\u91cd\u7f29\u653e\u548c\u68c0\u7d22\u5934\u5f15\u5bfc\u7684\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4e3a\u89e3\u7801\u65f6\u6ce8\u610f\u529b\u884c\u4e3a\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf"}}
{"id": "2602.22193", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22193", "abs": "https://arxiv.org/abs/2602.22193", "authors": ["Melody Ma", "John Hewitt"], "title": "Improving Parametric Knowledge Access in Reasoning Language Models", "comment": null, "summary": "We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowledge reasoning by default: adding a simple \"think step-by-step\" cue demonstrates statistically significant improvement in knowledge recall but not math. Motivated by this, we propose training models to reason over their parametric knowledge using world-knowledge question answering as a verifiable reward. After reinforcement learning on TriviaQA (+9.9%), performance also improves on Natural Questions, HotpotQA, SimpleQA, and StrategyQA by 4.2%, 2.1%, 0.6%, and 3.0%, respectively. Reasoning models are under-optimized for parametric knowledge access, but can be easily trained to reason better.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u8bbf\u95ee\u81ea\u8eab\u53c2\u6570\u5316\u77e5\u8bc6\u65f6\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u68c0\u7d22\u6548\u679c", "motivation": "\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u901a\u8fc7\u63a8\u7406\u89e3\u51b3\u6570\u5b66\u95ee\u9898\uff0c\u4f46\u5728\u8bbf\u95ee\u81ea\u8eab\u5b58\u50a8\u7684\u4e16\u754c\u77e5\u8bc6\u65f6\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u4f18\u5316", "method": "\u4f7f\u7528\"\u9010\u6b65\u601d\u8003\"\u63d0\u793a\u8bcd\u9a8c\u8bc1\u6a21\u578b\u77e5\u8bc6\u68c0\u7d22\u80fd\u529b\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728TriviaQA\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u63a8\u7406", "result": "TriviaQA\u6027\u80fd\u63d0\u53479.9%\uff0cNatural Questions\u3001HotpotQA\u3001SimpleQA\u3001StrategyQA\u5206\u522b\u63d0\u53474.2%\u30012.1%\u30010.6%\u30013.0%", "conclusion": "\u63a8\u7406\u6a21\u578b\u5728\u53c2\u6570\u5316\u77e5\u8bc6\u8bbf\u95ee\u65b9\u9762\u5b58\u5728\u4f18\u5316\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u77e5\u8bc6\u63a8\u7406\u80fd\u529b"}}
{"id": "2602.22200", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22200", "abs": "https://arxiv.org/abs/2602.22200", "authors": ["Cole Simmons", "Richard Diehl Martinez", "Dan Jurafsky"], "title": "SumTablets: A Transliteration Dataset of Sumerian Tablets", "comment": "11 pages with 3 figures", "summary": "Sumerian transliteration is a conventional system for representing a scholar's interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet's cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration.\n  To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub.\n  Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph's possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.", "AI": {"tldr": "SumTablets\u6570\u636e\u96c6\u5c0691,606\u5757\u82cf\u7f8e\u5c14\u6954\u5f62\u6587\u5b57\u6ce5\u677f\u7684Unicode\u8868\u793a\u4e0eOracc\u53d1\u5e03\u7684\u97f3\u8bd1\u6587\u672c\u914d\u5bf9\uff0c\u586b\u8865\u4e86\u6954\u5f62\u6587\u5b57\u4e0e\u97f3\u8bd1\u4e4b\u95f4\u7f3a\u4e4f\u5927\u89c4\u6a21\u5bf9\u9f50\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u82cf\u7f8e\u5c14\u97f3\u8bd1\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u82cf\u7f8e\u5c14\u6954\u5f62\u6587\u5b57\u6ce5\u677f\u7684\u97f3\u8bd1\u6570\u636e\u5728\u7ebf\u53d1\u5e03\uff0c\u4f46\u7f3a\u4e4f\u5c06\u97f3\u8bd1\u6587\u672c\u4e0e\u6954\u5f62\u6587\u5b57Unicode\u8868\u793a\u914d\u5bf9\u7684\u5927\u89c4\u6a21\u3001\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86\u73b0\u4ee3NLP\u65b9\u6cd5\u5728\u82cf\u7f8e\u5c14\u97f3\u8bd1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "1. \u9884\u5904\u7406\u548c\u6807\u51c6\u5316Oracc\u97f3\u8bd1\u6570\u636e\uff1b2. \u5c06\u6bcf\u4e2a\u97f3\u8bd1\u6620\u5c04\u56de\u6e90\u6954\u5f62\u6587\u5b57\u7684Unicode\u8868\u793a\uff1b3. \u4f7f\u7528\u7279\u6b8a\u6807\u8bb0\u4fdd\u7559\u5e73\u884c\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u8868\u9762\u3001\u6362\u884c\u3001\u7834\u635f\u7247\u6bb5\uff09\uff1b4. \u6784\u5efa\u5305\u542b91,606\u5757\u6ce5\u677f\u30016,970,407\u4e2a\u6954\u5f62\u6587\u5b57\u7684\u6570\u636e\u96c6\uff1b5. \u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e24\u79cd\u97f3\u8bd1\u57fa\u7ebf\u65b9\u6cd5\uff1a\u52a0\u6743\u91c7\u6837\u548c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u3002", "result": "1. \u53d1\u5e03\u4e86SumTablets\u6570\u636e\u96c6\uff08CC BY 4.0\u8bb8\u53ef\uff09\uff0c\u5305\u542b\u6954\u5f62\u6587\u5b57Unicode\u8868\u793a\u4e0e\u97f3\u8bd1\u7684\u914d\u5bf9\uff1b2. \u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5b57\u7b26\u7ea7F\u5206\u6570\uff08chrF\uff09\u4e0a\u8fbe\u523097.55\u7684\u5e73\u5747\u5206\uff0c\u663e\u793a\u51fa\u57fa\u4e8eTransformer\u7684\u97f3\u8bd1\u6a21\u578b\u5728\u8f85\u52a9\u4e13\u5bb6\u5feb\u901f\u9a8c\u8bc1\u751f\u6210\u97f3\u8bd1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "SumTablets\u6570\u636e\u96c6\u586b\u8865\u4e86\u82cf\u7f8e\u5c14\u6954\u5f62\u6587\u5b57\u97f3\u8bd1\u7814\u7a76\u7684\u5173\u952e\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u5e94\u7528\u73b0\u4ee3NLP\u65b9\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u5b9e\u9a8c\u8868\u660e\u57fa\u4e8eTransformer\u7684\u97f3\u8bd1\u6a21\u578b\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u5e2e\u52a9\u4e13\u5bb6\u4ece\u624b\u52a8\u9010\u5757\u97f3\u8bd1\u8f6c\u5411\u5feb\u901f\u9a8c\u8bc1\u751f\u6210\u97f3\u8bd1\uff0c\u63d0\u9ad8\u7814\u7a76\u6548\u7387\u3002"}}
