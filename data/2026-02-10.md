<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.IT](#cs.IT) [Total: 13]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 该论文提出使用视角不变语义特征作为条件输入，以解决传统全头3D GAN中因使用视角角度作为条件导致的生成偏差和全局不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统全头3D GAN使用视角角度作为条件输入，导致学习到的3D头空间沿条件视角方向产生偏差，表现为条件视角与非条件视角在生成质量和多样性上的显著差异，造成不同头部区域的全局不一致性。

Method: 提出使用视角不变语义特征作为条件输入。通过FLUX.1 Kontext将现有高质量正面人脸数据集扩展到多视角，提取正面视角的图像clip特征作为所有视角的共享语义条件，确保语义对齐同时消除方向偏差。

Result: 在全头合成和单视角GAN反演实验中，该方法在保真度、多样性和泛化能力方面显著优于传统方法，实现了更高质量的3D头部生成。

Conclusion: 使用视角不变语义条件能够解耦3D头部的生成能力与视角方向，消除方向偏差，提高全局一致性，并促进生成器的持续学习和多样性生成。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [2] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: VideoNeuMat：两阶段流水线从视频扩散模型中提取可重用神经材质资产，通过虚拟测角反射计生成材质样本视频，再通过大型重建模型重建为神经材质参数


<details>
  <summary>Details</summary>
Motivation: 创建逼真的3D渲染材质需要高超的艺术技能，而生成模型受限于高质量训练数据的缺乏。虽然视频生成模型能产生逼真的材质外观，但这些知识与几何和光照纠缠在一起，无法直接重用

Method: 两阶段方法：1）微调大型视频模型（Wan 2.1 14B）在受控相机和光照轨迹下生成材质样本视频，创建"虚拟测角反射计"；2）通过从较小Wan 1.3B视频骨干微调的大型重建模型（LRM），从17个生成视频帧中单次推理预测神经材质参数

Result: 生成的材质在真实感和多样性方面远超有限的合成训练数据，神经材质参数能够泛化到新的观察和光照条件，成功将互联网规模视频模型中的材质知识转移到独立的可重用神经3D资产中

Conclusion: VideoNeuMat证明了可以从视频扩散模型中成功提取材质知识，并将其转化为可重用的神经3D材质资产，解决了高质量材质训练数据缺乏的问题，为3D渲染提供了高质量的材质生成解决方案

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [3] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出一种从超高速运动模糊图像恢复3D形状的逆渲染方法，通过快速重心坐标求解器显著提升计算效率，实现高效逼真的高速运动模拟和3D重建。


<details>
  <summary>Details</summary>
Motivation: 在自然和工业场景中，高速运动物体（如体育中的球体、旋转机械）会产生严重运动模糊，传统多视图立体等3D重建方法失效，需要新的解决方案从极端运动模糊图像中恢复几何形状。

Method: 提出一种新颖的逆渲染方法：1）开发快速重心坐标求解器，解决传统渲染中重复计算重心权重的计算瓶颈，实现4.57倍加速；2）构建完全可微分的渲染流程，使梯度能从渲染图像传播到底层3D形状；3）针对快速平移和旋转两种典型运动类型进行验证。

Result: 实验结果表明：1）前向模拟中能高效逼真地建模超高速运动物体；2）成功从经历极端平移和旋转运动的物体2D图像中恢复3D形状；3）计算效率显著提升，为基于视觉的3D重建开辟新边界。

Conclusion: 该方法通过创新的快速重心坐标求解器和完全可微分逆渲染框架，有效解决了从超高速运动模糊图像恢复3D形状的挑战，在计算效率和重建质量方面均取得显著进展，推动了高速运动场景下的3D重建技术发展。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [4] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus是一个能够从参考形状中提取可重用的几何和外观属性，并与文本结合生成个性化3D形状的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D形状个性化生成方面存在局限性，需要能够同时学习几何和外观概念，并支持细粒度控制和跨类别场景的灵活生成。

Method: 1) 将3D形状个性化定义为提取类别无关的几何和外观属性；2) 设计渐进优化策略解耦几何和外观概念学习；3) 扩展到区域级概念学习，使用上下文感知和无上下文损失。

Result: 实验表明PEGAsus能够从广泛参考形状中有效提取属性，并与文本灵活组合生成新形状，支持细粒度控制和多样化个性化结果，在跨类别场景中表现优异。

Conclusion: PEGAsus通过解耦几何和外观概念学习，实现了高效的3D形状个性化生成，在定量和定性评估中均优于现有最先进方法。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [5] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出TIBR4D框架，通过两阶段迭代边界精化实现动态4D高斯场景的无学习对象分割，提升遮挡处理和边界精度


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的对象级分割面临复杂运动、遮挡和模糊边界的挑战，现有方法难以有效处理

Method: 提出两阶段迭代边界精化框架：1) 时间片段级的迭代高斯实例追踪(IGIT)，通过迭代追踪精化高斯到实例的概率；2) 帧级高斯渲染范围控制(RCC)，抑制边界附近不确定高斯；结合时间分割合并策略平衡身份一致性和动态感知

Result: 在HyperNeRF和Neu3D数据集上验证，相比SOTA方法能生成边界更清晰、效率更高的准确对象高斯点云

Conclusion: TIBR4D框架有效解决了动态4D高斯场景的对象分割问题，通过迭代精化和边界控制机制显著提升了分割质量和效率

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [6] [An Automata-Based Approach to Games with $ω$-Automatic Preferences](https://arxiv.org/abs/2602.08549)
*Véronique Bruyère,Emmanuel Filiot,Christophe Grandmont,Jean-François Raskin*

Main category: cs.GT

TL;DR: 本文研究基于ω-自动关系建模玩家偏好的多人回合制图游戏，分析其计算复杂性，包括零和对抗情况下的阈值问题、价值概念、最优策略以及纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定奖励函数，而本文采用更一般的ω-自动关系（由确定性奇偶自动机给出）来建模玩家偏好，以进行更全面的计算分析。

Method: 1. 引入价值概念（玩家能保证改进的玩法集合），证明该集合可由多项式大小的交替奇偶自动机识别；2. 利用ω-自动特性分析价值和最优策略相关问题的计算复杂性；3. 扩展到多人游戏和纳什均衡，基于APW构造解决文献中的复杂性缺口。

Result: 1. 价值集合可由多项式大小的APW识别；2. 确定了价值和最优策略相关问题的计算复杂性；3. 填补了多人游戏中阈值问题的复杂性缺口；4. 证明合作理性综合是PSPACE完全的，而非合作情况下不可判定。

Conclusion: 本文通过ω-自动关系为玩家偏好建模，提供了图游戏中计算问题的系统分析框架，解决了零和对抗和多人游戏中的关键复杂性结果，特别是合作理性综合的可判定性和非合作情况下的不可判定性。

Abstract: This paper studies multiplayer turn-based games on graphs in which player preferences are modeled as $ω$-automatic relations given by deterministic parity automata. This contrasts with most existing work, which focuses on specific reward functions. We conduct a computational analysis of these games, starting with the threshold problem in the antagonistic zero-sum case. As in classical games, we introduce the concept of value, defined here as the set of plays a player can guarantee to improve upon, relative to their preference relation. We show that this set is recognized by an alternating parity automaton APW of polynomial size. We also establish the computational complexity of several problems related to the concepts of value and optimal strategy, taking advantage of the $ω$-automatic characterization of value. Next, we shift to multiplayer games and Nash equilibria, and revisit the threshold problem in this context. Based on an APW construction again, we close complexity gaps left open in the literature, and additionally show that cooperative rational synthesis is $\mathsf{PSPACE}$-complete, while it becomes undecidable in the non-cooperative case.

</details>


### [7] [Distortion of Metric Voting with Bounded Randomness](https://arxiv.org/abs/2602.08871)
*Ziyi Cai,D. D. Gao,Prasanna Ramakrishnan,Kangning Wang*

Main category: cs.GT

TL;DR: 论文提出了一种使用有限随机性突破度量失真3障碍的投票规则，在确定性识别出的常数大小候选者列表中均匀随机选择获胜者，实现最多3-ε的失真。


<details>
  <summary>Details</summary>
Motivation: 研究度量失真框架中投票规则的设计，探索确定性和随机性之间的权衡。已知确定性规则至少遭受3的失真，而随机规则可以实现小于3的失真，但通常以降低透明度和可解释性为代价。本文旨在探究是否可以使用"有界"随机性突破失真3的障碍。

Method: 提出一种新的投票规则：首先确定性地识别出一个常数大小的候选者列表，然后从这个列表中均匀随机选择获胜者。分析基于最大彩票和稳定彩票的失真和近似性的新结构结果。

Result: 成功证明了使用有限随机性可以突破失真3的障碍，提出的投票规则实现了最多3-ε的失真（其中ε>0是绝对常数），同时保持了较好的透明度和可解释性。

Conclusion: 该研究在度量失真框架中找到了确定性和完全随机性之间的有效平衡点，通过使用有界随机性实现了优于确定性规则的失真性能，同时避免了完全随机规则在透明度和可解释性方面的缺陷。

Abstract: We study the design of voting rules in the metric distortion framework. It is known that any deterministic rule suffers distortion of at least $3$, and that randomized rules can achieve distortion strictly less than $3$, often at the cost of reduced transparency and interpretability. In this work, we explore the trade-off between these paradigms by asking whether it is possible to break the distortion barrier of $3$ using only "bounded" randomness. We answer in the affirmative by presenting a voting rule that (1) achieves distortion of at most $3 - \varepsilon$ for some absolute constant $\varepsilon > 0$, and (2) selects a winner uniformly at random from a deterministically identified list of constant size. Our analysis builds on new structural results for the distortion and approximation of Maximal Lotteries and Stable Lotteries.

</details>


### [8] [Maximin Shares with Lower Quotas](https://arxiv.org/abs/2602.08966)
*Hirota Kinoshita,Ayumi Igarashi*

Main category: cs.GT

TL;DR: 该论文研究了在个体异质性加性估值下，带有上下配额约束的不可分割物品公平分配问题，提出了针对物品和杂务的近似最大化最小份额（MMS）分配算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于实际应用中需要满足上下配额约束的公平分配问题，如人员分配和计算资源分配等。现有研究主要关注基数约束（仅上配额），而实际应用中常同时存在上下配额约束，需要更通用的理论框架。

Method: 采用最大化最小份额（MMS）作为公平性标准，设计多项式时间算法。对于一般上下配额约束，为物品设计(2n/(3n-1))-MMS算法，为杂务设计((3n-1)/(2n))-MMS算法。对于多类别分区场景，为物品设计(n/(2n-1))-MMS算法，为杂务设计((2n-1)/n)-MMS算法。

Result: 证明了在任意上下配额约束下，存在(2n/(3n-1))-MMS物品分配和((3n-1)/(2n))-MMS杂务分配，且可在多项式时间内计算。在多类别分区场景下，存在(n/(2n-1))-MMS物品分配和((2n-1)/n)-MMS杂务分配，同样可在多项式时间内计算。

Conclusion: 该研究扩展了基数约束下的公平分配理论，为具有上下配额约束的不可分割物品分配问题提供了有效的近似MMS算法，适用于更广泛的实际应用场景。

Abstract: We study the fair division of indivisible items among $n$ agents with heterogeneous additive valuations, subject to lower and upper quotas on the number of items allocated to each agent. Such constraints are crucial in various applications, ranging from personnel assignments to computing resource distribution. This paper focuses on the fairness criterion known as maximin shares (MMS) and its approximations. Under arbitrary lower and upper quotas, we show that a $\left(\frac{2n}{3n-1}\right)$-MMS allocation of goods exists and can be computed in polynomial time, while we also present a polynomial-time algorithm for finding a $\left(\frac{3n-1}{2n}\right)$-MMS allocation of chores. Furthermore, we consider the generalized scenario where items are partitioned into multiple categories, each with its own lower and upper quotas. In this setting, our algorithm computes an $\left(\frac{n}{2n-1}\right)$-MMS allocation of goods or a $\left(\frac{2n-1}{n}\right)$-MMS allocation of chores in polynomial time. These results extend previous work on the cardinality constraints, i.e., the special case where only upper quotas are imposed.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [9] [Convex Primitive Decomposition for Collision Detection](https://arxiv.org/abs/2602.07369)
*Julian Knodt,Xifeng Gao*

Main category: cs.GR

TL;DR: 该论文提出了一种用于3D模型的凸体基元分解方法，通过自底向上的方式将复杂网格分解为凸体基元（如包围盒、胶囊体、球体等），用于刚体模拟碰撞检测，相比现有方法具有更好的性能和更低的复杂度。


<details>
  <summary>Details</summary>
Motivation: 为3D模型创建碰撞体是一项耗时的手动任务，现有基于凸包的自适应凸分解方法在游戏等性能敏感应用中不实用，因为碰撞检测速度慢且难以手动修改输出同时保持凸性。需要一种更实用的方法生成适合刚体模拟的碰撞体。

Method: 提出了一种受四元网格简化启发的自底向上分解方法，将输入网格分解为凸体基元（如包围盒、胶囊体、球体等）。该方法保证生成的凸体基元完全包围输入表面，并针对刚体模拟进行优化。

Result: 在超过60个Sketchfab模型上测试，相比V-HACD和CoACD，凸体基元分解具有更低的单向平均和中值Hausdorff距离和Chamfer距离，复杂度（总字节数）不到三分之一。在24个测试模型上，刚体模拟性能（挂钟时间）持续改善。

Conclusion: 凸体基元分解方法能够为复杂真实世界网格生成合理的碰撞体，在保持凸性的同时提供更好的模拟性能和更低的复杂度，适用于游戏等性能敏感应用。

Abstract: Creation of collision objects for 3D models is a time-consuming task, requiring modelers to manually place primitives such as bounding boxes, capsules, spheres, and other convex primitives to approximate complex meshes. While there has been work in automatic approximate convex decompositions of meshes using convex hulls, they are not practical for applications with tight performance budgets such as games due to slower collision detection and inability to manually modify the output while maintaining convexity as compared to manually placed primitives. Rather than convex decomposition with convex hulls, we devise an approach for bottom-up decomposition of an input mesh into convex primitives specifically for rigid body simulation inspired by quadric mesh simplification. This approach fits primitives to complex, real-world meshes that provide plausible simulation performance and are guaranteed to enclose the input surface. We test convex primitive decomposition on over 60 models from Sketchfab, showing the algorithm's effectiveness. On this dataset, convex primitive decomposition has lower one-way mean and median Hausdorff and Chamfer distance from the collider to the input compared to V-HACD and CoACD, with less than one-third of the complexity as measured by total bytes for each collider. On top of that, rigid-body simulation performance measured by wall-clock time is consistently improved across 24 tested models.

</details>


### [10] [Low-Rank Koopman Deformables with Log-Linear Time Integration](https://arxiv.org/abs/2602.07687)
*Yue Chang,Peter Yichen Chen,Eitan Grinspun,Maurizio M. Chiaramonte*

Main category: cs.GR

TL;DR: 提出基于低秩Koopman算子的可变形子空间模拟加速方法，通过动态模态分解参数化学习动力学演化，实现对数线性时间步长缩放和轨迹跳跃预测，并扩展为离散化无关的多形状多分辨率通用模型。


<details>
  <summary>Details</summary>
Motivation: 传统可变形体模拟需要逐时间步积分计算，计算成本高，尤其在优化任务（如控制和初始状态估计）中需要多次模拟。现有基于DMD的降阶模型局限于单一形状和离散化，限制了在涉及几何变化任务中的应用。

Method: 采用动态模态分解参数化Koopman算子，学习可变形动力学的时域演化，通过高效矩阵计算而非顺序时间积分预测未来状态。进一步提出离散化无关的扩展，学习跨多个形状和网格分辨率的共享动态行为。

Result: 方法实现对数线性时间步长缩放，允许跳过大部分轨迹同时保持精度。扩展后的模型能够泛化到不同形状和离散化，支持以前对DMD模型不切实际的快速形状优化。

Conclusion: Koopman算子学习可作为可变形模拟和设计的实用高效工具，其时间效率特别有利于依赖最终配置的优化任务，扩展能力突显了在图形学中基于Koopman的降阶模型的潜力。

Abstract: We present a low-rank Koopman operator formulation for accelerating deformable subspace simulation. Using a Dynamic Mode Decomposition (DMD) parameterization of the Koopman operator, our method learns the temporal evolution of deformable dynamics and predicts future states through efficient matrix evaluations instead of sequential time integration. This yields log-linear scaling in the number of time steps and allows large portions of the trajectory to be skipped while retaining accuracy. The resulting temporal efficiency is especially advantageous for optimization tasks such as control and initial-state estimation, where the objective often depends largely on the final configuration.
  To broaden the scope of Koopman-based reduced-order models in graphics, we introduce a discretization-agnostic extension that learns shared dynamic behavior across multiple shapes and mesh resolutions. Prior DMD-based approaches have been restricted to a single shape and discretization, which limits their usefulness for tasks involving geometry variation. Our formulation generalizes across both shape and discretization, which enables fast shape optimization that was previously impractical for DMD models. This expanded capability highlights the potential of Koopman operator learning as a practical tool for efficient deformable simulation and design.

</details>


### [11] [TABI: Tight and Balanced Interactive Atlas Packing](https://arxiv.org/abs/2602.07782)
*Floria Gu,Nicholas Vining,Alla Sheffer*

Main category: cs.GR

TL;DR: TABI：一种面向交互式应用的GPU纹理图集打包方法，在保持实时性能的同时提供接近离线方法的质量，通过紧凑化和平衡化技术显著减少图表缩放


<details>
  <summary>Details</summary>
Motivation: 现有实时GPU打包方法在交互式应用中存在明显缺陷：产生大量图表间空隙，打包结果不对称或不平衡，导致图表缩放严重。而离线方法虽然质量高但运行时间过长（比交互式要求慢2个数量级以上），无法满足内容创作工具、游戏动态图集生成、纹理空间着色等需要实时打包的应用需求。

Method: TABI（Tight And Balanced Interactive）方法采用两种图表形状近似表示以支持高效并行处理，通过水平和垂直方向紧凑化不规则形状图表间的空白空间来生成紧密打包。通过自动调整图集行宽和方向以适应不同图表高度，实现平衡化打包输出。该方法支持用户在性能和质量之间进行灵活权衡控制。

Result: 相比现有交互式方法，TABI显著减少了图表缩放，打包质量接近离线方法。同时保持交互式性能，比离线替代方案快几个数量级。该方法消除了实时打包方法中常见的大空隙和不平衡问题，实现了紧凑且平衡的打包结果。

Conclusion: TABI方法成功解决了交互式纹理图集打包中的质量与性能权衡问题，在保持实时性能的同时提供接近离线方法的打包质量，为需要实时打包的应用场景提供了可行的解决方案。

Abstract: Atlas packing is a key step in many computer graphics applications. Packing algorithms seek to arrange a set of charts within a fixed-size atlas with as little downscaling as possible. Many packing applications such as content creation tools, dynamic atlas generation for video games, and texture space shading require on-the-fly interactive atlas packing. Unfortunately, while many methods have been developed for generating tight high-quality packings, they are designed for offline settings and have running times two or more orders of magnitude greater than what is required for interactive performance. While real-time GPU packing methods exist, they significantly downscale packed charts compared to offline methods. We introduce a GPU packing method that targets interactive speeds, provides packing quality approaching that of offline methods, and supports flexible user control over the tradeoff between performance and quality. We observe that current real-time packing methods leave large gaps between charts and often produce asymmetric, or poorly balanced, packings. These artifacts dramatically degrade packing quality. Our Tight And Balanced method eliminates these artifacts while retaining Interactive performance. TABI generates tight packings by compacting empty space between irregularly shaped charts both horizontally and vertically, using two approximations of chart shape that support efficient parallel processing. We balance packing outputs by automatically adjusting atlas row widths and orientations to accommodate varying chart heights. We show that our method significantly reduces chart downscaling compared to existing interactive methods while remaining orders of magnitude faster than offline alternatives.

</details>


### [12] [MPM Lite: Linear Kernels and Integration without Particles](https://arxiv.org/abs/2602.07853)
*Xiang Feng,Yunuo Chen,Chang Yu,Hao Su,Demetri Terzopoulos,Yin Yang,Joe Masterjohn,Alejandro Castro,Chenfanfu Jiang*

Main category: cs.GR

TL;DR: MPM Lite是一种新的混合拉格朗日/欧拉方法，通过将粒子状态重采样到固定位置的积分点，消除了求解时对粒子积分的需求，使求解器复杂度与粒子数量无关，显著提升了隐式求解性能。


<details>
  <summary>Details</summary>
Motivation: 传统MPM方法存在性能瓶颈：昂贵的隐式求解与粒子-单元比（PPC）成正比，这源于粒子积分和宽模板核函数的选择。需要一种能保持MPM鲁棒性和多功能性，同时显著提升性能的新方法。

Method: 将背景笛卡尔网格概念化为体素六面体网格，使用高效紧凑的线性核函数将粒子状态重采样到固定位置的积分点。提出新颖的应力传递和拉伸重构策略：重采样扩展的Kirchhoff应力并推导无旋转的变形参考解，支持基于优化的增量势能公式。方法实现为模块化重采样单元与FEM风格积分模块的耦合。

Result: MPM Lite在保持传统MPM对各种材料鲁棒性和多功能性的同时，在隐式设置中实现了显著加速，同时改进了显式设置。求解器复杂度不再与粒子相关，可直接使用现成的非线性求解器、预处理器和明确的边界条件。

Conclusion: MPM Lite通过架构转变消除了MPM的性能瓶颈，将粒子主要视为运动状态和材料历史的载体，使力组装和整个时间积分过程无需访问粒子，为MPM提供了高效、模块化的实现框架。

Abstract: In this paper, we introduce MPM Lite, a new hybrid Lagrangian/Eulerian method that eliminates the need for particle-based quadrature at solve time. Standard MPM practices suffer from a performance bottleneck where expensive implicit solves are proportional to particle-per-cell (PPC) counts due to the the choices of particle-based quadrature and wide-stencil kernels. In contrast, MPM Lite treats particles primarily as carriers of kinematic state and material history. By conceptualizing the background Cartesian grid as a voxel hexahedral mesh, we resample particle states onto fixed-location quadrature points using efficient, compact linear kernels. This architectural shift allows force assembly and the entire time-integration process to proceed without accessing particles, making the solver complexity no longer relate to particles. At the core of our method is a novel stress transfer and stretch reconstruction strategy. To avoid non-physical averaging of deformation gradients, we resample the extensive Kirchhoff stress and derive a rotation-free deformation reference solution, which naturally supports an optimization-based incremental potential formulation. Consequently, MPM Lite can be implemented as modular resampling units coupled with an FEM-style integration module, enabling the direct use of off-the-shelf nonlinear solvers, preconditioners, and unambiguous boundary conditions. We demonstrate through extensive experiments that MPM Lite preserves the robustness and versatility of traditional MPM across diverse materials while delivering significant speedups in implicit settings and improving explicit settings at the same time. Check our project page at https://mpmlite.github.io.

</details>


### [13] [Energy-Controllable Time Integration for Elastodynamic Contact](https://arxiv.org/abs/2602.08094)
*Kevin You,Juntian Zheng,Minchen Li*

Main category: cs.GR

TL;DR: 提出A-search时间积分器，通过简单修改隐式欧拉方法实现能量可控，在保持稳定性的同时允许用户指定能量目标，平衡能量耗散与守恒


<details>
  <summary>Details</summary>
Motivation: 传统数值积分器在弹性体动态模拟中存在局限性：隐式欧拉和BDF2方法在大时间步长下稳定但能量耗散不可控；辛方法如隐式中点能守恒能量但无条件稳定性不足，无法处理中等刚度问题

Method: 提出一类针对哈密顿问题的数值积分器，在线性问题上是辛的，在非线性问题上具有优越稳定性。基于此推导出A-search能量可控时间积分器，通过对隐式欧拉的简单修改实现用户指定能量目标，支持灵活控制能量耗散或守恒

Result: A-search在广泛材料参数和场景评估中表现出色：倾向于将能量保持在低频运动而非耗散；在相似总运行时间下优于传统方法如BDF2，能更好地维持能量并产生更理想的视觉效果；无缝集成障碍型能量，提供无反转和无穿透保证

Conclusion: A-search提供了一种新颖的能量可控时间积分方法，在保持隐式欧拉稳定性的同时实现了灵活的能量控制，特别适合处理大变形和复杂碰撞，在物理保真度和视觉质量方面优于传统方法

Abstract: Dynamic simulation of elastic bodies is a longstanding task in engineering and computer graphics. In graphics, numerical integrators like implicit Euler and BDF2 are preferred due to their stability at large time steps, but they tend to dissipate energy uncontrollably. In contrast, symplectic methods like implicit midpoint can conserve energy but are not unconditionally stable and fail on moderately stiff problems. To address these limitations, we propose a general class of numerical integrators for Hamiltonian problems which are symplectic on linear problems, yet have superior stability on nonlinear problems. With this, we derive a novel energy-controllable time integrator, A-search, a simple modification of implicit Euler that can follow user-specified energy targets, enabling flexible control over energy dissipation or conservation while maintaining stability and physical fidelity. Our method integrates seamlessly with barrier-type energies and allows for inversion-free and penetration-free guarantees, making it well-suited for handling large deformations and complex collisions. Extensive evaluations over a wide range of material parameters and scenes demonstrate that A-search has biases to keep energy in low frequency motion rather than dissipation, and A-search outperforms traditional methods such as BDF2 at similar total running times by maintaining energy and leading to more visually desirable simulations.

</details>


### [14] [Forget Superresolution, Sample Adaptively (when Path Tracing)](https://arxiv.org/abs/2602.08642)
*Martin Bálint,Corentin Salaün,Hans-Peter Seidel,Karol Myszkowski*

Main category: cs.GR

TL;DR: 提出一种针对亚1-spp（每像素少于1个样本）渲染的端到端自适应采样与去噪流水线，通过随机采样决策实现梯度估计，结合色调映射感知训练和感知损失，在极低采样预算下提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 实时路径追踪面临极低采样预算（低于1-spp）的挑战，现有超分辨率方法会均匀牺牲空间细节，而自适应采样方法在稀疏采样区域存在近似失效问题。需要专门针对亚1-spp区域设计自适应采样方案。

Method: 1. 使用随机采样决策公式实现梯度估计，稳定训练神经采样器；2. 提出色调映射感知训练流水线，集成可微分胶片操作符和感知损失；3. 引入基于聚集的金字塔去噪滤波器和可学习的反照率解调泛化方法。

Result: 相比均匀稀疏采样，该方法在感知关键细节（如镜面高光和阴影边界）的重建上表现更优，证明自适应采样在极低预算下仍然有效。

Conclusion: 该方法为亚1-spp渲染提供了一种有效的端到端自适应采样与去噪解决方案，通过感知优化和专门设计的去噪组件，在极低采样预算下实现了更好的视觉质量。

Abstract: Real-time path tracing increasingly operates under extremely low sampling budgets, often below one sample per pixel, as rendering complexity, resolution, and frame-rate requirements continue to rise. While super-resolution is widely used in production, it uniformly sacrifices spatial detail and cannot exploit variations in noise, reconstruction difficulty, and perceptual importance across the image. Adaptive sampling offers a compelling alternative, but existing end-to-end approaches rely on approximations that break down in sparse regimes.
  We introduce an end-to-end adaptive sampling and denoising pipeline explicitly designed for the sub-1-spp regime. Our method uses a stochastic formulation of sample placement that enables gradient estimation despite discrete sampling decisions, allowing stable training of a neural sampler at low sampling budgets. To better align optimization with human perception, we propose a tonemapping-aware training pipeline that integrates differentiable filmic operators and a state-of-the-art perceptual loss, preventing oversampling of regions with low visual impact.
  In addition, we introduce a gather-based pyramidal denoising filter and a learnable generalization of albedo demodulation tailored to sparse sampling. Our results show consistent improvements over uniform sparse sampling, with notably better reconstruction of perceptually critical details such as specular highlights and shadow boundaries, and demonstrate that adaptive sampling remains effective even at minimal budgets.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [15] [Information Theoretic Modeling of Interspecies Molecular Communication](https://arxiv.org/abs/2602.07474)
*Bitop Maitra,Murat Kuscu,Ozgur B. Akan*

Main category: cs.IT

TL;DR: 提出信息论框架建模植物与昆虫间的挥发性有机化合物分子通信，考虑环境噪声和受体随机响应


<details>
  <summary>Details</summary>
Motivation: 植物与昆虫通过挥发性有机化合物进行化学通信，这种通信发生在有噪声的环境中（风、距离、生物反应），需要理论框架来理解这种跨物种分子通信的基本原理

Method: 开发信息论框架，将受体响应建模为多项式分布的概率过程，考虑环境参数（风速、距离、释放分子数）对通信的影响

Result: 数值结果表明通信性能依赖于环境参数，如风速、距离和释放分子数量，框架为现实生物环境条件下的VOC通信提供了基础见解

Conclusion: 提出的信息论框架能够有效建模跨物种分子通信，揭示了环境因素对挥发性有机化合物通信系统性能的影响机制

Abstract: Plants and insects communicate using chemical signals like volatile organic compounds (VOCs). A plant encodes information using different blends of VOCs, which propagate through the air to represent different symbolic information. This communication occurs in a noisy environment, characterized by wind, distance, and complex biological reactions. At the receiver, cross-reactive olfactory receptors produce stochastic binding events whose discretized durations form the receiver observation. In this paper, an information-theoretic framework is developed to model interspecies molecular communication (MC), where receptor responses are modeled probabilistically using a multinomial distribution. Numerical results show that the communication depends on environmental parameters such as wind speed, distance, and the number of released molecules. The proposed framework provides fundamental insights into the VOC-based interspecies communication under realistic biological and environmental conditions.

</details>


### [16] [Expected Recovery Time in DNA-based Distributed Storage Systems](https://arxiv.org/abs/2602.07601)
*Adi Levy,Roni Con,Eitan Yaakobi,Han Mao Kiah*

Main category: cs.IT

TL;DR: 研究DNA分布式存储系统，分析在DNA存储容器故障时基于随机采样的数据恢复时间和性能


<details>
  <summary>Details</summary>
Motivation: DNA存储容器受测序技术限制，每次读取只能随机采样容器中的DNA链，需要研究在这种约束下的分布式存储系统可靠性

Method: 将DNA存储容器建模为M个容器，分析多种纠删码方案，通过推广经典优惠券收集问题来研究数据恢复时间

Result: 获得了DNA分布式存储系统中故障容器数据恢复的期望时间分析结果

Conclusion: DNA分布式存储系统需要专门设计以应对随机采样约束，推广的优惠券收集问题分析为此类系统提供了理论基础

Abstract: We initiate the study of DNA-based distributed storage systems, where information is encoded across multiple DNA data storage containers to achieve robustness against container failures. In this setting, data are distributed over $M$ containers, and the objective is to guarantee that the contents of any failed container can be reliably reconstructed from the surviving ones. Unlike classical distributed storage systems, DNA data storage containers are fundamentally constrained by sequencing technology, since each read operation yields the content of a uniformly random sampled strand from the container. Within this framework, we consider several erasure-correcting codes and analyze the expected recovery time of the data stored in a failed container. Our results are obtained by analyzing generalized versions of the classical Coupon Collector's Problem, which may be of independent interest.

</details>


### [17] [Data Compression with Stochastic Codes](https://arxiv.org/abs/2602.07635)
*Gergely Flamich,Deniz Gündüz*

Main category: cs.IT

TL;DR: 相对熵编码综述：机器学习在数据压缩中的新兴方向，重点介绍随机编码作为量化与熵编码的替代方案，涵盖理论基础、计算实践和应用前景。


<details>
  <summary>Details</summary>
Motivation: 机器学习在过去十年对数据压缩产生重大影响，激发了新的理论和应用问题。本文旨在填补相对熵编码领域在计算和实践方面的文献空白，为不同背景的读者提供全面概述。

Method: 采用综述性方法，系统梳理相对熵编码的理论框架和算法构造。重点分析随机编码作为有损源编码中量化和熵编码的替代方案，强调其计算可行性和实际实现。

Result: 提供了相对熵编码领域的全面概览，阐明了该领域的基本原理、算法结构和应用场景。展示了相对熵编码作为简单而优雅的构造方法，在数据压缩研究中的潜力和价值。

Conclusion: 相对熵编码是数据压缩研究中一个简单而令人兴奋的新兴领域，具有重要的理论和应用价值。本文为不同背景的研究者提供了理解该领域的直观框架，并指出了未来研究和应用的发展方向。

Abstract: Machine learning has had a major impact on data compression over the last decade and inspired many new, exciting theoretical and applied questions.
  This paper describes one such direction -- relative entropy coding -- which focuses on constructing stochastic codes, primarily as an alternative to quantisation and entropy coding in lossy source coding. Our primary aim is to provide a broad overview of the topic, with an emphasis on the computational and practical aspects currently missing from the literature.
  Our goal is threefold: for the curious reader, we aim to provide an intuitive picture of the field and convince them that relative entropy coding is a simple yet exciting emerging field in data compression research. For a reader interested in applied research on lossy data compression, we provide an account of the most salient contemporary applications. Finally, for the reader who has heard of relative entropy coding but has never been quite sure what it is or how the algorithms fit together, we hope to illustrate how simple and elegant the underlying constructions are.

</details>


### [18] [Capacity Scaling Laws for Boundary-Induced Drift-Diffusion Noise Channels](https://arxiv.org/abs/2602.07866)
*Yen-Chi Lee*

Main category: cs.IT

TL;DR: 该论文研究了多维漂移扩散过程在吸收超平面上首次击中位置产生的加性噪声信道的高功率容量缩放，提出了NDFHL噪声模型，证明了在高信噪比下各向同性高斯信令是渐近容量可达的，容量仅由接收边界维度决定，揭示了边界击中信道的几何和信息论特性。


<details>
  <summary>Details</summary>
Motivation: 研究边界诱导噪声信道的容量特性，这类噪声源于多维漂移扩散过程在吸收超平面上的首次击中位置，在物理和生物系统中具有重要应用。需要理解这种非高斯、半重尾噪声对信道容量的影响，以及几何因素如何决定信道自由度。

Method: 将底层随机传输机制识别为高斯方差混合，引入并分析正态漂移首次击中位置（NDFHL）家族作为几何驱动的边界诱导噪声模型。在二阶矩约束下，推导精确的高信噪比容量展开，证明渐近上下界在常数水平上重合，得到消失的容量间隙。

Result: 各向同性高斯信令对所有固定漂移强度都是渐近容量可达的，尽管噪声是非高斯和半重尾的。预对数因子仅由接收边界的维度决定，揭示了信道自由度的几何起源。精细展开进一步揭示了熵主导的普适性：传输过程的所有物理参数仅通过诱导噪声的微分熵影响容量。

Conclusion: NDFHL模型为边界击中信道提供了统一的几何和信息论表征，跨越了规则和奇异传输机制。噪声熵有限且在漂移消失时连续变化，连接了有限方差机制与奇异无限方差柯西极限。这些结果深化了对几何诱导噪声信道基本极限的理解。

Abstract: This paper studies the high-power capacity scaling of additive noise channels whose noise arises from the first-hitting location of a multidimensional drift-diffusion process on an absorbing hyperplane. By identifying the underlying stochastic transport mechanism as a Gaussian variance-mixture, we introduce and analyze the Normally-Drifted First-Hitting Location (NDFHL) family as a geometry-driven model for boundary-induced noise. Under a second-moment constraint, we derive an exact high-SNR capacity expansion and show that the asymptotic upper and lower bounds coincide at the constant level, yielding a vanishing capacity gap. As a consequence, isotropic Gaussian signaling is asymptotically capacity-achieving for all fixed drift strengths, despite the non-Gaussian and semi-heavy-tailed nature of the noise. The pre-log factor is determined solely by the dimension of the receiving boundary, revealing a geometric origin of the channel's degrees of freedom. The refined expansion further uncovers an entropy-dominant universality, whereby all physical parameters of the transport process -- including drift strength, diffusion coefficient, and boundary separation -- affect the capacity only through the differential entropy of the induced noise. Although the NDFHL density does not admit a simple closed form, its entropy is shown to be finite and to vary continuously as the drift vanishes, thereby connecting the finite-variance regime with the singular infinite-variance Cauchy limit. Together, these results provide a unified geometric and information-theoretic characterization of boundary-hitting channels across both regular and singular transport regimes.

</details>


### [19] [OFDM Enabled Over-the-Air Computation Systems with Two-Dimensional Fluid Antennas](https://arxiv.org/abs/2602.07953)
*Heyang Xiong,Quanzhong Li,Qi Zhang*

Main category: cs.IT

TL;DR: 提出一种基于二维流体天线系统的OFDM空中计算方案，通过优化天线位置和收发处理来降低计算均方误差


<details>
  <summary>Details</summary>
Motivation: 利用流体天线系统在频率选择性信道中开发空间自由度，提升无线计算系统的性能

Method: 将MSE最小化问题分解为发射预编码器优化和天线位置与接收合并器优化，后者采用主化-最小化与顺序优化相结合的方法

Result: 数值结果表明，所提方案相比固定位置天线方案实现了MSE降低

Conclusion: 二维流体天线系统结合优化算法能有效提升OFDM空中计算系统的性能

Abstract: Fluid antenna system (FAS) is able to exploit spatial degrees of freedom (DoFs) in wireless channels. In this letter, to exploit spatial DoFs in frequency-selective environments, we investigate an orthogonal frequency division multiplexing enabled over-the-air computation system, where the access point is equipped with a two-dimensional FAS to enhance performance. We solve the computation mean square error (MSE) minimization problem by transforming the original problem into transmit precoders optimization problem and antenna positions optimization along with receive combiners optimization problem. The latter is solved via a majorization-minimization approach combined with sequential optimization. Numerical results confirm that the proposed scheme achieves MSE reduction over the scheme with fixed position antennas.

</details>


### [20] [Tighter Information-Theoretic Generalization Bounds via a Novel Class of Change of Measure Inequalities](https://arxiv.org/abs/2602.07999)
*Yanxiao Liu,Yijun Fan an Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出基于f-散度数据处理不等式的统一框架，推导出更紧的测度变换不等式，并将其应用于随机学习算法的泛化误差分析，得到新的信息论泛化界


<details>
  <summary>Details</summary>
Motivation: 现有信息论泛化界存在紧致性不足的问题，需要建立更统一的框架来推导更紧的测度变换不等式，以改进随机学习算法的泛化误差分析

Method: 基于f-散度的数据处理不等式建立统一框架，推导出涵盖f-散度、Rényi散度和α-互信息等广泛信息度量的测度变换不等式，并将其嵌入到随机学习算法的泛化误差分析中

Result: 获得了更紧的高概率信息论泛化界，同时通过简化分析恢复了多个已知最佳结果；框架具有灵活性，可适应条件互信息框架、PAC-Bayesian理论和差分隐私机制等多种设置

Conclusion: 提出的基于f-散度数据处理不等式的统一框架能够产生更紧的测度变换不等式，为随机学习算法的泛化分析提供了强大的工具，并在多种设置下获得了新的泛化界

Abstract: In this paper, we propose a novel class of change of measure inequalities via a unified framework based on the data processing inequality for $f$-divergences, which is surprisingly elementary yet powerful enough to yield tighter inequalities. We provide change of measure inequalities in terms of a broad family of information measures, including $f$-divergences (with Kullback-Leibler divergence and $χ^2$-divergence as special cases), Rényi divergence, and $α$-mutual information (with maximal leakage as a special case). We then embed these inequalities into the analysis of generalization error for stochastic learning algorithms, yielding novel and tighter high-probability information-theoretic generalization bounds, while also recovering several best-known results via simplified analyses. A key advantage of our framework is its flexibility: it readily adapts to a range of settings, including the conditional mutual information framework, PAC-Bayesian theory, and differential privacy mechanisms, for which we derive new generalization bounds.

</details>


### [21] [Term Coding and Dispersion: A Perfect-vs-Rate Complexity Dichotomy for Information Flow](https://arxiv.org/abs/2602.08110)
*Søren Riis*

Main category: cs.IT

TL;DR: 提出"项编码"框架用于离散数学和信息流中的极值问题，通过选择函数符号的解释来最大化满足项方程系统的赋值数量。聚焦于分散化问题，证明了最大分散度为Θ(n^D)，其中D为关联有向图的猜测数，并给出了多项式时间算法计算D。完美分散化的可判定性在r≥3时不可判定。


<details>
  <summary>Details</summary>
Motivation: 研究离散数学和信息流中的极值问题，特别是如何通过选择函数符号的解释来最大化满足项方程系统的赋值数量。分散化作为特例，关注项映射图像的大小，这在信息论、编码理论和组合优化中具有重要意义。

Method: 引入"项编码"新框架，将问题形式化为选择函数符号解释以最大化满足项方程系统的赋值数量。对于分散化问题，将系统视为项映射Θ^I: A^k→A^r，研究其图像大小。通过关联有向图的猜测数D来表征最大分散度Θ(n^D)，并设计多项式时间算法计算D。

Result: 证明了最大分散度为Θ(n^D)，其中D为关联有向图的猜测数。给出了计算D的多项式时间算法。发现完美分散化（Disp_n(t)=n^r）在r≥3时不可判定，尽管相应的渐近率阈值问题可在多项式时间内判定。

Conclusion: 项编码框架为离散数学和信息流中的极值问题提供了新视角。分散化问题的最大分散度可由关联图的猜测数精确刻画，且存在高效算法。然而，完美分散化的存在性在较高维度下不可判定，揭示了该问题的计算复杂性。

Abstract: We introduce a new framework term coding for extremal problems in discrete mathematics and information flow, where one chooses interpretations of function symbols so as to maximise the number of satisfying assignments of a finite system of term equations.
  We then focus on dispersion, the special case in which the system defines a term map $Θ^\mathcal I:\A^k\to\A^r$ and the objective is the size of its image. Writing $n:=|\A|$, we show that the maximum dispersion is $Θ(n^D)$ for an integer exponent $D$ equal to the guessing number of an associated directed graph, and we give a polynomial-time algorithm to compute $D$. In contrast, deciding whether \emph{perfect dispersion} ever occurs (i.e.\ whether $\Disp_n(\mathbf t)=n^r$ for some finite $n\ge 2$) is undecidable once $r\ge 3$, even though the corresponding asymptotic rate-threshold questions are polynomial-time decidable.

</details>


### [22] [Optimal Transmit Beamforming for MIMO ISAC with Unknown Target and User Locations](https://arxiv.org/abs/2602.08255)
*Yizhuo Wang,Shuowen Zhang*

Main category: cs.IT

TL;DR: 该论文研究了MIMO ISAC系统中目标和用户位置均未知且随机时的波束成形设计问题，基于概率分布信息建立性能度量，推导最优传输协方差矩阵，并证明静态波束成形策略已足够达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 在MIMO ISAC系统中，当感知目标和通信用户的位置都未知且随机时，如何仅基于概率分布信息设计发射波束成形以充分利用空间资源，使感知和通信在统计意义上都能达到满意性能，这是一个具有挑战性的问题。同时，研究目标和用户位置分布相似性对ISAC性能的影响也具有重要意义。

Method: 基于概率分布信息建立通信和感知性能度量：通过推导期望速率或后验克拉美罗下界(PCRB)。然后，在期望速率约束下最小化PCRB的发射波束成形优化问题，推导出最优解。分析最优传输协方差矩阵的秩上界，并研究动态波束成形策略的必要性。

Result: 推导出最优发射协方差矩阵，其秩上界为所有可能用户位置的MIMO通信信道矩阵之和。证明使用静态波束成形策略足以达到最优性能，无需动态调整。数值结果表明：当目标/用户位置分布越相似时，ISAC性能越好，并为BS-用户/目标关联策略提供了有用见解。

Conclusion: 在目标和用户位置均随机未知的MIMO ISAC系统中，仅基于概率分布信息即可设计有效的波束成形策略。静态波束成形已足够优化，且目标和用户位置分布的相似性对ISAC性能有积极影响，这为实际系统设计提供了重要指导。

Abstract: This paper studies a challenging scenario in a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system where the locations of the sensing target and the communication user are both unknown and random, while only their probability distribution information is known. In this case, how to fully utilize the spatial resources by designing the transmit beamforming such that both sensing and communication can achieve satisfactory performance statistically is a difficult problem, which motivates the study in this paper. Moreover, we aim to reveal if it is desirable to have similar probability distributions for the target and user locations in terms of the ISAC performance. Firstly, based on only probability distribution information, we establish communication and sensing performance metrics via deriving the expected rate or posterior Cramér-Rao bound (PCRB). Then, we formulate the transmit beamforming optimization problem to minimize the PCRB subject to the expected rate constraint, for which the optimal solution is derived. It is unveiled that the rank of the optimal transmit covariance matrix is upper bounded by the summation of MIMO communication channel matrices for all possible user locations. Furthermore, due to the need to cater to multiple target/user locations, we investigate whether dynamically employing different beamforming designs over different time slots improves the performance. It is proven that using a static beamforming strategy is sufficient for achieving the optimal performance. Numerical results validate our analysis, show that ISAC performance improves as the target/user location distributions become similar, and provide useful insights on the BS-user/-target association strategy.

</details>


### [23] [Hierarchical Subcode Ensemble Decoding of Polar Codes](https://arxiv.org/abs/2602.08391)
*Yubeen Jo,Geon Choi,Chanho Park,Namyoon Lee*

Main category: cs.IT

TL;DR: 提出分层子码集成解码（HSCED）框架，通过分层递归生成子码奇偶校验约束，在保证线性覆盖性的同时扩展集成规模，显著提升极化码的BP解码性能。


<details>
  <summary>Details</summary>
Motivation: 现有子码集成解码方法缺乏在保持线性覆盖特性的同时系统性地扩展集成规模的方法，而极化码的密集奇偶校验矩阵导致严重的停止集效应，限制了传统BP解码的性能。

Method: 提出分层子码集成解码（HSCED）框架，通过递归生成子码奇偶校验约束构建分层结构，确保每一级都保持线性覆盖特性，从而在控制复杂度的前提下实现大规模集成解码。

Result: 仿真实验表明，在相同解码延迟约束下，HSCED相比标准BP解码和传统子码集成解码方法，在块错误率方面取得了显著改进。

Conclusion: HSCED提供了一种系统性的方法来扩展子码集成解码的规模，同时保证线性覆盖特性，为极化码等受停止集效应限制的码型提供了有效的解码增强方案。

Abstract: Subcode-ensemble decoders improve iterative decoding by running multiple decoders in parallel over carefully chosen subcodes, increasing the likelihood that at least one decoder avoids the dominant trapping structures. Achieving strong diversity gains, however, requires constructing many subcodes that satisfy a linear covering property-yet existing approaches lack a systematic way to scale the ensemble size while preserving this property. This paper introduces hierarchical subcode ensemble decoding (HSCED), a new ensemble decoding framework that expands the number of constituent decoders while still guaranteeing linear covering. The key idea is to recursively generate subcode parity constraints in a hierarchical structure so that coverage is maintained at every level, enabling large ensembles with controlled complexity. To demonstrate its effectiveness, we apply HSCED to belief propagation (BP) decoding of polar codes, where dense parity-check matrices induce severe stopping-set effects that limit conventional BP. Simulations confirm that HSCED delivers significant block-error-rate improvements over standard BP and conventional subcode-ensemble decoding under the same decoding-latency constraint.

</details>


### [24] [Multipoint Code-Weight Sphere Decoding: Parallel Near-ML Decoding for Short-Blocklength Codes](https://arxiv.org/abs/2602.08501)
*Yubeen Jo,Geon Choi,Yongjune Kim,Namyoon Lee*

Main category: cs.IT

TL;DR: 提出一种适用于任何线性分组码的两阶段近最大似然解码框架，第一阶段使用低复杂度解码器，失败时激活第二阶段的多点码重球面解码器，通过预计算低重量码字进行局部搜索，在保持低延迟的同时实现近ML性能。


<details>
  <summary>Details</summary>
Motivation: URLLC使用短数据包，有限块长效应使得近ML解码变得重要但计算成本高昂。需要一种既能实现近ML性能又能保持低复杂度和低延迟的解码方案。

Method: 提出两阶段解码框架：第一阶段使用低复杂度解码器生成候选码字和CRC校验；失败时激活第二阶段MP-WSD解码器。MP-WSD预计算一组低重量码字，从第一阶段输出开始，通过添加这些低重量码字在欧几里得球面内迭代搜索候选码字，随着找到更好候选而缩小搜索区域。

Result: 仿真结果表明，所提出的解码器在短块长、低速率码上实现了近ML性能，同时保持了低解码延迟。在高信噪比下，第一阶段成功概率高，第二阶段很少激活；即使激活，搜索也保持局部化。

Conclusion: 该两阶段解码框架为URLLC应用提供了一种有效的解决方案，能够在保持低复杂度和低延迟的同时实现近ML解码性能，特别适用于短块长、低速率码。

Abstract: Ultra-reliable low-latency communications (URLLC) operate with short packets, where finite-blocklength effects make near-maximum-likelihood (near-ML) decoding desirable but often too costly. This paper proposes a two-stage near-ML decoding framework that applies to any linear block code. In the first stage, we run a low-complexity decoder to produce a candidate codeword and a cyclic redundancy check. When this stage succeeds, we terminate immediately. When it fails, we invoke a second-stage decoder, termed multipoint code-weight sphere decoding (MP-WSD). The central idea behind {MP-WSD} is to concentrate the ML search where it matters. We pre-compute a set of low-weight codewords and use them to generate structured local perturbations of the current estimate. Starting from the first-stage output, MP-WSD iteratively explores a small Euclidean sphere of candidate codewords formed by adding selected low-weight codewords, tightening the search region as better candidates are found. This design keeps the average complexity low: at high signal-to-noise ratio, the first stage succeeds with high probability and the second stage is rarely activated; when it is activated, the search remains localized. Simulation results show that the proposed decoder attains near-ML performance for short-blocklength, low-rate codes while maintaining low decoding latency.

</details>


### [25] [Reliable one-bit quantization of bandlimited graph data via single-shot noise shaping](https://arxiv.org/abs/2602.08669)
*Johannes Maly,Anna Veselovska*

Main category: cs.IT

TL;DR: 提出一种用于图结构带限数据的高效单次噪声整形量化方法，可在任意比特级别（包括单比特）实现可靠量化


<details>
  <summary>Details</summary>
Motivation: 图数据在自然科学和机器学习中普遍存在，需要一种能够在低通滤波下保持信息的同时，以每项少量比特对图结构带限数据进行量化的方法

Method: 提出一种高效的单次噪声整形方法，该方法能够实现最先进的性能，并具有严格的误差界限

Result: 该方法在任意比特级别（包括极端情况下的单比特量化）都能实现可靠量化，性能达到最先进水平

Conclusion: 所提出的噪声整形方法为图结构带限数据提供了一种高效可靠的量化方案，特别适用于低比特率场景

Abstract: Graph data are ubiquitous in natural sciences and machine learning. In this paper, we consider the problem of quantizing graph structured, bandlimited data to few bits per entry while preserving its information under low-pass filtering. We propose an efficient single-shot noise shaping method that achieves state-of-the-art performance and comes with rigorous error bounds. In contrast to existing methods it allows reliable quantization to arbitrary bit-levels including the extreme case of using a single bit per data coefficient.

</details>


### [26] [Trellis codes with a good distance profile constructed from expander graphs](https://arxiv.org/abs/2602.08718)
*Yubin Zhu,Zitan Chen*

Main category: cs.IT

TL;DR: 该论文推导了网格码的自由距离和列距离的Singleton型界，证明了网格码的最大列距离可以超过卷积码，并利用扩展图构造了接近最大距离轮廓卷积码性能的网格码。


<details>
  <summary>Details</summary>
Motivation: 研究网格码的距离特性，探索其在给定时间实例下是否能够超越卷积码的列距离性能，并寻求在恒定字母表大小下实现接近最大距离轮廓卷积码性能的构造方法。

Method: 1. 推导网格码的自由距离和列距离的Singleton型界；2. 理论分析网格码与卷积码的列距离比较；3. 利用扩展图构造恒定字母表大小的网格码，实现接近最大距离轮廓卷积码的速率-距离权衡。

Result: 1. 证明了在给定时间实例下，网格码的最大可达列距离可以超过卷积码；2. 构造了在恒定大小字母表上的网格码，其速率-距离权衡可以任意接近具有最大距离轮廓的卷积码；3. 与已知卷积码构造相比，新方法避免了字母表大小随输出符号数指数增长的问题。

Conclusion: 网格码在距离性能方面具有超越卷积码的潜力，通过扩展图构造的网格码能够在恒定字母表大小下实现接近最优的速率-距离权衡，这为实际编码系统设计提供了更优的选择。

Abstract: We derive Singleton-type bounds on the free distance and column distances of trellis codes. Our results show that, at a given time instant, the maximum attainable column distance of trellis codes can exceed that of convolutional codes. Moreover, using expander graphs, we construct trellis codes over constant-size alphabets that achieve a rate-distance trade-off arbitrarily close to that of convolutional codes with a maximum distance profile. By comparison, all known constructions of convolutional codes with a maximum distance profile require working over alphabets whose size grows at least exponentially with the number of output symbols per time instant.

</details>


### [27] [Clique-Based Deletion-Correcting Codes via Penalty-Guided Clique Search](https://arxiv.org/abs/2602.08952)
*Aniruddh Pandav,Rajshekhar V Bhat*

Main category: cs.IT

TL;DR: 该论文提出了一种基于最大团问题的d-删除纠错二进制码构造方法，使用惩罚引导团搜索启发式算法获得比现有方法更大的码本，并设计了优化的LCS解码器降低解码复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究d-删除纠错二进制码的构造问题，目标是找到比现有方法（如Helberg码）更大的码本，并降低解码复杂度。

Method: 1. 将码本构造问题建模为最大团问题：顶点代表候选码字，边连接那些最长公共子序列距离能保证纠正最多d个删除的码字对。2. 使用惩罚引导团搜索（PGCS）启发式算法寻找最大团。3. 针对分段接收场景（码字边界已知），提出优化的LCS解码器，利用符号计数过滤和提前终止减少LCS计算量。

Result: 1. 对于块长度n=8-14和删除参数d=1-3，PGCS算法生成的码本比现有的图启发式方法（最小度和着色方法）更大。2. 在某些有限长度情况下，得到的码本达到已知最优大小，并优于经典Helberg码构造。3. 优化的LCS解码器显著降低了平均情况解码复杂度，相比基线O(|C| n²)方法有实质性改进。

Conclusion: 惩罚引导团搜索是构造d-删除纠错二进制码的有效启发式方法，能产生比现有图启发式方法更大的码本；同时，优化的LCS解码器在保持精确解码保证的前提下，显著降低了分段接收场景下的解码复杂度。

Abstract: We study the construction of $d$-deletion-correcting binary codes by formulating the problem as a Maximum Clique Problem (MCP). In this formulation, vertices represent candidate codewords and edges connect pairs whose longest common subsequence (LCS) distance guarantees correction of up to $d$ deletions. A valid codebook corresponds to a clique in the resulting graph, and finding the largest codebook is equivalent to identifying a maximum clique. While MCP-based formulations for deletion-correcting codes have previously been explored, we demonstrate that applying Penalty-Guided Clique Search (PGCS), a lightweight stochastic clique-search heuristic inspired by Dynamic Local Search (DLS), consistently yields larger codebooks than existing graph-based heuristics, including minimum-degree and coloring methods, for block lengths $n = 8,9,\dots,14$ and deletion parameters $d = 1,2,3$. In several finite-length regimes, the resulting codebooks match known optimal sizes and outperform classical constructions such as Helberg codes. For decoding under segmented reception, where codeword boundaries are known, we propose an optimized LCS-based decoder that exploits symbol-count filtering and early termination to substantially reduce the number of LCS evaluations while preserving exact decoding guarantees. These optimizations lead to significantly lower average-case decoding complexity than the baseline $O(|C| n^2)$ approach.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL框架利用符号微分进行可验证的数学问题分解，通过严格验证的子问题训练语言模型，相比启发式分解方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法多为启发式，无法保证子问题更简单、解决子问题有助于父任务、或分解关系具有数学基础。需要一种可验证的分解框架来确保训练的有效性。

Method: 提出Verify-RL框架，利用符号微分提供自然分解结构：微积分规则明确定义表达式如何分解为更简单的组件，并具有可证明的性质。每个父-子分解满足三个可验证条件：结构复杂度严格递减、解包含性、形式规则推导。

Result: 消除无效分解带来显著收益，最难题目的准确率从32%提升至68%（超过两倍增长），整体相对改进达40%。通过符号计算实现"构造即验证"，确保所有分解都有效。

Conclusion: 符号微分为数学问题分解提供了可验证的结构基础，Verify-RL框架通过确保分解的数学正确性显著提升了语言模型解决复杂数学问题的能力，证明了可验证分解的重要性。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>
