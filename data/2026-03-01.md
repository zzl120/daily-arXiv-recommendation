<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.IR](#cs.IR) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: Search-P1框架通过路径中心奖励塑造改进Agentic RAG训练，解决传统RL方法中的稀疏奖励和低样本效率问题


<details>
  <summary>Details</summary>
Motivation: 传统单轮检索在处理复杂多步推理时存在局限，而现有基于RL的Agentic RAG训练方法面临稀疏结果奖励（丢弃中间信号）和低样本效率（失败样本无贡献）的问题

Method: 提出Search-P1框架，包含两个关键组件：1) 路径中心奖励，通过顺序无关的步骤覆盖度和软评分评估推理轨迹的结构质量，即使从失败样本中也能提取学习信号；2) 双轨路径评分，使用离线生成的参考规划器从自一致性和参考对齐两个角度评估路径

Result: 在多个QA基准测试中，Search-P1相比Search-R1和其他强基线实现了显著改进，平均准确率提升7.7个百分点

Conclusion: Search-P1通过路径中心奖励塑造有效解决了Agentic RAG训练中的奖励稀疏性和样本效率问题，显著提升了多步推理任务的性能

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [2] [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584)
*Wenwei Li,Ming Xu,Tianle Xia,Lingxiang Hu,Yiding Sun,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 提出强化协同适应框架，通过图感知检索和证据约束强化学习联合优化检索与生成，显著降低广告问答中的幻觉率


<details>
  <summary>Details</summary>
Motivation: 工业广告问答中幻觉内容（特别是伪造URL）可能导致经济损失、合规违规和法律风险。现有检索增强生成（RAG）在工业知识场景下面临挑战：知识具有关系性、频繁更新且与生成目标对齐不足

Method: 提出强化协同适应框架，包含两个组件：1) 图感知检索（GraphRAG），在高引用知识子图上建模实体关系结构，实现多跳、领域特定的证据选择；2) 通过组相对策略优化（GRPO）进行证据约束强化学习，使用覆盖忠实性、风格合规性、安全性和URL有效性的多维奖励

Result: 在内部广告QA数据集上，在专家评判的准确性、完整性和安全性等维度上获得一致提升，幻觉率降低72%。两周在线A/B测试显示：点赞率增加28.6%，差评率降低46.2%，URL幻觉减少92.7%。系统已在生产环境运行半年多，服务数百万次QA交互

Conclusion: 提出的强化协同适应框架有效解决了工业广告问答中的幻觉问题，通过联合优化检索和生成显著提升了回答质量、安全性和可靠性，已在生产环境中验证其实际价值

Abstract: Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\%. A two-week online A/B test demonstrates a 28.6\% increase in like rate, a 46.2\% decrease in dislike rate, and a 92.7\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.

</details>


### [3] [TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought](https://arxiv.org/abs/2602.22828)
*Jianmin Li,Ying Chang,Su-Kit Tang,Yujia Liu,Yanwen Wang,Shuyuan Lin,Binkai Ou*

Main category: cs.CL

TL;DR: TCM-DiffRAG：针对中医领域改进的RAG框架，结合知识图谱和思维链，显著提升LLM在中医诊断任务中的性能


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在中医临床诊断和治疗领域表现不佳，因为中医涉及复杂的推理过程和显著的个体差异。需要开发专门针对中医推理特点的改进RAG框架

Method: 开发TCM-DiffRAG框架，将知识图谱（KG）与思维链（CoT）相结合，在三个不同的中医测试数据集上进行评估

Result: TCM-DiffRAG显著提升了原生LLM的性能（如qwen-plus模型从0.927/0.361/0.038提升到0.952/0.788/0.356），对非中文LLM提升更明显，且优于直接监督微调和其他基准RAG方法

Conclusion: 将结构化的中医知识图谱与基于思维链的推理相结合，能显著提升个性化诊断任务的性能。通用和个性化知识图谱的联合使用实现了通用知识与临床推理的有效对齐，展示了推理感知RAG框架在中医LLM应用中的潜力

Abstract: Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.

</details>


### [4] [MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations](https://arxiv.org/abs/2602.23184)
*Sara Rosenthal,Yannis Katsis,Vraj Shah,Lihong He,Lucian Popa,Marina Danilevsky*

Main category: cs.CL

TL;DR: MTRAG-UN是一个用于探索多轮检索增强生成中开放挑战的基准测试，包含666个任务、2800多个对话轮次，覆盖6个领域，重点关注UNanswerable、UNderspecified、NONstandalone和UNclear等挑战性问题


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成模型在多轮对话中面临诸多挑战，特别是在处理不可回答、未充分指定、非独立以及响应不清晰等问题时表现不佳，需要专门的基准来评估和改进这些能力

Method: 构建了包含666个任务的多轮对话基准，涵盖2800多个对话轮次和6个不同领域，并提供了相应的语料库，用于系统评估检索和生成模型在复杂对话场景中的表现

Result: 实验表明，现有的检索和生成模型在处理UNanswerable（不可回答）、UNderspecified（未充分指定）、NONstandalone（非独立）和UNclear（不清晰）等类型的对话问题时仍然存在显著困难

Conclusion: MTRAG-UN基准揭示了多轮检索增强生成中的关键挑战，为未来研究提供了重要的评估工具，有助于推动该领域在复杂对话场景中的技术进步

Abstract: We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统通过结合作者知识图谱和检索增强生成，为LLMs提供可控的学术背景和可追溯的灵感路径，显著提升科学创意生成的质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学创意生成方面存在局限性：生成的创意缺乏可控的学术背景和可追溯的灵感路径，难以满足学术研究的严谨性要求。

Method: 1) 提出以作者为中心的知识图谱构建方法和灵感源采样算法构建外部知识库；2) 设计结合RAG和GraphRAG的混合检索机制获取深度和广度知识；3) 提出基于强化学习原理的Prompt优化策略自动指导LLMs优化结果。

Result: 在基于arXiv(2018-2023)构建的数据集上，GYWI在GPT-4o、DeepSeek-V3、Qwen3-8B、Gemini 2.5等多个LLMs上显著优于主流方法，在新颖性、可靠性、相关性等多个指标上表现优异。

Conclusion: GYWI系统通过整合知识图谱和检索增强生成，有效解决了LLMs在科学创意生成中的背景控制和灵感追溯问题，为AI辅助科学研究提供了新思路。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [6] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard：基于检索增强的多智能体框架，将安全评估重构为证据辩论，实现零样本适应性，无需微调即可达到SOTA安全性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制依赖静态微调分类器，存在适应性僵化问题，无法在不昂贵重训练的情况下强制执行新的治理规则

Method: 引入CourtGuard框架，通过检索增强的多智能体系统，将安全评估重构为基于外部政策文档的证据辩论，实现对抗性辩论

Result: 在7个安全基准测试中达到SOTA性能，超越专用政策遵循基线；零样本适应性在维基百科破坏检测任务中达到90%准确率；自动数据策展和审计生成了9个新颖的对抗攻击数据集

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了一条鲁棒、可解释且适应性强的路径，能够满足当前和未来的监管要求

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [7] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文发现示例引导在数学推理中的效果不稳定源于策略使用与策略可执行性之间的差距，并提出选择性策略检索框架来提升推理性能


<details>
  <summary>Details</summary>
Motivation: 基于示例的引导在推理时广泛用于改进数学推理，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用与策略可执行性之间未被充分探索的差距。

Method: 通过分析配对的人工编写和模型生成的解决方案，识别策略使用与可执行性的系统性差异。基于此提出选择性策略检索框架，通过经验性、多路径、源感知信号来选择性检索和组合策略，显式建模可执行性。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导实现了可靠且一致的改进，在AIME25上准确率提升高达+13点，在Apex上提升+5点，特别对紧凑推理模型效果显著。

Conclusion: 策略使用与可执行性之间的差距是示例引导效果不稳定的关键原因，选择性策略检索框架通过显式建模可执行性，能够有效提升数学推理性能，为推理时引导提供了更可靠的方法。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [8] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于多渠道在线广告自动出价的分层框架，结合生成式规划和实时控制，相比现有基线提升13.57%的整体回报。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂动态，多渠道场景下预算和约束分配对投资回报优化至关重要。现有优化方法缺乏动态适应性，强化学习方法难以捕捉历史依赖和观测模式。

Method: 提出AHBid框架：高层使用基于扩散模型的生成式规划器动态分配预算和约束，捕捉历史上下文和时间模式；引入约束执行机制确保合规，轨迹精炼机制利用历史数据增强适应性；结合基于控制的出价算法，协同历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中验证有效性，相比现有基线实现13.57%的整体回报提升。

Conclusion: AHBid通过集成生成式规划和实时控制，有效解决了多渠道自动出价中的动态适应性和历史依赖捕捉问题，显著提升了广告投资回报。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [9] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决数据稀疏性和冷启动问题，避免负迁移。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能依赖于训练数据的质量、数量和相关性。现有模型中心范式依赖复杂的定制架构，难以捕捉跨域的微妙非结构序列依赖，导致泛化能力差和计算资源需求高。跨域数据存在固有领域差距，可能降低混合数据质量，引发负迁移和模型性能下降。

Method: 提出Taesar（目标对齐序列再生）框架，采用数据中心的对比解码机制，自适应地将跨域上下文编码到目标域序列中。该方法使标准模型能够学习复杂依赖关系，无需复杂的融合架构，有效结合数据和模型中心范式的优势。

Result: 实验表明Taesar优于模型中心解决方案，并能泛化到各种序列模型。通过生成丰富的数据集，有效结合了数据和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生方法，解决了跨域推荐中的数据稀疏性和冷启动问题，避免了负迁移，同时降低了模型复杂度和计算资源需求，为跨域推荐提供了有效的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [10] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench：评估LLM智能体长时记忆的新基准，包含真实和合成轨迹数据，揭示现有记忆系统不足并提出改进方案


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆评估主要关注对话式人机交互，而实际应用中智能体记忆是连续的机器生成环境交互流，需要更贴近真实应用的评估标准

Method: 提出AMA-Bench基准，包含真实世界智能体轨迹和可扩展的合成轨迹，配套专家标注和规则生成的QA；并设计AMA-Agent记忆系统，采用因果图结构和工具增强检索

Result: 现有记忆系统在AMA-Bench上表现不佳，主要缺乏因果性和目标信息，受限于基于相似性的检索损失；AMA-Agent达到57.22%平均准确率，比最强基线提升11.16%

Conclusion: AMA-Bench填补了智能体记忆评估的空白，揭示了现有系统的局限性，提出的因果图和工具增强检索方法显著提升了长时记忆性能

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [11] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS：用于质谱预测的灵活基准框架，支持动态构建多种模型架构并进行评估，提供性能影响因素分析和实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 质谱技术在药物发现和材料科学中至关重要，但实验光谱数据缺乏阻碍了分子鉴定，需要计算模型进行预测。深度学习模型在预测分子结构光谱方面有潜力，但方法异质性和缺乏明确定义的基准使得整体评估具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种不同的模型架构组合，在预处理的公共数据集上使用不同指标评估性能。分析影响性能的因素，包括数据集结构多样性、学习率和数据稀疏性等超参数、预训练效果、元数据消融设置和跨域迁移学习分析。

Result: FlexMS框架提供了实用的模型选择指导，通过检索基准模拟实际鉴定场景，基于预测光谱对潜在匹配进行评分。该框架能够评估不同模型架构在质谱预测任务中的表现。

Conclusion: FlexMS基准框架解决了质谱预测领域缺乏标准化评估的问题，为研究人员提供了灵活的工具来构建和评估不同模型架构，并通过深入分析性能影响因素为实际应用提供指导。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [12] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电图解码方法，显著提升性能、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的EEG分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，这限制了脑活动解码的准确性和信息提取能力。

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真，增强几何一致性。

Result: 大量实验表明，该框架显著优于现有的EEG表示方法，展现出卓越的鲁棒性和泛化能力。

Conclusion: RepSPD通过结合几何深度学习和功能连接特征，有效解决了当前SPD方法在频率同步和局部拓扑结构方面的不足，为脑电图解码提供了更强大的工具。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [13] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个测试时修正或拒绝的剪枝框架，用于动态优化多智能体系统的信息流，无需重新训练，通过检索增强的修正器和故障驱动指示器池来纠正错误，不可修复的输出被剪枝以防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息的级联影响问题。当前解决方案通常采用刚性结构工程或昂贵的微调，限制了系统的可部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，使用检索增强的修正器基于故障驱动指示器池迭代纠正错误。该机制利用蒸馏的故障模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能够根据任务难度动态调整修正力度，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时框架，无需重新训练即可动态优化多智能体系统的信息流，通过修正或拒绝机制减少错误传播，显著提升系统性能并保持适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [14] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN：基于神经ODE的脑电动态预测框架，通过谱图节点整合时空频特征，克服传统方法累积误差问题


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通过循环架构离散化时间建模连续脑动态，导致累积预测误差且无法捕捉EEG的瞬时非线性特征

Method: 提出ODEBRAIN框架：1) 将时空频特征整合到谱图节点中；2) 使用神经ODE建模连续潜动态；3) 确保潜表示能捕捉任意时间点的复杂脑状态随机变化

Result: 大量实验验证ODEBRAIN在EEG动态预测上显著优于现有方法，具有增强的鲁棒性和泛化能力

Conclusion: ODEBRAIN通过神经ODE框架成功解决了传统方法在连续脑动态建模中的局限性，为神经科学研究和临床应用提供了更有效的工具

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [15] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: Taxoria是一个利用大语言模型增强现有分类法的管道，通过提示LLM生成候选节点并验证后集成，解决分类法覆盖不足和节点过时/模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类法存在覆盖范围有限、节点过时或模糊的问题，降低了知识检索的有效性，需要一种方法来增强和更新分类法。

Method: Taxoria使用现有分类法作为种子，提示大语言模型生成候选节点进行增强，然后验证候选节点以减少幻觉并确保语义相关性，最后集成到分类法中。

Result: Taxoria输出增强后的分类法，包含来源追踪功能，并提供最终合并分类法的可视化分析。

Conclusion: Taxoria提供了一种新颖的分类法增强方法，利用LLM的能力来改进现有分类法的覆盖范围和时效性，同时通过验证机制确保质量。

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


### [16] [Adaptive Prefiltering for High-Dimensional Similarity Search: A Frequency-Aware Approach](https://arxiv.org/abs/2602.22214)
*Teodor-Ioan Calin*

Main category: cs.IR

TL;DR: 提出基于查询频率的自适应预过滤框架，通过Zipf分布划分查询空间并差异化分配计算资源，在ImageNet-1k上实现相同召回率下减少20.4%距离计算


<details>
  <summary>Details</summary>
Motivation: 现有统一搜索策略无法利用真实世界查询分布的异质性，导致计算资源分配效率低下

Method: 基于查询频率模式和聚类一致性度量的自适应预过滤框架，将查询空间按Zipf分布划分为频率层级，根据历史访问模式和局部密度特征分配差异化搜索策略

Result: 在ImageNet-1k使用CLIP嵌入的实验中，频率感知预算分配相比静态nprobe选择在保持相同召回率的情况下减少20.4%距离计算，同时在GPU加速的FAISS索引上保持亚毫秒级延迟

Conclusion: 该框架通过轻量级频率跟踪引入最小开销，并通过基于一致性的回退策略为未见查询提供优雅降级，有效提升了高维相似性搜索的效率

Abstract: High-dimensional similarity search underpins modern retrieval systems, yet uniform search strategies fail to exploit the heterogeneous nature of real-world query distributions. We present an adaptive prefiltering framework that leverages query frequency patterns and cluster coherence metrics to dynamically allocate computational budgets. Our approach partitions the query space into frequency tiers following Zipfian distributions and assigns differentiated search policies based on historical access patterns and local density characteristics. Experiments on ImageNet-1k using CLIP embeddings demonstrate that frequency-aware budget allocation achieves equivalent recall with 20.4% fewer distance computations compared to static nprobe selection, while maintaining sub-millisecond latency on GPU-accelerated FAISS indices. The framework introduces minimal overhead through lightweight frequency tracking and provides graceful degradation for unseen queries through coherence-based fallback policies.

</details>


### [17] [Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories](https://arxiv.org/abs/2602.22216)
*Diogo Pires,Yuriy Perezhohin,Mauro Castelli*

Main category: cs.IR

TL;DR: 本研究提出并评估了一个专为解剖病理学实验室设计的检索增强生成助手，用于解决静态协议文档过时、分散和难以搜索的问题，通过优化分块策略、检索方法和嵌入模型来提高协议查询的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解剖病理学中高达70%的医疗决策依赖于实验室诊断，但当前使用的静态文档（如印刷手册或PDF）往往过时、分散且难以搜索，导致工作流程错误和诊断延迟的风险增加。

Method: 研究构建了包含99个葡萄牙医疗机构解剖病理学协议的新语料库和323个问答对用于系统评估。进行了10个实验，比较不同分块策略（递归分块）、检索方法（混合检索）和嵌入模型。使用RAGAS框架（忠实度、答案相关性、上下文召回率）和top-k检索指标评估性能。

Result: 递归分块和混合检索提供了最强的基线性能。结合生物医学专用嵌入模型（MedEmbed）进一步提高了答案相关性（0.74）、忠实度（0.70）和上下文召回率（0.77）。top-k分析显示检索单个最高排名分块（k=1）能最大化效率和准确性，反映了解剖病理学协议的模块化结构。

Conclusion: 研究结果强调了在医疗保健领域部署RAG系统的关键设计考虑，展示了将静态文档转化为动态、可靠知识助手的潜力，从而提高实验室工作流程效率并支持患者安全。

Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks of workflow errors and diagnostic delays. This study proposes and evaluates a Retrieval-Augmented Generation (RAG) assistant tailored to AP laboratories, designed to provide technicians with context-grounded answers to protocol-related queries. We curated a novel corpus of 99 AP protocols from a Portuguese healthcare institution and constructed 323 question-answer pairs for systematic evaluation. Ten experiments were conducted, varying chunking strategies, retrieval methods, and embedding models. Performance was assessed using the RAGAS framework (faithfulness, answer relevance, context recall) alongside top-k retrieval metrics. Results show that recursive chunking and hybrid retrieval delivered the strongest baseline performance. Incorporating a biomedical-specific embedding model (MedEmbed) further improved answer relevance (0.74), faithfulness (0.70), and context recall (0.77), showing the importance of domain-specialised embeddings. Top-k analysis revealed that retrieving a single top-ranked chunk (k=1) maximized efficiency and accuracy, reflecting the modular structure of AP protocols. These findings highlight critical design considerations for deploying RAG systems in healthcare and demonstrate their potential to transform static documentation into dynamic, reliable knowledge assistants, thus improving laboratory workflow efficiency and supporting patient safety.

</details>


### [18] [RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge](https://arxiv.org/abs/2602.22217)
*Ahmed Bin Khalid*

Main category: cs.IR

TL;DR: RAGdb提出了一种单文件知识容器架构，将多模态数据摄取、ONNX提取和混合向量检索整合到单个SQLite容器中，消除对GPU推理的依赖，显著降低边缘计算门槛。


<details>
  <summary>Details</summary>
Motivation: 传统RAG架构需要云托管向量数据库、重型深度学习框架和高延迟嵌入推理服务器，这种"基础设施膨胀"阻碍了边缘计算、隔离环境和隐私敏感应用的发展，需要更轻量、便携的解决方案。

Method: 提出RAGdb单文件架构，整合自动多模态摄取、ONNX提取和混合向量检索到SQLite容器中。设计确定性混合评分函数(HSF)，结合亚线性TF-IDF向量化和精确子串增强，消除查询时GPU推理需求。

Result: 在Intel i7-1165G7消费级笔记本上测试显示：实体检索Recall@1达到100%；增量更新效率比冷启动提升31.6倍；磁盘占用比标准Docker RAG栈减少约99.5%，实现单文件知识容器。

Conclusion: RAGdb证明了单文件知识容器作为去中心化、本地优先AI可行原语的潜力，为边缘计算、隐私敏感和资源受限环境提供了高效、轻量的RAG解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, heavy deep learning frameworks (e.g., PyTorch, CUDA), and high-latency embedding inference servers. This ``infrastructure bloat'' creates a significant barrier to entry for edge computing, air-gapped environments, and privacy-constrained applications where data sovereignty is paramount.
  This paper introduces RAGdb, a novel monolithic architecture that consolidates automated multimodal ingestion, ONNX-based extraction, and hybrid vector retrieval into a single, portable SQLite container. We propose a deterministic Hybrid Scoring Function (HSF) that combines sublinear TF-IDF vectorization with exact substring boosting, eliminating the need for GPU inference at query time. Experimental evaluation on an Intel i7-1165G7 consumer laptop demonstrates that RAGdb achieves 100\% Recall@1 for entity retrieval and an ingestion efficiency gain of 31.6x during incremental updates compared to cold starts. Furthermore, the system reduces disk footprint by approximately 99.5\% compared to standard Docker-based RAG stacks, establishing the ``Single-File Knowledge Container'' as a viable primitive for decentralized, local-first AI.
  Keywords: Edge AI, Retrieval-Augmented Generation, Vector Search, Green AI, Serverless Architecture, Knowledge Graphs, Efficient Computing.

</details>


### [19] [Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications](https://arxiv.org/abs/2602.22219)
*Teri Rumble,Zbyněk Gazdík,Javad Zarrin,Jagdeep Ahluwalia*

Main category: cs.IR

TL;DR: 该研究提出并评估了针对知识图谱自然语言查询的多重检索-重排序RAG管道，在电子商务环境中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在非结构化文本上表现良好，但在结构化知识图谱应用中面临挑战：跨连接图谱的检索扩展和生成过程中上下文关系的保持。交叉编码器提高了检索精度，但与结构化数据的整合尚未充分探索。解决这些挑战对于开发生产环境中的领域特定助手至关重要。

Method: 研究设计并比较了针对知识图谱自然语言查询的多个检索器-重排序管道配置。使用生产规模的电子商务数据集STaRK半结构化知识库(SKB)，评估了针对语言查询优化的多种RAG管道配置。

Result: 实验结果显示相比已发布的基准有显著改进：Hit@1提高了20.4%，平均倒数排名(MRR)提高了14.5%。这些结果建立了将领域特定SKB集成到生成系统中的实用框架。

Conclusion: 该研究为生产就绪RAG系统的部署提供了可行的见解，其影响不仅限于电子商务，还扩展到其他需要从结构化知识库进行信息检索的领域。建立了一个将领域特定半结构化知识库集成到生成系统中的实用框架。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integrating external knowledge sources with generative models. Although RAG demonstrates strong performance on unstructured text, its application to structured knowledge graphs presents challenges: scaling retrieval across connected graphs and preserving contextual relationships during response generation. Cross-encoders refine retrieval precision, yet their integration with structured data remains underexplored. Addressing these challenges is crucial for developing domain-specific assistants that operate in production environments. This study presents the design and comparative evaluation of multiple Retriever-Reranker pipelines for knowledge graph natural language queries in e-Commerce contexts. Using the STaRK Semi-structured Knowledge Base (SKB), a production-scale e-Commerce dataset, we evaluate multiple RAG pipeline configurations optimized for language queries. Experimental results demonstrate substantial improvements over published benchmarks, achieving 20.4% higher Hit@1 and 14.5% higher Mean Reciprocal Rank (MRR). These findings establish a practical framework for integrating domain-specific SKBs into generative systems. Our contributions provide actionable insights for the deployment of production-ready RAG systems, with implications that extend beyond e-Commerce to other domains that require information retrieval from structured knowledge bases.

</details>


### [20] [SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG](https://arxiv.org/abs/2602.22225)
*Xuechen Zhang,Koustava Goswami,Samet Oymak,Jiasi Chen,Nedim Lipka*

Main category: cs.IR

TL;DR: SmartChunk是一个查询自适应的检索增强生成框架，通过动态调整检索粒度来提升长文档问答的效率和准确性，避免固定分块策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统存在静态分块和平面检索的局限性：文档被分割为固定大小的短块，检索质量对块大小高度敏感，容易引入噪声，且在大规模语料库上扩展性差。需要一种能根据查询动态调整检索粒度的自适应方法。

Method: SmartChunk包含两个核心组件：(1)规划器预测每个查询的最优分块抽象级别；(2)轻量级压缩模块生成高级分块嵌入而无需重复摘要。采用新颖的强化学习方案STITCH来训练规划器推理分块抽象。

Result: 在五个QA基准测试和一个域外数据集上，SmartChunk优于最先进的RAG基线方法，同时降低了成本。分析显示其在大规模语料库上具有良好的扩展性，并在域外数据集上获得一致增益。

Conclusion: SmartChunk通过查询自适应的分块检索，有效平衡了准确性和效率，避免了固定策略的缺点，为自适应检索提供了一个通用框架，在多样化的文档类型和查询风格中表现出色。

Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.

</details>


### [21] [SEGB: Self-Evolved Generative Bidding with Local Autoregressive Diffusion](https://arxiv.org/abs/2602.22226)
*Yulong Gao,Wan Jiang,Mingzhe Cao,Xuepu Wang,Zeyu Pan,Haonan Yang,Ye Liu,Xin Yang*

Main category: cs.IR

TL;DR: 提出SEGB框架，通过合成短期未来状态进行前瞻性规划，并结合价值引导的策略精炼，在离线环境下实现自动出价策略的自我进化，显著提升广告投放效果。


<details>
  <summary>Details</summary>
Motivation: 现有离线训练的生成式自动出价策略缺乏对动态市场的短期预见能力，通常依赖模拟器或外部专家进行后训练改进，存在关键局限性。

Method: 提出自我进化的生成式出价框架SEGB：1) 合成合理的短期未来状态为每次出价提供动态预见；2) 执行价值引导的策略精炼，无需外部干预即可迭代发现更优策略。

Result: 在AuctionNet基准测试和大规模A/B测试中，SEGB显著优于最先进的基线方法。大规模在线部署实现了目标成本+10.19%的增长，证明了其商业价值。

Conclusion: SEGB框架通过前瞻性规划和自我精炼，仅从静态数据中实现了稳健的策略改进，为自动出价提供了有效的离线自我进化解决方案。

Abstract: In the realm of online advertising, automated bidding has become a pivotal tool, enabling advertisers to efficiently capture impression opportunities in real-time. Recently, generative auto-bidding has shown significant promise, offering innovative solutions for effective ad optimization. However, existing offline-trained generative policies lack the near-term foresight required for dynamic markets and usually depend on simulators or external experts for post-training improvement. To overcome these critical limitations, we propose Self-Evolved Generative Bidding (SEGB), a framework that plans proactively and refines itself entirely offline. SEGB first synthesizes plausible short-horizon future states to guide each bid, providing the agent with crucial, dynamic foresight. Crucially, it then performs value-guided policy refinement to iteratively discover superior strategies without any external intervention. This self-contained approach uniquely enables robust policy improvement from static data alone. Experiments on the AuctionNet benchmark and a large-scale A/B test validate our approach, demonstrating that SEGB significantly outperforms state-of-the-art baselines. In a large-scale online deployment, it delivered substantial business value, achieving a +10.19% increase in target cost, proving the effectiveness of our advanced planning and evolution paradigm.

</details>


### [22] [RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval](https://arxiv.org/abs/2602.22278)
*Dawei Su,Dongsheng Wang*

Main category: cs.IR

TL;DR: RetLLM：无需训练和数据的多模态信息检索框架，通过直接提示MLLMs预测检索分数，采用粗筛-精排两阶段流程，在多个基准测试中超越微调模型


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的多模态信息检索方法存在预训练不一致问题且需要大量数据集进行微调，限制了其应用范围。需要一种无需训练和数据的MMIR方法，充分利用MLLMs固有的多模态推理能力。

Method: RetLLM将MMIR定义为相似度分数生成任务，采用粗筛-精排两阶段流程：1）粗筛阶段使用top-k过滤策略构建高质量候选池；2）精排阶段将查询和候选同时输入MLLMs预测检索分数。特别设计了视觉增强模块，在推理过程中帮助MLLMs重新拾取被遗忘的视觉信息。

Result: 在多个MMIR基准测试中，RetLLM超越了经过微调的模型性能。消融研究验证了各个组件的有效性，证明了MLLMs无需任何训练即可实现强大的MMIR性能。

Conclusion: RetLLM展示了MLLMs固有的多模态推理能力，通过简单可扩展的框架实现了无需训练和数据的强大MMIR性能，为多模态检索提供了新的范式。

Abstract: Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM

</details>


### [23] [TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.22521)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: TFPS是一种时间过滤增强的正样本集构建方法，通过时间衰减模型、分层过滤和层增强策略，为隐式反馈推荐系统构建高质量正样本集，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有负采样方法主要优化负样本而忽视正样本探索；现有去噪方法忽略时间信息；现有序列方法忽略时间间隔信息，难以准确捕捉用户当前偏好。

Method: 1) 基于交互时间间隔设计时间衰减模型，将原始图转换为加权用户-物品二部图；2) 基于预定义过滤操作对加权二部图进行分层；3) 设计层增强策略为分层子图构建高质量正样本集。

Result: 在三个真实世界数据集上的大量实验证明了该方法的有效性，TFPS能够提高Recall@k和NDCG@k指标，并且可以与各种隐式CF推荐器或负采样方法集成以增强性能。

Conclusion: TFPS从数据角度提出了一种新颖的时间过滤增强方法，通过构建高质量正样本集解决了现有方法忽视时间间隔信息的问题，能够更准确地捕捉用户当前偏好，提升推荐系统性能。

Abstract: The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.

</details>


### [24] [Towards Dynamic Dense Retrieval with Routing Strategy](https://arxiv.org/abs/2602.22547)
*Zhan Su,Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Bingbing Wen,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出动态稠密检索(DDR)方法，通过前缀调优模块和动态路由策略，实现仅需2%训练参数的灵活领域适应，解决传统稠密检索模型微调存在的领域适应困难和更新成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统稠密检索(DR)应用范式存在两大局限：1) 训练数据有限时难以适应新领域；2) 模型更新需要从头训练，频繁更新场景下成本过高。需要一种更灵活、高效的稠密检索适应方法。

Method: 提出动态稠密检索(DDR)，使用前缀调优作为特定领域专用模块，通过动态路由策略组合这些模块，实现检索部分的灵活领域适应。仅需训练少量参数(约2%)。

Result: 在六个零样本下游任务上的广泛评估表明，DDR方法能够超越传统稠密检索方法，同时仅使用2%的训练参数，为信息检索中实现更灵活的稠密检索铺平道路。

Conclusion: DDR通过模块化设计和动态路由策略，解决了稠密检索领域适应和频繁更新的成本问题，是未来将稠密检索应用于各种任务的有前景方向。

Abstract: The \textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.
  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \textit{dynamic dense retrieval} (DDR). DDR uses \textit{prefix tuning} as a \textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.

</details>


### [25] [Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking](https://arxiv.org/abs/2602.22591)
*Haodong Chen,Shengyao Zhuang,Zheng Yao,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 本文系统评估了LLM文档重排中的生成、似然和内部注意力机制，发现注意力信号在Transformer层中呈现"钟形曲线"分布，据此提出Selective-ICR策略，在保持效果的同时将推理延迟降低30%-50%，小模型即可超越传统生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文档重排方法主要依赖生成评分或输出logits，面临推理延迟和结果一致性的瓶颈。虽然ICR方法通过提取内部注意力信号避免了生成开销，但现有方法简单聚合所有层信号，未探索层间贡献差异及其在不同架构中的一致性，也缺乏对内部注意力与传统生成/似然机制的统一比较。

Method: 1) 对生成、似然和内部注意力机制进行正交评估，比较它们在多种重排框架下的表现；2) 识别Transformer层中相关性信号的"钟形曲线"分布规律；3) 基于此提出Selective-ICR策略，选择性地利用关键层信号而非简单聚合所有层；4) 在BRIGHT推理密集型基准上进行评估。

Result: 1) 发现内部注意力信号在Transformer层中呈现一致的"钟形曲线"分布；2) Selective-ICR将推理延迟降低30%-50%而不损失效果；3) 在BRIGHT基准上，零样本8B模型匹配14B强化学习重排器的性能，0.6B模型甚至超越最先进的生成方法；4) 高质量上下文注意力信号显著减少了对模型缩放和强化学习的依赖。

Conclusion: 内部注意力信号在复杂推理排序任务中具有巨大潜力，Selective-ICR重新定义了LLM重排的效率-效果边界。精确捕捉高质量上下文注意力信号可以替代模型缩放和强化学习，为高效文档重排提供了新方向。

Abstract: Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.
  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal "bell-curve" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.

</details>


### [26] [Fine-grained Semantics Integration for Large Language Model-based Recommendation](https://arxiv.org/abs/2602.22632)
*Jiawen Feng,Xiaoyu Kong,Leheng Sheng,Bin Wu,Chao Yi,Feifang Yang,Xiang-Rong Sheng,Han Zhu,Xiang Wang,Jiancan Wu,Xiangnan He*

Main category: cs.IR

TL;DR: TS-Rec通过语义感知嵌入初始化和令牌级语义对齐，解决LLM推荐系统中语义标识符空间建模的两大挑战，显著提升生成式推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的生成式推荐系统在语义标识符（SID）空间建模面临两个根本挑战：1）语义无意义的初始化（SID令牌随机初始化，切断了与预训练语言空间的语义联系）；2）粗粒度对齐（现有SFT对齐任务主要关注项目级优化，忽略了SID序列中单个令牌的语义）。

Method: TS-Rec包含两个核心组件：1）语义感知嵌入初始化（SA-Init）：通过教师模型提取关键词，对其预训练嵌入应用均值池化来初始化SID令牌嵌入；2）令牌级语义对齐（TS-Align）：将SID序列中的单个令牌与对应项目簇的共享语义进行对齐。

Result: 在两个真实世界基准测试上的广泛实验表明，TS-Rec在所有标准指标上持续优于传统和生成式基线方法。结果证明整合细粒度语义信息能显著提升基于LLM的生成式推荐器性能。

Conclusion: TS-Rec通过解决SID空间建模中的语义初始化和对齐问题，成功将令牌级语义整合到LLM推荐器中，为生成式推荐系统提供了更精细的语义建模方法。

Abstract: Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.

</details>


### [27] [Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators](https://arxiv.org/abs/2602.22647)
*Zhengyang Su,Isay Katsman,Yueqi Wang,Ruining He,Lukasz Heldt,Raghunandan Keshavan,Shao-Chuan Wang,Xinyang Yi,Mingyan Gao,Onkar Dalal,Lichan Hong,Ed Chi,Ningren Han*

Main category: cs.IR

TL;DR: STATIC是一种用于TPU/GPU的高效约束解码技术，通过将前缀树扁平化为静态CSR矩阵，将不规则树遍历转换为向量化稀疏矩阵操作，显著提升LLM生成检索在工业推荐系统中的性能。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统需要将输出空间限制在基于业务逻辑的约束子集（如内容新鲜度或产品类别），但标准自回归解码无法原生支持这种约束。现有基于前缀树的约束解码方法在硬件加速器上会产生严重的延迟惩罚。

Method: 提出STATIC技术，将前缀树扁平化为静态压缩稀疏行（CSR）矩阵，将不规则树遍历转换为完全向量化的稀疏矩阵操作，从而在TPU/GPU上实现高效约束解码。

Result: 在大型工业视频推荐平台部署STATIC，产生显著产品指标影响，延迟开销极低（每步0.033ms，占推理时间0.25%），相比CPU trie实现加速948倍，相比硬件加速二分搜索基线加速47-1033倍。在学术基准测试中，STATIC显著改善了生成检索的冷启动性能。

Conclusion: STATIC实现了首个生产规模的严格约束生成检索部署，通过将树结构转换为稀疏矩阵操作，在硬件加速器上实现了高效、可扩展的约束解码，为工业LLM推荐系统提供了实用的解决方案。

Abstract: Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.

</details>


### [28] [Generative Recommendation for Large-Scale Advertising](https://arxiv.org/abs/2602.22732)
*Ben Xue,Dan Liu,Lixiang Wang,Mingjie Sun,Peng Wang,Pengfei Zhang,Shaoyun Shi,Tianyu Xu,Yunhao Sha,Zhiqiang Liu,Bo Kong,Bo Wang,Hang Yang,Jieting Xue,Junhao Wang,Shengyu Wang,Shuping Hui,Wencai Ye,Xiao Lin,Yongzhi Li,Yuhang Chen,Zhihui Yin,Quan Chen,Shiyang Wen,Wenjin Wu,Han Li,Guorui Zhou,Changcheng Li,Peng Jiang*

Main category: cs.IR

TL;DR: GR4AD是一个面向广告场景的生产级生成式推荐系统，通过统一广告语义ID、惰性自回归解码器、价值感知优化算法等创新设计，在大规模实时广告推荐中实现了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 在大规模广告系统中部署实时生成式推荐面临独特挑战：需要超越传统LLM训练和服务范式，在固定服务预算下实现规模化，同时将优化与业务价值对齐。

Method: 1. UA-SID统一广告语义ID捕获复杂业务信息；2. LazyAR惰性自回归解码器放松层间依赖，降低短序列多候选生成推理成本；3. VSL价值感知监督学习和RSPO排序引导的软最大偏好优化算法，实现列表级强化学习；4. 动态束搜索服务根据生成层级和在线负载自适应调整束宽。

Result: 大规模在线A/B测试显示广告收入提升最高达4.2%，超越现有DLRM基线；模型扩展和推理时扩展均带来持续增益；已在快手广告系统全量部署，服务超4亿用户，实现高吞吐实时服务。

Conclusion: GR4AD通过架构、学习和服务的协同设计，成功将生成式推荐应用于大规模广告系统，在保持效果的同时控制推理成本，为工业级生成式推荐部署提供了可行方案。

Abstract: Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.

</details>


### [29] [SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress](https://arxiv.org/abs/2602.22913)
*Yang Yu,Lei Kou,Huaikuan Yi,Bin Chen,Yayu Cao,Lei Shen,Chao Zhang,Bing Wang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: SIGMA是阿里巴巴速卖通开发的语义基础指令驱动生成式多任务推荐系统，通过统一潜在空间捕捉语义和协同关系，结合混合项目标记化和三步骤生成过程，实现基于指令的多样化推荐需求。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法大多局限于交互驱动的下一项预测范式，难以快速适应趋势变化，也无法满足现实场景中多样化的推荐任务和业务特定需求。

Method: 1) 通过统一潜在空间将项目实体基础于通用语义，捕捉语义和协同关系；2) 开发混合项目标记化方法进行精确建模和高效生成；3) 构建大规模多任务SFT数据集支持指令跟随；4) 设计三步骤项目生成过程，结合自适应概率融合机制校准输出分布。

Result: 广泛的离线实验和在线A/B测试证明了SIGMA的有效性，能够满足多样化的推荐需求并提升推荐准确性和多样性。

Conclusion: SIGMA通过语义基础和指令驱动的方法，成功解决了传统生成式推荐在适应性和任务多样性方面的限制，为实际业务场景提供了灵活高效的推荐解决方案。

Abstract: With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.

</details>


### [30] [Sequential Regression for Continuous Value Prediction using Residual Quantization](https://arxiv.org/abs/2602.23012)
*Runpeng Cui,Zhipeng Sun,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出基于残差量化的序列学习框架，通过从粗到细的递归预测有序量化码来表示连续值，解决了推荐系统中连续值预测的分布复杂性和长尾问题。


<details>
  <summary>Details</summary>
Motivation: 工业级推荐系统中的连续值预测（如观看时长、GMV）面临数据分布高度复杂和长尾的挑战。现有生成方法依赖刚性参数分布假设，当假设与现实数据不符时性能受限：简化形式无法充分建模现实复杂性，复杂假设则存在可扩展性和泛化性差的问题。

Method: 提出基于残差量化(RQ)的序列学习框架：1) 将目标连续值表示为有序量化码的和；2) 从粗到细粒度递归预测，量化误差逐渐减小；3) 引入表示学习目标，使RQ码嵌入空间与目标值的序数结构对齐，捕获量化码的连续表示以提高预测精度。

Result: 在LTV和观看时长预测的公共基准测试以及工业级短视频推荐平台的大规模在线GMV预测实验中，该方法均优于最先进方法，并在推荐系统多样连续值预测任务中展现出强大的泛化能力。

Conclusion: 残差量化框架有效解决了推荐系统中连续值预测的分布复杂性问题，通过从粗到细的递归量化码预测和表示学习对齐，实现了优于现有方法的性能，并在多个任务中表现出良好的泛化性。

Abstract: Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.

</details>


### [31] [MoDora: Tree-Based Semi-Structured Document Analysis System](https://arxiv.org/abs/2602.23061)
*Bangrui Xu,Qihang Yao,Zirui Tang,Xuanhe Zhou,Yeye He,Shihan Yu,Qianqian Xu,Bin Wang,Guoliang Li,Conghui He,Fan Wu*

Main category: cs.IR

TL;DR: MoDora：基于LLM的半结构化文档分析系统，通过布局感知组件构建、层次化组织结构和问题类型感知检索，显著提升文档问答准确率


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在半结构化文档问答中的三大技术挑战：OCR提取元素碎片化且缺乏语义上下文；缺乏有效表示来捕捉文档层次结构和布局差异；难以检索和整合跨区域分散的相关信息

Method: 1) 局部对齐聚合策略将OCR解析元素转换为布局感知组件，并进行类型特定信息提取；2) 设计组件关联树(CCTree)层次化组织组件，通过自底向上级联摘要建模组件间关系和布局差异；3) 问题类型感知检索策略，支持基于布局的网格分区定位检索和LLM引导的语义检索剪枝

Result: 实验表明MoDora在准确率上比基线方法提升5.97%-61.07%，显著优于现有方法

Conclusion: MoDora通过创新的组件表示、层次化组织和智能检索策略，有效解决了半结构化文档问答的关键挑战，为实际应用提供了强大工具

Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.
  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.

</details>


### [32] [MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System](https://arxiv.org/abs/2602.23105)
*Yusheng Huang,Pengbo Xu,Shen Wang,Changxin Lao,Jiangxia Cao,Shuang Wen,Shuang Yang,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: MaRI是一种无损加速推荐系统排序模型推理的矩阵重参数化框架，通过消除用户侧计算冗余实现加速


<details>
  <summary>Details</summary>
Motivation: 现有排序模型加速技术（结构轻量化或知识蒸馏）通常会导致精度下降，而通过优化特征融合矩阵乘法（特别是结构重参数化）实现无损加速的研究不足。观察到特征融合矩阵乘法中用户侧计算存在冗余

Method: 提出MaRI（Matrix Re-parameterized Inference）框架，采用结构重参数化哲学来缓解用户侧计算冗余，作为现有技术的补充方法

Result: 未在摘要中明确说明，但声称能实现排序模型推理加速且无任何精度损失

Conclusion: MaRI是一种新颖的矩阵重参数化推理框架，可作为现有技术的补充，实现排序模型的无损加速

Abstract: Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.

</details>


### [33] [From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2602.23132)
*Ruochen Yang,Xiaodong Li,Jiawei Sheng,Jiangxia Cao,Xinkui Lin,Shen Wang,Shuang Yang,Zhaojie Liu,Tingwen Liu*

Main category: cs.IR

TL;DR: FatsMB是一个基于扩散模型的框架，通过从行为无关到行为特定的潜在空间偏好生成，实现多样且准确的多行为序列推荐。


<details>
  <summary>Details</summary>
Motivation: 现有多行为序列推荐方法存在两个主要问题：1）忽视用户潜在偏好决策机制，导致次优解；2）基于偏好评分的判别式范式无法有效处理从低熵行为到高熵项目的不确定性，难以提供高效多样的推荐。

Method: 提出FatsMB框架：1）设计多行为自动编码器（MBAE）构建统一的用户潜在偏好空间，使用行为感知RoPE（BaRoPE）进行多信息融合；2）在潜在空间进行目标行为特定偏好转移，丰富信息先验；3）引入多条件引导层归一化（MCGLN）进行去噪。

Result: 在真实世界数据集上的大量实验证明了模型的有效性。

Conclusion: FatsMB通过从行为无关到行为特定的潜在空间偏好生成，能够有效捕获用户潜在偏好并处理不确定性，实现多样且准确的多行为序列推荐。

Abstract: Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.

</details>


### [34] [Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments](https://arxiv.org/abs/2602.23234)
*Evangelia Christakopoulou,Vivekkumar Patel,Hemanth Velaga,Sandip Gaikwad*

Main category: cs.IR

TL;DR: 使用LLM生成文本相关性标签解决数据稀缺问题，通过增强生产排序器在行为相关性和文本相关性上同时提升，A/B测试显示转化率显著提高


<details>
  <summary>Details</summary>
Motivation: 商业搜索系统需要同时优化行为相关性（用户点击/下载倾向）和文本相关性（结果与查询的语义匹配），但面临专家提供的文本相关性标签稀缺而行为相关性标签丰富的问题

Method: 系统评估LLM配置，发现专门微调的模型在提供高质量文本相关性标签方面优于大型预训练模型；使用最优模型作为"力倍增器"生成数百万文本相关性标签；将这些标签集成到生产排序器中

Result: 离线NDCG在行为相关性和文本相关性上同时提升，Pareto前沿向外移动；全球A/B测试显示App Store排序器转化率显著提高+0.24%，尾部查询表现提升最明显，文本相关性标签在缺乏可靠行为相关性信号时提供了稳健信号

Conclusion: 使用专门微调的LLM生成大规模文本相关性标签是解决数据稀缺问题的有效方法，能够同时提升搜索系统的行为相关性和文本相关性，特别是在尾部查询场景中效果显著

Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.

</details>
