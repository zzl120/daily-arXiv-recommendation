<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

TL;DR: DSKD框架将词典资源整合到解码器式LLM训练中，无需推理时词典查找，显著提升知识蒸馏性能


<details>
  <summary>Details</summary>
Motivation: LLM学习丰富的语义信息，但常忽略结构化词汇知识（如词义和关系）。先前工作表明词典能改进编码器模型的知识蒸馏，但应用于解码器生成模型仍具挑战性

Method: 提出Decoder-based Sense Knowledge Distillation (DSKD)框架，将词典资源整合到解码器式LLM训练中，无需推理时进行词典查找

Result: 在多样化基准测试上的广泛实验表明，DSKD显著提升解码器的知识蒸馏性能，使生成模型能继承结构化语义同时保持高效训练

Conclusion: DSKD成功将词典资源整合到解码器LLM训练中，解决了生成模型继承结构化词汇知识的挑战，实现了高效的知识蒸馏

Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: Taxoria：一种基于大语言模型的分类法增强管道，通过现有分类法作为种子，利用LLM生成候选节点，经过验证后整合，以解决现有分类法覆盖不足、节点过时或模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类法存在覆盖范围有限、节点过时或模糊等问题，降低了知识检索的有效性。需要一种方法来增强和更新分类法，提高其在实际应用中的效用。

Method: Taxoria采用基于大语言模型的分类法增强管道：1）以现有分类法作为种子；2）提示LLM生成候选节点；3）验证候选节点以减少幻觉并确保语义相关性；4）整合验证后的节点；5）提供增强后的分类法，包含来源追踪和可视化分析。

Result: Taxoria能够生成增强的分类法，包含：1）扩展的节点覆盖；2）更新的分类结构；3）语义相关的节点；4）来源追踪信息；5）可视化分析工具。

Conclusion: Taxoria提供了一种有效的分类法增强方法，利用LLM的能力扩展和更新现有分类法，同时通过验证机制确保质量，为知识检索和结构化信息管理提供了改进方案。

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统通过结合作者知识图谱与检索增强生成，为LLMs提供可控的学术背景和可追溯的灵感路径，以生成更优质的科学创意。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在科学创意生成中缺乏可控的学术背景和可追溯的灵感路径，导致生成结果质量受限，需要构建能够提供结构化外部知识支持的系统。

Method: 1) 提出作者中心的知识图谱构建方法和灵感源采样算法构建外部知识库；2) 设计结合RAG和GraphRAG的混合检索机制获取深度和广度知识；3) 采用基于强化学习原理的Prompt优化策略自动指导LLMs优化结果。

Result: 在基于arXiv(2018-2023)构建的数据集上，使用GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5进行实验，GYWI在创新性、可靠性、相关性等多个指标上显著优于主流LLMs。

Conclusion: GYWI系统通过结构化外部知识库和混合检索机制，有效提升了LLMs生成科学创意的质量，为可控、可追溯的科学创意生成提供了可行方案。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>
