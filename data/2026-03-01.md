<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.IR](#cs.IR) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统通过作者知识图谱与检索增强生成结合，为LLMs提供可控学术背景和可追溯灵感路径，显著提升科学想法生成质量


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在科学想法生成中缺乏可控的学术背景和可追溯的灵感路径，需要构建能够提供深度知识支持和灵感溯源的系统

Method: 提出GYWI系统：1) 作者中心知识图谱构建与灵感源采样算法；2) RAG与GraphRAG混合检索机制；3) 结合强化学习的Prompt优化策略；4) 基于arXiv的多维度评估方法

Result: 在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等LLMs上实验，GYWI在新颖性、可靠性、相关性等多个指标上显著优于主流LLMs

Conclusion: GYWI系统通过整合知识图谱与混合检索机制，有效解决了LLMs科学想法生成中背景不可控和灵感路径不可追溯的问题，显著提升了生成质量

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [2] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard：基于检索增强的多智能体框架，将安全评估重构为证据辩论，实现无需微调的零样本适应性安全机制


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全机制依赖静态微调分类器，存在适应性僵化问题，无法在不昂贵重新训练的情况下执行新的治理规则。需要一种能够灵活适应新政策的安全评估框架。

Method: 引入CourtGuard框架，采用检索增强的多智能体架构，将安全评估重构为基于外部政策文档的证据辩论。通过编排对抗性辩论，实现无需微调的安全评估。

Result: 在7个安全基准测试中达到最先进性能，超越专用政策遵循基线。零样本适应性方面，通过替换参考政策成功泛化到域外维基百科破坏检测任务（90%准确率）。自动数据策展和审计方面，利用框架策展和审计了9个新颖的对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了稳健、可解释且适应性强的路径，能够满足当前和未来的监管要求。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [3] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文提出选择性策略检索（SSR）框架，通过建模策略可执行性来改进数学推理的示例引导效果，解决了现有方法在不同问题和模型间效果不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 基于示例的引导在推理时被广泛用于改进数学推理，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用（策略是否出现在成功解决方案中）与策略可执行性（策略作为目标模型引导时是否仍然有效）之间未被充分探索的差距。

Method: 通过分析配对的人类编写和模型生成的解决方案，发现人类和模型衍生策略在结构化、领域依赖方面存在差异，导致互补优势和一致的源依赖反转。基于此诊断，提出选择性策略检索（SSR）框架，通过经验性、多路径、源感知信号选择性地检索和组合策略来显式建模可执行性。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导取得了可靠且一致的改进，在AIME25上准确率提升高达+13点，在Apex上提升+5点，特别适用于紧凑推理模型。

Conclusion: 策略使用与可执行性之间的系统差异是示例引导不稳定的根本原因，SSR框架通过显式建模可执行性并选择性组合不同来源的策略，显著提升了数学推理的引导效果和稳定性。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [4] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：一种利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务来减少长时智能体任务中的内存使用


<details>
  <summary>Details</summary>
Motivation: 长时智能体任务（如深度研究）需要在多个网页和文档间进行多跳推理，导致LLM上下文被外部检索的令牌主导，内存使用快速增长并限制解码性能。现有启发式KV缓存压缩技术无法有效支持多步推理模型。

Method: 提出SideQuest方法，利用大型推理模型自身通过推理上下文令牌的有用性来执行KV缓存压缩。为防止压缩管理过程的令牌污染模型内存，将KV缓存压缩框架为与主要推理任务并行执行的辅助任务。

Result: 仅使用215个样本训练的模型评估显示，SideQuest在智能体任务中将峰值令牌使用减少高达65%，精度下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让大型推理模型自身管理其KV缓存，有效解决了长时智能体任务中的内存增长问题，为多步推理模型提供了高效的压缩解决方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [5] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于在线广告多渠道自动出价的分层框架，结合生成式规划和实时控制，通过扩散模型进行预算分配，相比现有基线提升13.57%的回报率。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂多变，特别是在多渠道场景下，需要有效分配预算和约束以优化投资回报。现有方法存在局限性：基于优化的方法缺乏动态适应性，而强化学习方法难以捕捉历史依赖性和观测模式。

Method: 提出AHBid（Adaptable Hierarchical Bidding）框架，整合生成式规划和实时控制。高层使用基于扩散模型的生成式规划器动态分配预算和约束，捕捉历史上下文和时间模式；引入约束执行机制确保符合约束条件，以及轨迹细化机制利用历史数据增强环境适应性；底层采用基于控制的出价算法，结合历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中验证了AHBid的有效性，相比现有基线实现了13.57%的整体回报提升。

Conclusion: AHBid框架通过整合生成式规划和实时控制，有效解决了多渠道自动出价中的动态适应性和历史依赖性问题，显著提升了广告投放的投资回报。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [6] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决数据稀疏和冷启动问题，避免负迁移。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能依赖于训练数据的质量、数量和相关性。跨域数据融合面临领域差异导致的负迁移问题，现有模型中心范式依赖复杂架构但难以捕捉跨域的非结构化序列依赖，导致泛化能力差且计算资源需求高。

Method: 提出Taesar框架，采用数据中心的序列再生方法，使用对比解码机制自适应地将跨域上下文编码到目标域序列中，使标准模型无需复杂融合架构即可学习复杂依赖关系。

Result: 实验表明Taesar优于模型中心解决方案，并能泛化到各种序列模型。通过生成丰富的数据集，Taesar有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生框架，解决了跨域推荐中的负迁移问题，使标准模型能够学习跨域的复杂依赖关系，在性能和泛化能力上优于现有模型中心方法。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [7] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench：用于评估LLM在真实智能体应用中长期记忆能力的新基准，包含真实和合成轨迹数据，并提出了基于因果图和工具增强检索的AMA-Agent记忆系统


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆评估标准与现实应用存在显著差距：当前基准主要关注对话为中心的人机交互，而实际智能体记忆由连续的机器生成表示组成的环境交互流构成

Method: 1) 提出AMA-Bench基准，包含真实世界智能体轨迹和可扩展到任意长度的合成轨迹；2) 设计AMA-Agent记忆系统，采用因果图和工具增强检索来克服现有记忆系统的局限性

Result: AMA-Agent在AMA-Bench上达到57.22%的平均准确率，比最强的记忆系统基线高出11.16%。研究表明现有记忆系统表现不佳主要因为缺乏因果关系和目标信息，以及相似性检索的损失性限制

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效解决了现有记忆系统的局限性，为LLM在复杂智能体应用中的长期记忆能力提供了更好的解决方案

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [8] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS：一个用于质谱预测的灵活基准框架，支持构建和评估多种深度学习模型架构，提供性能影响因素分析和实际应用指导


<details>
  <summary>Details</summary>
Motivation: 化学分子的鉴定和性质预测在药物发现和材料科学中至关重要，串联质谱技术提供了有价值的分裂线索。然而，实验光谱的缺乏阻碍了分子鉴定，需要建立计算模型的预测方法。深度学习模型在预测分子结构光谱方面表现出潜力，但由于方法异质性和缺乏明确定义的基准，整体评估仍然具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种不同模型架构组合，在预处理公共数据集上使用不同指标评估性能。框架分析了影响性能的因素，包括数据集结构多样性、学习率和数据稀疏性等超参数、预训练效果、元数据消融设置和跨域迁移学习分析。

Result: FlexMS提供了实用的模型选择指导，并通过检索基准模拟实际鉴定场景，基于预测光谱对潜在匹配进行评分。该框架能够系统评估不同模型架构在质谱预测任务中的表现。

Conclusion: FlexMS基准框架解决了质谱预测领域缺乏标准化评估的问题，为研究人员提供了灵活的工具来构建和比较不同深度学习模型，同时提供了影响模型性能的关键因素分析，有助于推动该领域的发展。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [9] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电信号解码方法，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的脑电分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，需要更精细的几何深度学习模型

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真

Result: 大量实验表明，该框架显著优于现有脑电表示方法，展现出卓越的鲁棒性和泛化能力

Conclusion: RepSPD通过结合几何深度学习和功能连接特征，有效提升了脑电信号解码的准确性和可靠性，为神经科学和临床应用提供了更强大的工具

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [10] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个无需重新训练的动态优化多智能体系统信息流框架，通过检索增强的校正器纠正错误，无法修复的输出则被剪枝以防止错误传播，在数学基准测试上平均准确率提升6.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在单个参与者生成错误信息导致的级联影响问题。现有解决方案通常采用刚性结构工程或昂贵的微调，限制了系统的可部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，使用检索增强校正器基于失败驱动指示器池迭代纠正错误。该机制利用蒸馏的失败模式作为先验知识精确识别潜在错误，无法修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试上，AgentDropoutV2显著提升了多智能体系统的任务性能，平均准确率增益达到6.3个百分点。系统表现出强大的泛化能力和适应性，能够根据任务难度动态调整校正力度，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2提供了一个无需重新训练的动态优化多智能体系统信息流的有效框架，通过主动错误检测、校正和剪枝机制显著提升系统性能，同时保持部署灵活性和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [11] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN：基于神经ODE的脑电动态预测框架，通过谱图节点整合时空频特征，建模连续潜在动态


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通过递归架构离散化时间建模连续脑动态，导致累积预测误差和无法捕捉EEG的瞬时非线性特征

Method: 提出ODEBRAIN框架：1) 将时空频特征整合到谱图节点中；2) 使用神经ODE建模连续潜在动态；3) 确保潜表示能捕捉任意时间点的复杂脑状态随机变化

Result: 大量实验验证ODEBRAIN在EEG动态预测上显著优于现有方法，具有增强的鲁棒性和泛化能力

Conclusion: ODEBRAIN通过神经ODE框架有效克服传统方法的局限性，为神经科学研究和临床应用提供了更准确的脑动态建模工具

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [12] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: Taxoria：一种利用大语言模型增强现有分类法的新型管道，通过候选节点生成与验证来扩展分类法覆盖范围


<details>
  <summary>Details</summary>
Motivation: 现有分类法存在覆盖范围有限、节点过时或模糊的问题，降低了知识检索的有效性，需要一种系统性的方法来增强和更新分类法

Method: Taxoria使用现有分类法作为种子，提示大语言模型生成候选节点，然后通过验证机制减轻幻觉并确保语义相关性，最后整合生成增强后的分类法，包含来源追踪和可视化功能

Result: 开发了一个完整的分类法增强管道，能够生成具有来源追踪的增强分类法，并提供最终合并分类法的可视化分析

Conclusion: Taxoria提供了一种有效的方法来增强现有分类法，通过结合大语言模型的生成能力和验证机制，解决了分类法覆盖不足和过时的问题

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


### [13] [Adaptive Prefiltering for High-Dimensional Similarity Search: A Frequency-Aware Approach](https://arxiv.org/abs/2602.22214)
*Teodor-Ioan Calin*

Main category: cs.IR

TL;DR: 提出自适应预过滤框架，利用查询频率模式和聚类一致性指标动态分配计算预算，在保持召回率的同时减少20.4%距离计算


<details>
  <summary>Details</summary>
Motivation: 高维相似性搜索是现代检索系统的基础，但统一的搜索策略无法利用现实世界查询分布的异构特性。需要根据查询频率模式和聚类特征动态分配计算资源。

Method: 基于Zipf分布将查询空间划分为频率层级，根据历史访问模式和局部密度特征分配差异化搜索策略。通过轻量级频率跟踪和基于一致性的回退策略处理未见查询。

Result: 在ImageNet-1k数据集使用CLIP嵌入的实验中，频率感知预算分配相比静态nprobe选择在保持相同召回率的情况下减少了20.4%的距离计算，同时在GPU加速的FAISS索引上保持亚毫秒级延迟。

Conclusion: 自适应预过滤框架通过利用查询频率模式和聚类一致性指标，能够有效减少高维相似性搜索的计算成本，同时保持检索性能，为实际检索系统提供了实用的优化方案。

Abstract: High-dimensional similarity search underpins modern retrieval systems, yet uniform search strategies fail to exploit the heterogeneous nature of real-world query distributions. We present an adaptive prefiltering framework that leverages query frequency patterns and cluster coherence metrics to dynamically allocate computational budgets. Our approach partitions the query space into frequency tiers following Zipfian distributions and assigns differentiated search policies based on historical access patterns and local density characteristics. Experiments on ImageNet-1k using CLIP embeddings demonstrate that frequency-aware budget allocation achieves equivalent recall with 20.4% fewer distance computations compared to static nprobe selection, while maintaining sub-millisecond latency on GPU-accelerated FAISS indices. The framework introduces minimal overhead through lightweight frequency tracking and provides graceful degradation for unseen queries through coherence-based fallback policies.

</details>


### [14] [Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories](https://arxiv.org/abs/2602.22216)
*Diogo Pires,Yuriy Perezhohin,Mauro Castelli*

Main category: cs.IR

TL;DR: 本研究提出并评估了一个专为解剖病理学实验室设计的检索增强生成助手，通过系统实验验证了递归分块、混合检索和生物医学专用嵌入模型在提升协议查询回答质量方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 解剖病理学中高达70%的医疗决策依赖于实验室诊断，但静态文档（如印刷手册或PDF）通常过时、分散且难以搜索，导致工作流程错误和诊断延迟的风险。需要将静态文档转化为动态、可靠的知识助手以提高实验室工作效率和患者安全。

Method: 研究构建了包含99个葡萄牙医疗机构解剖病理学协议的新语料库和323个问答对用于系统评估。进行了10个实验，比较不同分块策略、检索方法和嵌入模型。使用RAGAS框架（忠实度、答案相关性、上下文召回率）和top-k检索指标评估性能。

Result: 递归分块和混合检索提供了最强的基线性能。结合生物医学专用嵌入模型（MedEmbed）进一步提高了答案相关性（0.74）、忠实度（0.70）和上下文召回率（0.77）。top-k分析显示检索单个最高排名分块（k=1）能最大化效率和准确性。

Conclusion: 研究强调了在医疗保健领域部署RAG系统的关键设计考虑，展示了将静态文档转化为动态、可靠知识助手的潜力，从而改善实验室工作流程效率并支持患者安全。生物医学专用嵌入模型和优化的检索策略对提升系统性能至关重要。

Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks of workflow errors and diagnostic delays. This study proposes and evaluates a Retrieval-Augmented Generation (RAG) assistant tailored to AP laboratories, designed to provide technicians with context-grounded answers to protocol-related queries. We curated a novel corpus of 99 AP protocols from a Portuguese healthcare institution and constructed 323 question-answer pairs for systematic evaluation. Ten experiments were conducted, varying chunking strategies, retrieval methods, and embedding models. Performance was assessed using the RAGAS framework (faithfulness, answer relevance, context recall) alongside top-k retrieval metrics. Results show that recursive chunking and hybrid retrieval delivered the strongest baseline performance. Incorporating a biomedical-specific embedding model (MedEmbed) further improved answer relevance (0.74), faithfulness (0.70), and context recall (0.77), showing the importance of domain-specialised embeddings. Top-k analysis revealed that retrieving a single top-ranked chunk (k=1) maximized efficiency and accuracy, reflecting the modular structure of AP protocols. These findings highlight critical design considerations for deploying RAG systems in healthcare and demonstrate their potential to transform static documentation into dynamic, reliable knowledge assistants, thus improving laboratory workflow efficiency and supporting patient safety.

</details>


### [15] [RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge](https://arxiv.org/abs/2602.22217)
*Ahmed Bin Khalid*

Main category: cs.IR

TL;DR: RAGdb：一种新颖的单体式RAG架构，将多模态数据摄取、ONNX特征提取和混合向量检索整合到单个SQLite容器中，专为边缘计算和隐私敏感场景设计，无需GPU推理，显著降低资源占用。


<details>
  <summary>Details</summary>
Motivation: 传统RAG架构过于复杂，需要云托管向量数据库、深度学习框架和高延迟嵌入推理服务器，这种"基础设施膨胀"阻碍了边缘计算、隔离环境和隐私敏感应用的发展，需要更轻量、本地化的解决方案。

Method: 提出RAGdb单体架构，整合自动化多模态摄取、ONNX特征提取和混合向量检索到单个SQLite容器。设计确定性混合评分函数（HSF），结合亚线性TF-IDF向量化和精确子串增强，查询时无需GPU推理。

Result: 在Intel i7-1165G7消费级笔记本上测试：实体检索Recall@1达到100%；增量更新效率比冷启动提升31.6倍；磁盘占用比标准Docker RAG栈减少约99.5%，实现"单文件知识容器"。

Conclusion: RAGdb证明了"单文件知识容器"作为去中心化、本地优先AI可行原型的有效性，为边缘计算、隐私敏感和资源受限环境提供了高效、轻量的RAG解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, heavy deep learning frameworks (e.g., PyTorch, CUDA), and high-latency embedding inference servers. This ``infrastructure bloat'' creates a significant barrier to entry for edge computing, air-gapped environments, and privacy-constrained applications where data sovereignty is paramount.
  This paper introduces RAGdb, a novel monolithic architecture that consolidates automated multimodal ingestion, ONNX-based extraction, and hybrid vector retrieval into a single, portable SQLite container. We propose a deterministic Hybrid Scoring Function (HSF) that combines sublinear TF-IDF vectorization with exact substring boosting, eliminating the need for GPU inference at query time. Experimental evaluation on an Intel i7-1165G7 consumer laptop demonstrates that RAGdb achieves 100\% Recall@1 for entity retrieval and an ingestion efficiency gain of 31.6x during incremental updates compared to cold starts. Furthermore, the system reduces disk footprint by approximately 99.5\% compared to standard Docker-based RAG stacks, establishing the ``Single-File Knowledge Container'' as a viable primitive for decentralized, local-first AI.
  Keywords: Edge AI, Retrieval-Augmented Generation, Vector Search, Green AI, Serverless Architecture, Knowledge Graphs, Efficient Computing.

</details>


### [16] [Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications](https://arxiv.org/abs/2602.22219)
*Teri Rumble,Zbyněk Gazdík,Javad Zarrin,Jagdeep Ahluwalia*

Main category: cs.IR

TL;DR: 该研究提出并评估了针对知识图谱自然语言查询的多重检索-重排序RAG管道，在电子商务场景中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在非结构化文本上表现良好，但在结构化知识图谱应用中面临挑战：需要扩展检索范围到连接图谱，并在响应生成中保持上下文关系。跨编码器提高了检索精度，但其与结构化数据的集成尚未充分探索。解决这些挑战对于开发生产环境中的领域特定助手至关重要。

Method: 研究设计和比较评估了针对知识图谱自然语言查询的多重检索-重排序管道，使用STaRK半结构化知识库（SKB）这一生产规模的电子商务数据集，评估了针对语言查询优化的多种RAG管道配置。

Result: 实验结果显示相对于已发布基准有显著改进：Hit@1提高了20.4%，平均倒数排名（MRR）提高了14.5%。

Conclusion: 研究为将领域特定半结构化知识库集成到生成系统中建立了实用框架，为生产就绪RAG系统的部署提供了可操作的见解，其影响超越了电子商务领域，适用于其他需要从结构化知识库中检索信息的领域。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integrating external knowledge sources with generative models. Although RAG demonstrates strong performance on unstructured text, its application to structured knowledge graphs presents challenges: scaling retrieval across connected graphs and preserving contextual relationships during response generation. Cross-encoders refine retrieval precision, yet their integration with structured data remains underexplored. Addressing these challenges is crucial for developing domain-specific assistants that operate in production environments. This study presents the design and comparative evaluation of multiple Retriever-Reranker pipelines for knowledge graph natural language queries in e-Commerce contexts. Using the STaRK Semi-structured Knowledge Base (SKB), a production-scale e-Commerce dataset, we evaluate multiple RAG pipeline configurations optimized for language queries. Experimental results demonstrate substantial improvements over published benchmarks, achieving 20.4% higher Hit@1 and 14.5% higher Mean Reciprocal Rank (MRR). These findings establish a practical framework for integrating domain-specific SKBs into generative systems. Our contributions provide actionable insights for the deployment of production-ready RAG systems, with implications that extend beyond e-Commerce to other domains that require information retrieval from structured knowledge bases.

</details>


### [17] [SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG](https://arxiv.org/abs/2602.22225)
*Xuechen Zhang,Koustava Goswami,Samet Oymak,Jiasi Chen,Nedim Lipka*

Main category: cs.IR

TL;DR: SmartChunk提出了一种查询自适应的检索增强生成框架，通过动态规划最优块抽象级别和轻量压缩模块，解决了传统RAG中静态分块和扁平检索的局限性，在多个QA基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成(RAG)系统存在静态分块和扁平检索的局限性：文档被预分割为固定大小的短块，检索质量对块大小高度敏感，容易引入不相关或误导性块的噪声，且在大规模语料库上扩展性差。需要一种能够根据查询自适应调整检索粒度的方法。

Method: SmartChunk框架包含两个核心组件：(1) 规划器：为每个查询预测最优的块抽象级别；(2) 轻量压缩模块：无需重复摘要即可生成高级块嵌入。采用新颖的强化学习方案STITCH，使规划器能够推理块抽象级别。框架支持动态调整检索粒度，平衡准确性与效率。

Result: 在五个QA基准测试和一个域外数据集上的评估表明，SmartChunk优于最先进的RAG基线方法，同时降低了成本。进一步分析显示，在大规模语料库上具有良好的扩展性，在域外数据集上获得一致的性能提升。

Conclusion: SmartChunk作为一种自适应检索的通用框架，通过查询自适应的块检索策略有效解决了传统RAG的局限性，在准确性、效率和扩展性方面均表现出色，为长文档问答提供了更稳健的解决方案。

Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.

</details>


### [18] [SEGB: Self-Evolved Generative Bidding with Local Autoregressive Diffusion](https://arxiv.org/abs/2602.22226)
*Yulong Gao,Wan Jiang,Mingzhe Cao,Xuepu Wang,Zeyu Pan,Haonan Yang,Ye Liu,Xin Yang*

Main category: cs.IR

TL;DR: 提出SEGB框架，通过合成短期未来状态引导出价决策，并进行价值引导的策略精炼，实现完全离线的自我进化竞价策略


<details>
  <summary>Details</summary>
Motivation: 现有离线训练的生成式竞价策略缺乏对动态市场的短期预见能力，通常依赖模拟器或外部专家进行后训练改进，存在关键局限性

Method: SEGB框架包含两个核心：1)合成合理的短期未来状态来指导每次出价，为智能体提供动态预见能力；2)执行价值引导的策略精炼，无需外部干预迭代发现更优策略

Result: 在AuctionNet基准测试和大规模A/B测试中验证了SEGB显著优于最先进的基线方法；大规模在线部署实现了目标成本+10.19%的增长，证明了其商业价值

Conclusion: SEGB通过先进的规划和进化范式，实现了仅从静态数据中进行稳健策略改进的自包含方法，为在线广告自动竞价提供了有效的解决方案

Abstract: In the realm of online advertising, automated bidding has become a pivotal tool, enabling advertisers to efficiently capture impression opportunities in real-time. Recently, generative auto-bidding has shown significant promise, offering innovative solutions for effective ad optimization. However, existing offline-trained generative policies lack the near-term foresight required for dynamic markets and usually depend on simulators or external experts for post-training improvement. To overcome these critical limitations, we propose Self-Evolved Generative Bidding (SEGB), a framework that plans proactively and refines itself entirely offline. SEGB first synthesizes plausible short-horizon future states to guide each bid, providing the agent with crucial, dynamic foresight. Crucially, it then performs value-guided policy refinement to iteratively discover superior strategies without any external intervention. This self-contained approach uniquely enables robust policy improvement from static data alone. Experiments on the AuctionNet benchmark and a large-scale A/B test validate our approach, demonstrating that SEGB significantly outperforms state-of-the-art baselines. In a large-scale online deployment, it delivered substantial business value, achieving a +10.19% increase in target cost, proving the effectiveness of our advanced planning and evolution paradigm.

</details>


### [19] [RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval](https://arxiv.org/abs/2602.22278)
*Dawei Su,Dongsheng Wang*

Main category: cs.IR

TL;DR: RetLLM是一个无需训练和数据的多模态信息检索框架，通过直接提示多模态大语言模型预测检索分数，采用粗筛-精排两阶段流程，并引入视觉增强模块提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的MMIR方法存在预训练不一致问题，且需要大量数据集进行微调。作者希望开发一个无需训练和数据的框架，直接利用MLLMs的内在多模态推理能力进行信息检索。

Method: RetLLM将MMIR定义为相似度分数生成任务，采用粗筛-精排两阶段流程：1) 粗筛阶段使用top-k过滤策略构建高质量候选池；2) 精排阶段将查询和候选同时输入MLLMs预测检索分数；3) 引入视觉增强模块帮助MLLMs重新拾取被遗忘的视觉信息。

Result: 在MMIR基准测试上的广泛实验表明，RetLLM超越了经过微调的模型性能。消融研究进一步验证了每个组件的有效性。

Conclusion: 多模态大语言模型无需任何训练即可实现强大的MMIR性能，展示了其在简单、可扩展框架中的内在多模态推理能力。RetLLM为训练和数据自由的MMIR提供了新思路。

Abstract: Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM

</details>


### [20] [TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.22521)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出TFPS方法，通过时间过滤增强构建高质量正样本集，提升隐式反馈协同过滤推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有负采样策略主要优化负样本而忽略正样本探索；现有去噪方法忽略时间信息；现有序列方法忽略时间间隔信息，难以准确捕捉用户当前偏好

Method: 1) 基于交互时间间隔设计时间衰减模型，将原始图转换为加权用户-物品二分图；2) 基于预定义过滤操作对加权图进行分层；3) 设计层增强策略为分层子图构建高质量正样本集

Result: 在三个真实数据集上的实验证明了方法的有效性；TFPS可集成到各种隐式CF推荐器或负采样方法中提升性能；理论分析解释了TFPS为何能改进Recall@k和NDCG@k

Conclusion: TFPS从数据角度提出时间过滤增强方法，有效构建高质量正样本集，解决了现有方法忽略时间间隔信息的问题，能更准确捕捉用户当前偏好

Abstract: The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.

</details>


### [21] [Towards Dynamic Dense Retrieval with Routing Strategy](https://arxiv.org/abs/2602.22547)
*Zhan Su,Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Bingbing Wen,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出动态稠密检索（DDR）方法，使用前缀调优作为领域专用模块，通过动态路由策略组合模块，实现高效灵活的领域适应，仅需2%训练参数即可超越传统稠密检索。


<details>
  <summary>Details</summary>
Motivation: 传统稠密检索（DR）应用范式存在两个主要限制：（1）当训练数据有限时难以适应新领域；（2）模型更新时需要从头训练，成本高昂。特别是在需要频繁更新的场景中，这种范式代价过高。

Method: 提出动态稠密检索（DDR）方法，使用前缀调优作为特定领域的专用模块。这些模块可以通过动态路由策略进行组合，实现检索部分的高度灵活领域适应。

Result: 在六个零样本下游任务上的广泛评估表明，该方法仅使用2%的训练参数就能超越传统稠密检索（DR）性能，为信息检索中实现更灵活的稠密检索铺平了道路。

Conclusion: DDR是一种有前景的未来方向，能够将稠密检索灵活应用于各种任务，解决了传统方法在领域适应和模型更新方面的局限性。

Abstract: The \textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.
  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \textit{dynamic dense retrieval} (DDR). DDR uses \textit{prefix tuning} as a \textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.

</details>


### [22] [Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking](https://arxiv.org/abs/2602.22591)
*Haodong Chen,Shengyao Zhuang,Zheng Yao,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 本文系统评估了LLM文档重排中的生成、似然和内部注意力机制，发现了注意力信号在Transformer层中的钟形分布规律，提出了Selective-ICR策略，在保持效果的同时将推理延迟降低30%-50%，小模型即可超越传统生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有零样本文档重排方法主要依赖生成评分或输出logits，存在推理延迟和结果一致性的瓶颈。虽然In-Context Re-ranking (ICR)作为O(1)替代方法被提出，但现有ICR方法简单聚合所有层的信号，未探索层间贡献差异及其在不同架构中的一致性，也缺乏对内部注意力与传统机制的系统比较。

Method: 1) 正交评估生成、似然和内部注意力机制在多种重排框架下的表现；2) 识别Transformer层中相关性信号的"钟形曲线"分布规律；3) 基于此提出Selective-ICR策略，选择性地利用关键层的注意力信号而非简单聚合所有层；4) 在BRIGHT推理密集型基准上进行评估。

Result: 1) 发现注意力信号在Transformer层中呈现一致的钟形分布；2) Selective-ICR将推理延迟降低30%-50%且不损失效果；3) 在BRIGHT基准上，零样本8B模型匹配14B强化学习重排器的性能，0.6B模型甚至超越最先进的生成方法；4) 高质量上下文注意力信号显著减少了对模型缩放和强化学习的依赖。

Conclusion: 本文重新定义了基于LLM的重排效率-效果边界，揭示了内部信号在复杂推理排序任务中的潜在价值。Selective-ICR策略通过利用注意力信号的层间分布规律，实现了高效且有效的文档重排，为小模型在复杂任务上的应用提供了新思路。

Abstract: Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.
  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal "bell-curve" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.

</details>


### [23] [Fine-grained Semantics Integration for Large Language Model-based Recommendation](https://arxiv.org/abs/2602.22632)
*Jiawen Feng,Xiaoyu Kong,Leheng Sheng,Bin Wu,Chao Yi,Feifang Yang,Xiang-Rong Sheng,Han Zhu,Xiang Wang,Jiancan Wu,Xiangnan He*

Main category: cs.IR

TL;DR: TS-Rec通过语义感知嵌入初始化和令牌级语义对齐，解决LLM推荐系统中语义标识符空间建模的两个关键挑战，显著提升生成式推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的生成式推荐系统面临两个根本挑战：1）语义标识符（SID）令牌随机初始化，切断了与预训练语言空间的语义联系；2）现有SFT对齐任务主要关注项目级优化，忽略了SID序列中单个令牌的语义。

Method: TS-Rec包含两个核心组件：1）语义感知嵌入初始化（SA-Init），通过教师模型提取关键词的预训练嵌入均值池化来初始化SID令牌嵌入；2）令牌级语义对齐（TS-Align），将SID序列中的单个令牌与对应项目簇的共享语义对齐。

Result: 在两个真实世界基准测试上的广泛实验表明，TS-Rec在所有标准指标上持续优于传统和生成式基线方法，证明了细粒度语义信息整合能显著提升基于LLM的生成式推荐器性能。

Conclusion: 整合细粒度语义信息能显著增强基于LLM的生成式推荐器性能，TS-Rec通过解决SID空间建模的两个关键挑战，为LLM推荐系统提供了有效的语义增强方案。

Abstract: Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.

</details>


### [24] [Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators](https://arxiv.org/abs/2602.22647)
*Zhengyang Su,Isay Katsman,Yueqi Wang,Ruining He,Lukasz Heldt,Raghunandan Keshavan,Shao-Chuan Wang,Xinyang Yi,Mingyan Gao,Onkar Dalal,Lichan Hong,Ed Chi,Ningren Han*

Main category: cs.IR

TL;DR: STATIC是一种针对TPU/GPU优化的高效约束解码技术，通过将前缀树扁平化为CSR稀疏矩阵，将不规则树遍历转换为向量化稀疏矩阵运算，显著提升生成式检索在工业推荐系统中的性能。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统需要将输出空间限制在基于业务逻辑的约束子集（如内容新鲜度或产品类别），但标准自回归解码无法原生支持这种约束。现有基于前缀树的约束解码方法在硬件加速器上会产生严重的延迟惩罚。

Method: 提出STATIC方法，将前缀树扁平化为静态压缩稀疏行(CSR)矩阵，将不规则树遍历转换为完全向量化的稀疏矩阵运算，从而在TPU/GPU上实现高效计算。

Result: 在大型工业视频推荐平台部署中，STATIC产生显著产品指标影响，延迟开销极低（每步0.033ms，占推理时间0.25%），相比CPU trie实现获得948倍加速，相比硬件加速二分搜索基线获得47-1033倍加速。在学术基准测试中，STATIC显著改善生成式检索的冷启动性能。

Conclusion: STATIC实现了首个生产规模的严格约束生成式检索部署，通过将树遍历转换为稀疏矩阵运算，在硬件加速器上实现了高效、可扩展的约束解码，为工业级LLM推荐系统提供了实用的解决方案。

Abstract: Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.

</details>


### [25] [Generative Recommendation for Large-Scale Advertising](https://arxiv.org/abs/2602.22732)
*Ben Xue,Dan Liu,Lixiang Wang,Mingjie Sun,Peng Wang,Pengfei Zhang,Shaoyun Shi,Tianyu Xu,Yunhao Sha,Zhiqiang Liu,Bo Kong,Bo Wang,Hang Yang,Jieting Xue,Junhao Wang,Shengyu Wang,Shuping Hui,Wencai Ye,Xiao Lin,Yongzhi Li,Yuhang Chen,Zhihui Yin,Quan Chen,Shiyang Wen,Wenjin Wu,Han Li,Guorui Zhou,Changcheng Li,Peng Jiang*

Main category: cs.IR

TL;DR: GR4AD是一个面向广告场景的生产级生成式推荐系统，通过统一广告语义ID、惰性自回归解码器、价值感知学习和排名引导的强化学习优化，在快手广告系统中实现了4.2%的广告收入提升。


<details>
  <summary>Details</summary>
Motivation: 在大规模广告系统中部署实时生成式推荐面临独特挑战，需要超越传统大语言模型训练和服务范式的设计。现有DLRM模型在生成式推荐场景下存在效率、扩展性和业务价值对齐方面的不足。

Method: 1. UA-SID统一广告语义ID捕获复杂业务信息；2. LazyAR惰性自回归解码器放松层间依赖，降低短序列多候选生成的推理成本；3. VSL价值感知监督学习；4. RSPO排名引导的软最大偏好优化，基于列表级指标的强化学习算法；5. 动态束搜索服务，根据生成层级和在线负载自适应调整束宽。

Result: 大规模在线A/B测试显示，相比现有DLRM基准，广告收入提升达4.2%。模型扩展和推理时扩展均带来持续收益。系统已在快手广告系统全面部署，服务超过4亿用户，实现高吞吐实时服务。

Conclusion: GR4AD通过架构、学习和服务的协同设计，成功解决了大规模广告系统中生成式推荐的生产部署挑战，在保持效果的同时显著降低推理成本，实现了业务价值的有效对齐和系统扩展。

Abstract: Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.

</details>


### [26] [SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress](https://arxiv.org/abs/2602.22913)
*Yang Yu,Lei Kou,Huaikuan Yi,Bin Chen,Yayu Cao,Lei Shen,Chao Zhang,Bing Wang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: SIGMA是阿里速卖通开发的语义基础指令驱动生成式多任务推荐系统，通过统一语义空间、混合项目标记化、多任务指令微调和自适应概率融合机制，实现多样化推荐需求。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法大多局限于交互驱动的下一项预测范式，难以快速适应趋势变化，也无法满足现实场景中多样化的推荐任务和业务特定需求。

Method: 1) 通过统一潜在空间将项目实体基础于通用语义，捕捉语义和协作关系；2) 开发混合项目标记化方法进行精确建模和高效生成；3) 构建大规模多任务SFT数据集支持指令跟随；4) 设计三步项目生成流程，集成自适应概率融合机制校准输出分布。

Result: 广泛的离线实验和在线A/B测试证明了SIGMA的有效性，能够满足各种推荐需求，在推荐准确性和多样性方面表现优异。

Conclusion: SIGMA通过语义基础和指令驱动的方法，成功解决了传统生成式推荐的局限性，实现了对多样化推荐任务的适应性，为实际业务场景提供了有效的解决方案。

Abstract: With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.

</details>


### [27] [Sequential Regression for Continuous Value Prediction using Residual Quantization](https://arxiv.org/abs/2602.23012)
*Runpeng Cui,Zhipeng Sun,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出基于残差量化的序列学习框架，通过从粗到细的递归预测有序量化码来表示连续值，在推荐系统的连续值预测任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的连续值预测（如观看时长、GMV预测）面临数据分布复杂、长尾的挑战。现有生成方法依赖刚性参数分布假设，当假设与现实数据不匹配时性能受限：简化形式无法建模现实复杂性，复杂假设则存在可扩展性和泛化性问题。

Method: 提出残差量化（RQ）序列学习框架：将目标连续值表示为有序量化码的和，从粗到细粒度递归预测，量化误差逐渐减小。引入表示学习目标，使RQ码嵌入空间与目标值的序结构对齐，从而学习量化码的连续表示，进一步提高预测精度。

Result: 在LTV和观看时长预测的公共基准测试，以及工业级短视频推荐平台的GMV预测大规模在线实验中，该方法均优于最先进方法，并在推荐系统多样连续值预测任务中展现出强泛化能力。

Conclusion: 残差量化序列学习框架有效解决了推荐系统连续值预测的挑战，通过从粗到细的递归预测和量化码表示学习，超越了依赖刚性分布假设的传统方法，在多个任务中表现出优越性能和泛化能力。

Abstract: Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.

</details>


### [28] [MoDora: Tree-Based Semi-Structured Document Analysis System](https://arxiv.org/abs/2602.23061)
*Bangrui Xu,Qihang Yao,Zirui Tang,Xuanhe Zhou,Yeye He,Shihan Yu,Qianqian Xu,Bin Wang,Guoliang Li,Conghui He,Fan Wu*

Main category: cs.IR

TL;DR: MoDora：基于LLM的半结构化文档分析系统，通过布局感知组件构建、层次化组织树和问题类型感知检索策略，显著提升文档问答准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理半结构化文档问答时面临三大技术挑战：OCR提取元素碎片化且缺乏语义上下文；缺乏有效表示来捕捉文档层次结构和布局差异；需要跨多个区域检索对齐分散信息

Method: 1. 采用局部对齐聚合策略将OCR解析元素转换为布局感知组件，并对含层次标题或非文本元素的组件进行类型特定信息提取；2. 设计组件关联树(CCTree)层次化组织组件，通过自底向上级联摘要过程显式建模组件间关系和布局差异；3. 提出问题类型感知检索策略，支持基于布局的网格划分位置检索和LLM引导的语义检索剪枝

Result: 实验表明MoDora在准确率上比基线方法提升5.97%-61.07%

Conclusion: MoDora通过创新的组件表示、层次化组织和智能检索策略，有效解决了半结构化文档问答的关键挑战，显著提升了分析性能

Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.
  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.

</details>


### [29] [MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System](https://arxiv.org/abs/2602.23105)
*Yusheng Huang,Pengbo Xu,Shen Wang,Changxin Lao,Jiangxia Cao,Shuang Wen,Shuang Yang,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: MaRI是一个基于矩阵重参数化的无损推理加速框架，通过消除特征融合矩阵乘法中的用户侧计算冗余来加速推荐排序模型，不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有排序模型加速方法（结构轻量化或知识蒸馏）通常会导致精度下降，而通过优化特征融合矩阵乘法、特别是结构重参数化实现无损加速的研究不足。观察到特征融合矩阵乘法中用户侧计算存在冗余，需要消除这种冗余。

Method: 提出MaRI（矩阵重参数化推理框架），采用结构重参数化思想来缓解用户侧计算冗余。作为现有技术的补充方法，通过优化特征融合矩阵乘法实现无损加速。

Result: 论文声称MaRI能够加速排序模型推理而不损失任何精度，解决了现有加速方法导致的精度下降问题。

Conclusion: MaRI为大规模推荐系统中的排序模型提供了一种无损加速的补充方法，通过结构重参数化优化特征融合矩阵乘法，在满足在线服务延迟要求的同时保持模型精度。

Abstract: Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.

</details>


### [30] [From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2602.23132)
*Ruochen Yang,Xiaodong Li,Jiawei Sheng,Jiangxia Cao,Xinkui Lin,Shen Wang,Shuang Yang,Zhaojie Liu,Tingwen Liu*

Main category: cs.IR

TL;DR: FatsMB是一个基于扩散模型的多行为序列推荐框架，通过从行为无关到行为特定的潜在空间偏好生成，实现多样化和准确的推荐。


<details>
  <summary>Details</summary>
Motivation: 现有多行为序列推荐方法存在两个主要问题：1）忽略了用户潜在偏好对决策的影响；2）基于偏好评分的判别式范式无法有效捕捉从低熵行为到高熵项目的不确定性，导致推荐效率和多样性不足。

Method: 提出FatsMB框架：1）设计多行为自编码器（MBAE）构建统一的用户潜在偏好空间；2）采用行为感知RoPE（BaRoPE）进行多信息融合；3）在潜在空间中进行目标行为特定的偏好转移；4）引入多条件引导层归一化（MCGLN）进行去噪。

Result: 在真实世界数据集上的大量实验证明了模型的有效性。

Conclusion: FatsMB通过扩散模型在潜在空间中实现从行为无关到行为特定的偏好生成，能够提供高效且多样化的多行为序列推荐。

Abstract: Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.

</details>


### [31] [Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments](https://arxiv.org/abs/2602.23234)
*Evangelia Christakopoulou,Vivekkumar Patel,Hemanth Velaga,Sandip Gaikwad*

Main category: cs.IR

TL;DR: 该论文通过使用专门微调的LLM生成数百万文本相关性标签，解决了商业搜索系统中文本相关性标签稀缺的问题，并将这些标签集成到生产排序器中，在行为相关性和文本相关性方面同时实现了离线改进，最终通过A/B测试验证了转化率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 商业搜索系统需要同时优化行为相关性（用户点击/下载倾向）和文本相关性（结果与查询的语义匹配），但面临文本相关性专家标注标签稀缺的问题，而行为相关性标签则相对丰富。

Method: 1. 系统评估LLM配置，发现专门微调的模型在提供高质量文本相关性标签方面显著优于更大的预训练模型；2. 使用最优模型作为"力量倍增器"生成数百万文本相关性标签；3. 将这些标签集成到生产排序器中，增强文本相关性信号。

Result: 1. 离线评估显示NDCG在行为相关性和文本相关性方面同时提升，实现了帕累托前沿的外移；2. 全球A/B测试显示App Store排序器转化率显著提升+0.24%；3. 尾部查询表现提升最显著，因为新标签在缺乏可靠行为相关性信号时提供了稳健的文本相关性信号。

Conclusion: 使用专门微调的LLM生成大规模文本相关性标签是解决标签稀缺问题的有效方法，能够同时提升行为相关性和文本相关性，特别是在尾部查询中效果显著，为商业搜索系统的相关性优化提供了可扩展的解决方案。

Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.

</details>
