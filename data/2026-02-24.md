<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 53]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 47]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ReportLogic: Evaluating Logical Quality in Deep Research Reports](https://arxiv.org/abs/2602.18446)
*Jujia Zhao,Zhaoxin Huan,Zihan Wang,Xiaolu Zhang,Jun Zhou,Suzan Verberne,Zhaochun Ren*

Main category: cs.CL

TL;DR: ReportLogic：一个评估LLM生成报告逻辑质量的基准，通过可审计性视角量化报告级逻辑质量，包含宏观逻辑、阐述逻辑和结构逻辑三个层次，并训练了开源LogicJudge进行可扩展评估。


<details>
  <summary>Details</summary>
Motivation: 用户越来越依赖大语言模型进行深度研究，需要LLM将多样来源合成为结构化报告以支持理解和行动。然而，当前评估框架大多忽视了报告的逻辑质量要求——即报告的主张和论点是否有明确支持，能否作为下游使用的可信基础，而不仅仅是表面流畅或信息丰富。

Method: 1. 引入ReportLogic基准，通过读者中心的可审计性视角量化报告级逻辑质量；2. 采用分层分类法：宏观逻辑（可追踪主题报告结构）、阐述逻辑（理解进展所需上下文）、结构逻辑（通过明确主张-支持关系验证结论）；3. 构建人工标注的基于量规的数据集；4. 训练开源LogicJudge进行可扩展评估；5. 通过对抗性攻击评估判断器的鲁棒性。

Result: 1. 创建了ReportLogic基准和数据集；2. 训练了开源LogicJudge用于可扩展的逻辑质量评估；3. 发现现成的LLM判断器经常受到表面线索（如冗长）的影响，推理模式可能掩盖断裂的支持关系；4. 通过对抗性攻击验证了判断器的鲁棒性。

Conclusion: 研究结果为构建更鲁棒的逻辑评估器和提高LLM生成报告的逻辑可靠性提供了可操作的指导。ReportLogic基准填补了当前评估框架在逻辑质量方面的空白，强调了报告级逻辑质量对深度研究应用的重要性。

Abstract: Users increasingly rely on Large Language Models (LLMs) for Deep Research, using them to synthesize diverse sources into structured reports that support understanding and action. In this context, the practical reliability of such reports hinges on logical quality: whether the report's claims and arguments are explicitly supported and can be trusted as a basis for downstream use, rather than merely appearing fluent or informative. However, current evaluation frameworks largely overlook this requirement. To bridge this gap, we introduce ReportLogic, a benchmark that quantifies report-level logical quality through a reader-centric lens of auditability. Specifically, ReportLogic adopts a hierarchical taxonomy that evaluates whether readers can (1) trace an on-topic report structure with a unified analytical arc (Macro-Logic), (2) understand the progression with necessary context (Expositional-Logic), and (3) verify conclusions via explicit claim--support (Structural-Logic). Based on this taxonomy, we construct a human-annotated rubric-guided dataset and train an open-source LogicJudge for scalable evaluation. We further evaluate judge robustness via adversarial attacks, showing that off-the-shelf LLM judges are frequently influenced by superficial cues (e.g., verbosity), and reasoning modes can mask broken support relations. Overall, our results provide actionable guidance for building more robust logic evaluators and improving the logical reliability of LLM-generated reports.

</details>


### [2] [ConfSpec: Efficient Step-Level Speculative Reasoning via Confidence-Gated Verification](https://arxiv.org/abs/2602.18447)
*Siran Liu,Cyril Y. He*

Main category: cs.CL

TL;DR: ConfSpec：基于置信度门控的级联验证框架，通过小型草稿模型进行步骤级验证，在保持目标模型精度的同时实现高达2.24倍的推理加速


<details>
  <summary>Details</summary>
Motivation: 链式思维推理显著提升大语言模型在复杂任务上的性能，但生成长推理轨迹导致高推理延迟。现有步骤级推测推理方法在准确性、推理速度和资源效率之间存在长期权衡

Method: 提出ConfSpec框架，核心洞察是生成与验证的不对称性：生成正确推理步骤需要大模型能力，而步骤级验证是受限判别任务，小型草稿模型在其能力范围内具有良好校准性。采用置信度门控级联验证，高置信度草稿决策直接接受，不确定情况选择性升级到大型目标模型

Result: 在多样化工作负载评估中，ConfSpec实现高达2.24倍的端到端加速，同时匹配目标模型精度。方法无需外部评判模型，且与词元级推测解码正交，可实现进一步乘法加速

Conclusion: ConfSpec解决了步骤级推测推理中准确性、速度和资源效率的长期权衡，通过利用生成与验证的不对称性，实现高效推理加速而不牺牲精度

Abstract: Chain-of-Thought reasoning significantly improves the performance of large language models on complex tasks, but incurs high inference latency due to long generation traces. Step-level speculative reasoning aims to mitigate this cost, yet existing approaches face a long-standing trade-off among accuracy, inference speed, and resource efficiency. We propose ConfSpec, a confidence-gated cascaded verification framework that resolves this trade-off. Our key insight is an asymmetry between generation and verification: while generating a correct reasoning step requires substantial model capacity, step-level verification is a constrained discriminative task for which small draft models are well-calibrated within their competence range, enabling high-confidence draft decisions to be accepted directly while selectively escalating uncertain cases to the large target model. Evaluation across diverse workloads shows that ConfSpec achieves up to 2.24$\times$ end-to-end speedups while matching target-model accuracy. Our method requires no external judge models and is orthogonal to token-level speculative decoding, enabling further multiplicative acceleration.

</details>


### [3] [Prompt Optimization Via Diffusion Language Models](https://arxiv.org/abs/2602.18449)
*Shiyu Wang,Haolin Chen,Liangwei Yang,Jielin Qiu,Rithesh Murthy,Ming Zhu,Zixiang Chen,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

TL;DR: 提出基于扩散模型的提示优化框架，利用扩散语言模型通过掩码去噪迭代优化系统提示，无需梯度访问或修改下游语言模型


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法通常需要梯度访问或模型修改，限制了其通用性和可扩展性。需要一种模型无关、无需修改目标模型的方法来优化系统提示

Method: 基于扩散语言模型的框架，通过条件化交互轨迹（用户查询、模型响应、可选反馈）进行掩码去噪，实现灵活的跨度级提示更新

Result: 在多个基准测试（τ-bench、SST-2、SST-5）上，DLM优化的提示能持续提升冻结目标LLM（如GPT-4o-mini）的性能，适中的扩散步数在精炼质量和稳定性间达到最佳平衡

Conclusion: 扩散基提示优化是一种通用、模型无关且可扩展的方法，通过迭代提示精炼增强LLM性能，为无需修改目标模型的提示优化提供了新途径

Abstract: We propose a diffusion-based framework for prompt optimization that leverages Diffusion Language Models (DLMs) to iteratively refine system prompts through masked denoising. By conditioning on interaction traces, including user queries, model responses, and optional feedback, our method enables flexible, span-level prompt updates without requiring gradient access or modifying the downstream language model. Across diverse benchmarks (e.g., $τ$-bench, SST-2, SST-5), DLM-optimized prompts consistently improve the performance of a frozen target LLM (e.g., GPT-4o-mini). We further show that moderate diffusion step counts provide the best balance between refinement quality and stability. These results highlight diffusion-based prompt optimization as a general, model-agnostic, and scalable approach for enhancing LLM performance through iterative prompt refinement.

</details>


### [4] [Asymptotic Semantic Collapse in Hierarchical Optimization](https://arxiv.org/abs/2602.18450)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: 研究多智能体语言系统中语义崩溃现象：在层次优化中，主导锚节点的语义惯性导致周边智能体语义逐渐对齐，形成全局共识，信息自由度消失。


<details>
  <summary>Details</summary>
Motivation: 多智能体语言系统存在一种失效模式：共享主导语境逐渐吸收个体语义，导致智能体行为趋于一致。本研究旨在分析这种"渐近语义崩溃"现象，探讨层次优化中语义对齐的机制和后果。

Method: 将语义状态建模为黎曼流形上的点，分析诱导投影动力学。在封闭语言环境中，设置具有无限语义惯性的主导锚节点，研究其与周边智能体节点的重复交互。结合信息论量和微分几何结构，分析从原子表示到完全纠缠表示的转变过程。

Result: 理论分析表明：1) 极限语义配置对优化历史不敏感，梯度更新和随机更新收敛到相同拓扑终点；2) 语境依赖程度控制信息内容，完全纠缠表示迫使节点熵（自由度）在极限中消失。实验在RWKV-7 13B GGUF检查点上验证，报告零哈希碰撞，贪婪解码平均依从度0.50，随机解码0.531，最终Jaccard相似度分别为0.295和0.224。

Conclusion: 渐近语义崩溃揭示了多智能体语言系统中不可变的共识规则，将智能体约束在共享语义语法中。该理论连接了信息论量与微分几何结构，为理解语义对齐和共识形成提供了框架。

Abstract: Multi-agent language systems can exhibit a failure mode where a shared dominant context progressively absorbs individual semantics, yielding near-uniform behavior across agents. We study this effect under the name Asymptotic Semantic Collapse in Hierarchical Optimization. In a closed linguistic setting with a Dominant Anchor Node whose semantic state has effectively infinite inertia, we show that repeated interactions with Peripheral Agent Nodes drive an asymptotic alignment that minimizes a global loss. We model semantic states as points on a Riemannian manifold and analyze the induced projection dynamics. Two consequences follow. First, the limiting semantic configuration is insensitive to the optimization history: both smooth gradient-style updates and stochastic noisy updates converge to the same topological endpoint, establishing path independence at convergence. Second, the degree of context dependence controls information content: moving from atomic (independent) representations to fully entangled (context-bound) representations forces the node entropy, interpreted as available degrees of freedom, to vanish in the limit. The theory connects information-theoretic quantities with differential-geometric structure and suggests an interpretation as an immutable consensus rule that constrains agents to a shared semantic grammar. A lightweight dataset-free benchmark on an RWKV-7 13B GGUF checkpoint complements the analysis, reporting zero hash collisions, mean compliance of 0.50 under greedy decoding and 0.531 under stochastic decoding, and final Jaccard-to-anchor similarity values of 0.295 and 0.224, respectively.

</details>


### [5] [The Million-Label NER: Breaking Scale Barriers with GLiNER bi-encoder](https://arxiv.org/abs/2602.18487)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov*

Main category: cs.CL

TL;DR: GLiNER-bi-Encoder是一种新型NER架构，通过双编码器设计解决原始GLiNER的二次复杂度问题，实现零样本灵活性与工业级效率的平衡，支持数千至数百万实体类型的识别。


<details>
  <summary>Details</summary>
Motivation: 原始GLiNER框架虽然具有强大的泛化能力，但其联合编码方法在实体标签数量增加时存在二次复杂度问题，限制了工业规模应用。需要一种既能保持零样本灵活性又能实现高效处理的架构。

Method: 提出双编码器设计，将过程解耦为专用标签编码器和上下文编码器，消除上下文窗口瓶颈。通过预计算标签嵌入实现高效处理，并基于此架构开发GLiNKER框架用于大规模知识库的实体链接。

Result: 在CrossNER基准测试中达到61.5%的Micro-F1分数，实现最先进的零样本性能。在1024个标签时，相比单编码器前身实现了高达130倍的吞吐量提升。

Conclusion: GLiNER-bi-Encoder成功解决了大规模实体识别中的效率瓶颈，将零样本NER扩展到工业规模应用，同时为大规模知识库实体链接提供了高效框架。

Abstract: This paper introduces GLiNER-bi-Encoder, a novel architecture for Named Entity Recognition (NER) that harmonizes zero-shot flexibility with industrial-scale efficiency. While the original GLiNER framework offers strong generalization, its joint-encoding approach suffers from quadratic complexity as the number of entity labels increases. Our proposed bi-encoder design decouples the process into a dedicated label encoder and a context encoder, effectively removing the context-window bottleneck. This architecture enables the simultaneous recognition of thousands, and potentially millions, of entity types with minimal overhead. Experimental results demonstrate state-of-the-art zero-shot performance, achieving 61.5 percent Micro-F1 on the CrossNER benchmark. Crucially, by leveraging pre-computed label embeddings, GLiNER-bi-Encoder achieves up to a 130 times throughput improvement at 1024 labels compared to its uni-encoder predecessors. Furthermore, we introduce GLiNKER, a modular framework that leverages this architecture for high-performance entity linking across massive knowledge bases such as Wikidata.

</details>


### [6] [Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering](https://arxiv.org/abs/2602.19317)
*Maryam Amirizaniani,Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: PR2是一个强化学习框架，通过集成推理和检索实现个性化问答，学习自适应检索-推理策略，在LaMP-QA基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的个性化问答方法直接使用用户查询检索个人文档，导致表面层次的个性化，无法充分整合用户背景、偏好和历史上下文。

Method: 提出PR2（个性化检索增强推理）强化学习框架，学习自适应检索-推理策略：决定何时检索、从用户档案中检索什么证据、如何将其整合到中间推理步骤中。通过个性化奖励函数优化多轮推理轨迹。

Result: 在LaMP-QA基准上使用三个LLM进行广泛实验，PR2始终优于强基线方法，在个性化QA中实现平均相对改进8.8%-12%。

Conclusion: PR2通过强化学习优化推理路径，更好地对齐用户特定偏好和上下文信号，实现了更深入有效的个性化问答。

Abstract: Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user's profile. Existing methods use the user's query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.

</details>


### [7] [Luna-2: Scalable Single-Token Evaluation with Small Language Models](https://arxiv.org/abs/2602.18583)
*Vatsal Goel,Rishon Dsouza,Nikhil Ega,Amey Ramesh Rambatla,Rob Friel,Shuai Shao,Yash Sheth*

Main category: cs.CL

TL;DR: Luna-2是一种基于小型语言模型(SLM)的实时护栏评估架构，通过轻量级LoRA/PEFT头实现数百种专用指标并行计算，在保持与前沿LLM相当或更高准确性的同时，将推理成本降低80倍以上、延迟降低20倍以上。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-judge(LLMAJ)评估方法存在速度慢、成本高、操作非确定性的问题，而实时护栏需要准确、廉价且快速的评估方案。需要一种能够在本地部署、保护隐私、优化延迟的确定性评估模型。

Method: 采用解码器专用小型语言模型(SLM)作为共享主干，每个评估指标实现为轻量级LoRA/PEFT头，支持数百种专用指标在单个GPU上并行运行。模型架构和训练方法专门设计用于可靠计算复杂任务特定的LLMAJ指标。

Result: 在内容安全和幻觉检测基准测试中，Luna-2与最先进的基于LLM的评估器准确率相当，同时推理成本降低80倍以上，延迟降低20倍以上。生产环境中保护超过1亿个AI会话，每月处理超过1000亿个token，每年节省超过3000万美元的评估成本。

Conclusion: Luna-2提供了一种高效、可扩展的实时护栏评估解决方案，通过SLM架构和LoRA/PEFT技术实现了准确性、成本效益和延迟的显著改进，适合本地部署并满足隐私保护和实时性要求。

Abstract: Real-time guardrails require evaluation that is accurate, cheap, and fast - yet today's default, LLM-as-a-judge (LLMAJ), is slow, expensive, and operationally non-deterministic due to multi-token generation. We present Luna-2, a novel architecture that leverages decoder-only small language models (SLMs) into a deterministic evaluation model to reliably compute complex task-specific LLMAJ metrics (e.g. toxicity, hallucination, tool selection quality, etc.) at an accuracy at par or higher than LLMAJ using frontier LLMs while drastically reducing the cost and latency of computation. Each metric is implemented as a lightweight LoRA/PEFT head on top of a shared SLM backbone, enabling hundreds of specialized metrics to run concurrently on a single GPU, deployable locally next to AI systems in a privacy-preserving and latency optimizing manner. Across content safety and hallucination benchmarks, Luna-2 matches the accuracy of state-of-the-art LLM-based evaluators while reducing inference cost by over 80x and latency by over 20x.
  In this paper, we outline the model architecture, training methodology and report real-world empirical results on accuracy, latency, and throughput results. In production, Luna-2 is protecting 100M+ AI sessions and processing over 100B tokens per month for our customers with eval cost savings of over $30M annually.

</details>


### [8] [DP-RFT: Learning to Generate Synthetic Text via Differentially Private Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.18633)
*Fangyuan Xu,Sihao Chen,Zinan Lin,Taiwei Shi,Sydney Graham,Pei Zhou,Mengting Wan,Alex Stein,Virginia Estellers,Charles Chen,Morris Sharp,Richard Speyer,Tadas Baltrusaitis,Jennifer Neville,Eunsol Choi,Longqi Yang*

Main category: cs.CL

TL;DR: DP-RFT：一种基于强化学习的差分隐私合成数据生成方法，通过隐私保护的最近邻投票作为奖励信号，训练LLM生成高质量合成文本，无需直接访问原始私有数据。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私合成数据生成面临两难困境：DP微调方法需要原始私有数据内容进行模型训练，而避免直接暴露的方法受限于未经微调的现成模型，其输出缺乏领域保真度。需要一种既能保护隐私又能生成高质量合成数据的方法。

Method: 提出DP-RFT（差分隐私强化微调），一种在线强化学习算法。使用DP保护的最近邻投票作为奖励信号，对LLM生成的合成样本进行评分。通过近端策略优化（PPO）迭代训练LLM，使其生成能最大化预期DP投票的合成数据。

Result: 实验表明，DP-RFT在长文本和领域特定合成数据生成（如新闻文章、会议记录、医学摘要）中，在保真度和下游效用方面缩小了私有评估与DP微调方法之间的差距，同时尊重私有数据边界。

Conclusion: DP-RFT能够在无需直接访问个体私有示例的情况下，训练LLM生成高质量的差分隐私合成文本，解决了传统方法在隐私保护与数据质量之间的权衡问题。

Abstract: Differentially private (DP) synthetic data generation plays a pivotal role in developing large language models (LLMs) on private data, where data owners cannot provide eyes-on access to individual examples. Generating DP synthetic data typically involves a difficult trade-off. On one hand, DP finetuning methods train an LLM as a synthetic data generator with formal privacy guarantees, yet it still requires the raw content of private examples for model training. However, methods that avoid direct exposure to private data are bounded by an off-the-shelf, un-finetuned model, whose outputs often lack domain fidelity. Can we train an LLM to generate high-quality synthetic text without eyes-on access to individual private examples? In this work, we introduce Differentially Private Reinforcement Fine-Tuning (DP-RFT), an online reinforcement learning algorithm for synthetic data generation with LLMs. DP-RFT leverages DP-protected nearest-neighbor votes from an eyes-off private corpus as a reward signal for on-policy synthetic samples generated by an LLM. The LLM iteratively learns to generate synthetic data to maximize the expected DP votes through Proximal Policy Optimization (PPO). We evaluate DP-RFT for long-form and domain-specific synthetic data generation, such as news articles, meeting transcripts, and medical article abstracts. Our experiments show that DP-RFT closes the gap between private evolution and DP finetuning methods in terms of the fidelity and downstream utility of the generated synthetic data, while respecting the private data boundary.

</details>


### [9] [Hyper-KGGen: A Skill-Driven Knowledge Extractor for High-Quality Knowledge Hypergraph Generation](https://arxiv.org/abs/2602.19543)
*Rizhuo Huang,Yifan Feng,Rundong Xue,Shihui Ying,Jun-Hai Yong,Chuan Shi,Shaoyi Du,Yue Gao*

Main category: cs.CL

TL;DR: Hyper-KGGen：一个技能驱动的知识超图生成框架，通过粗到细的文档分解和自适应技能获取，解决跨领域知识提取中的场景鸿沟问题，并在新基准HyperDocRED上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 知识超图能表示复杂的n元原子事实，但构建高质量超图面临"场景鸿沟"挑战：通用提取器难以适应不同领域的专业术语，现有方法无法平衡结构骨架与细粒度细节。

Method: 提出Hyper-KGGen框架：1）采用粗到细机制系统分解文档，实现从二元链接到复杂超边的全维度覆盖；2）引入自适应技能获取模块，通过基于稳定性的反馈循环将领域专业知识提炼到全局技能库中，从不稳定的跟踪和遗漏预测中诱导高质量技能。

Result: 实验表明Hyper-KGGen显著优于强基线方法，验证了在多场景设置中，演化技能比静态少样本示例能提供更丰富的指导。同时提出了严格标注的文档级知识超图提取基准HyperDocRED。

Conclusion: Hyper-KGGen通过将提取重新定义为动态技能演化过程，有效解决了知识超图构建中的场景鸿沟问题，自适应技能获取机制能够跨领域积累和利用专业知识，为复杂语义表示提供了更全面的解决方案。

Abstract: Knowledge hypergraphs surpass traditional binary knowledge graphs by encapsulating complex $n$-ary atomic facts, providing a more comprehensive paradigm for semantic representation. However, constructing high-quality hypergraphs remains challenging due to the \textit{scenario gap}: generic extractors struggle to generalize across diverse domains with specific jargon, while existing methods often fail to balance structural skeletons with fine-grained details. To bridge this gap, we propose \textbf{Hyper-KGGen}, a skill-driven framework that reformulates extraction as a dynamic skill-evolving process. First, Hyper-KGGen employs a \textit{coarse-to-fine} mechanism to systematically decompose documents, ensuring full-dimensional coverage from binary links to complex hyperedges. Crucially, it incorporates an \textit{adaptive skill acquisition} module that actively distills domain expertise into a Global Skill Library. This is achieved via a stability-based feedback loop, where extraction stability serves as a relative reward signal to induce high-quality skills from unstable traces and missed predictions. Additionally, we present \textbf{HyperDocRED}, a rigorously annotated benchmark for document-level knowledge hypergraph extraction. Experiments demonstrate that Hyper-KGGen significantly outperforms strong baselines, validating that evolved skills provide substantially richer guidance than static few-shot examples in multi-scenario settings.

</details>


### [10] [PolyFrame at MWE-2026 AdMIRe 2: When Words Are Not Enough: Multimodal Idiom Disambiguation](https://arxiv.org/abs/2602.18652)
*Nina Hosseini-Kivanani*

Main category: cs.CL

TL;DR: PolyFrame系统通过轻量级模块增强多模态模型对习语的理解，在15种语言的多模态习语消歧任务中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 多模态模型在处理习语表达时面临挑战，因为习语具有非组合性含义，这一问题在多语言环境中更加突出。需要开发有效方法来解决多模态习语消歧问题，而不必微调大型多模态编码器。

Method: 提出PolyFrame统一流水线，包含：1) 冻结的CLIP风格视觉-语言编码器和多语言BGE M3编码器；2) 仅训练轻量级模块：逻辑回归和基于LLM的句子类型预测器、习语同义词替换、干扰项感知评分、Borda排序融合；3) 从CLIP基线开始，逐步添加习语感知改写和显式句子类型分类。

Result: 从CLIP基线(英语开发集Top-1 26.7%，英语测试集6.7%)提升至英语Top-1 60.0%，葡萄牙语零样本迁移Top-1 60.0%(NDCG@5 0.822)。在多语言盲测中，Subtask A平均Top-1/NDCG为0.35/0.73，Subtask B为0.32/0.71(覆盖15种语言)。消融实验显示习语感知改写是主要性能贡献者。

Conclusion: 研究表明，无需微调大型多模态编码器，通过轻量级模块增强即可实现有效的习语消歧。习语感知改写是核心改进因素，句子类型预测和多模态融合增强了系统鲁棒性。

Abstract: Multimodal models struggle with idiomatic expressions due to their non-compositional meanings, a challenge amplified in multilingual settings. We introduced PolyFrame, our system for the MWE-2026 AdMIRe2 shared task on multimodal idiom disambiguation, featuring a unified pipeline for both image+text ranking (Subtask A) and text-only caption ranking (Subtask B). All model variants retain frozen CLIP-style vision--language encoders and the multilingual BGE M3 encoder, training only lightweight modules: a logistic regression and LLM-based sentence-type predictor, idiom synonym substitution, distractor-aware scoring, and Borda rank fusion. Starting from a CLIP baseline (26.7% Top-1 on English dev, 6.7% on English test), adding idiom-aware paraphrasing and explicit sentence-type classification increased performance to 60.0% Top-1 on English and 60.0% Top-1 (0.822 NDCG@5) in zero-shot transfer to Portuguese. On the multilingual blind test, our systems achieved average Top-1/NDCG scores of 0.35/0.73 for Subtask A and 0.32/0.71 for Subtask B across 15 languages. Ablation results highlight idiom-aware rewriting as the main contributor to performance, while sentence-type prediction and multimodal fusion enhance robustness. These findings suggest that effective idiom disambiguation is feasible without fine-tuning large multimodal encoders.

</details>


### [11] [Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework](https://arxiv.org/abs/2602.19549)
*Yibo Yan,Mingdong Ou,Yi Cao,Xin Zou,Jiahao Huo,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 提出Prune-then-Merge两阶段框架，通过自适应剪枝和分层合并解决视觉文档检索中多向量范式的高开销问题，在保持特征保真度的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 视觉文档检索(VDR)在多模态检索应用中具有重要意义，但当前最先进的多向量范式虽然性能优异，却存在过高开销的问题。现有的效率优化方法如剪枝和合并存在局限性，需要在压缩率和特征保真度之间做出困难权衡。

Method: 提出Prune-then-Merge两阶段框架：第一阶段采用自适应剪枝过滤低信息量图像块，生成精炼的高信号嵌入集合；第二阶段通过分层合并压缩这个预过滤的集合，有效总结语义内容，避免单阶段方法中噪声引起的特征稀释问题。

Result: 在29个VDR数据集上的广泛实验表明，该框架持续优于现有方法，显著扩展了近无损压缩范围，并在高压缩比下提供稳健性能。

Conclusion: Prune-then-Merge框架通过协同互补的剪枝和合并方法，有效解决了视觉文档检索中多向量范式的高开销问题，在保持特征保真度的同时实现了高效压缩。

Abstract: Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.

</details>


### [12] [From Trial by Fire To Sleep Like a Baby: A Lexicon of Anxiety Associations for 20k English Multiword Expressions](https://arxiv.org/abs/2602.18692)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 首个大规模多词表达焦虑关联词典，包含超过2万个英语多词表达，为心理学、NLP、公共卫生等领域的焦虑研究提供资源


<details>
  <summary>Details</summary>
Motivation: 目前已有词汇层面的焦虑关联研究，但缺乏针对更大文本单元（如多词表达）的焦虑关联资源。多词表达在语言中普遍存在，对理解焦虑在语言中的表达具有重要意义。

Method: 构建大规模多词表达焦虑关联词典，包含超过2万个英语多词表达。评估焦虑关联的可靠性，分析不同类型焦虑/平静相关多词表达的分布规律，研究多词表达焦虑关联的组合性（是否源于其组成词汇）。

Result: 焦虑关联具有高度可靠性。分析了不同长度（二词、三词、四词序列）多词表达中焦虑/平静关联的分布差异。研究了多词表达焦虑关联的组合性特征。

Conclusion: 该词典填补了多词表达层面焦虑关联研究的空白，为心理学、自然语言处理、公共卫生和社会科学等领域的焦虑相关研究提供了重要资源，词典已免费公开。

Abstract: Anxiety is the unease about a possible future negative outcome. In recent years, there has been growing interest in understanding how anxiety relates to our health, well-being, body, mind, and behaviour. This includes work on lexical resources for word-anxiety association. However, there is very little anxiety-related work on larger units of text such as multiword expressions (MWE). Here, we introduce the first large-scale lexicon capturing descriptive norms of anxiety associations for more than 20k English MWEs. We show that the anxiety associations are highly reliable. We use the lexicon to study prevalence of different types of anxiety- and calmness-associated MWEs; and how that varies across two-, three-, and four-word sequences. We also study the extent to which the anxiety association of MWEs is compositional (due to its constituent words). The lexicon enables a wide variety of anxiety-related research in psychology, NLP, public health, and social sciences. The lexicon is freely available: https://saifmohammad.com/worrylex.html

</details>


### [13] [Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval](https://arxiv.org/abs/2602.19961)
*Yibo Yan,Jiahao Huo,Guanbo Feng,Mingdong Ou,Yi Cao,Xin Zou,Shuliang Liu,Yuanhuiyi Lyu,Yu Huang,Jungang Li,Kening Zheng,Xu Zheng,Philip S. Yu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 本文首次对视觉文档检索（VDR）领域进行全面综述，重点关注多模态大语言模型（MLLM）时代的发展，涵盖基准测试、方法演进（多模态嵌入模型、重排序模型、RAG与智能体系统集成），并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态信息的快速增长，视觉文档检索成为连接非结构化视觉丰富数据与精确信息获取的关键前沿。视觉文档具有密集文本内容、复杂布局和细粒度语义依赖等独特特征，与传统自然图像检索不同，需要专门的研究综述来梳理该领域发展。

Method: 本文采用系统性综述方法：首先分析基准测试现状，然后深入研究方法演进，将方法分为三大类：多模态嵌入模型、多模态重排序模型，以及检索增强生成（RAG）与智能体系统在复杂文档智能中的集成应用。

Result: 本文提供了视觉文档检索领域的首个全面综述，系统梳理了多模态大语言模型时代的技术发展脉络，建立了清晰的方法分类体系，为研究者提供了该领域的整体视图。

Conclusion: 本文识别了视觉文档检索领域的持续挑战，并规划了有前景的未来研究方向，旨在为未来多模态文档智能提供清晰的发展路线图。

Abstract: With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.

</details>


### [14] [Contradiction to Consensus: Dual Perspective, Multi Source Retrieval Based Claim Verification with Source Level Disagreement using LLM](https://arxiv.org/abs/2602.18693)
*Md Badsha Biswas,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 提出基于大语言模型、多视角证据检索和跨源分歧分析的开放域声明验证系统，通过聚合多源证据提升验证准确性和透明度


<details>
  <summary>Details</summary>
Motivation: 现有自动声明验证系统通常依赖单一知识源，忽略不同来源间的分歧，限制了知识覆盖范围和系统透明度，需要更全面的多源证据聚合方法

Method: 开发开放域声明验证系统，采用新颖检索策略同时收集原始声明和否定形式的证据，从Wikipedia、PubMed和Google等多源获取支持性和矛盾性信息，经过过滤、去重和聚合形成统一知识库，利用LLMs进行声明验证，并通过置信度分析量化跨源分歧

Result: 在四个基准数据集和五个LLMs上的评估表明，知识聚合不仅提高了声明验证性能，还揭示了不同知识源间的推理差异，多源证据聚合显著改善系统可靠性

Conclusion: 证据的多样性、矛盾性和聚合对于构建可靠透明的声明验证系统至关重要，多视角证据检索和跨源分歧分析能有效提升事实核查系统的覆盖范围和解释性

Abstract: The spread of misinformation across digital platforms can pose significant societal risks. Claim verification, a.k.a. fact-checking, systems can help identify potential misinformation. However, their efficacy is limited by the knowledge sources that they rely on. Most automated claim verification systems depend on a single knowledge source and utilize the supporting evidence from that source; they ignore the disagreement of their source with others. This limits their knowledge coverage and transparency. To address these limitations, we present a novel system for open-domain claim verification (ODCV) that leverages large language models (LLMs), multi-perspective evidence retrieval, and cross-source disagreement analysis. Our approach introduces a novel retrieval strategy that collects evidence for both the original and the negated forms of a claim, enabling the system to capture supporting and contradicting information from diverse sources: Wikipedia, PubMed, and Google. These evidence sets are filtered, deduplicated, and aggregated across sources to form a unified and enriched knowledge base that better reflects the complexity of real-world information. This aggregated evidence is then used for claim verification using LLMs. We further enhance interpretability by analyzing model confidence scores to quantify and visualize inter-source disagreement. Through extensive evaluation on four benchmark datasets with five LLMs, we show that knowledge aggregation not only improves claim verification but also reveals differences in source-specific reasoning. Our findings underscore the importance of embracing diversity, contradiction, and aggregation in evidence for building reliable and transparent claim verification systems

</details>


### [15] [NanoKnow: How to Know What Your Language Model Knows](https://arxiv.org/abs/2602.20122)
*Lingwei Gu,Nour Jedidi,Jimmy Lin*

Main category: cs.CL

TL;DR: NanoKnow是一个基准数据集，用于研究LLMs如何编码知识，通过将问题按答案是否出现在预训练数据中划分，揭示了参数知识和外部知识的互补性。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs如何获取和编码知识是困难的，因为预训练数据通常是"黑箱"。nanochat模型的发布提供了完全开放的预训练数据，使得能够透明地研究模型参数知识的来源。

Method: 创建NanoKnow基准数据集，将Natural Questions和SQuAD中的问题划分为两个子集：答案出现在nanochat预训练语料中的问题和答案未出现的问题。使用八个nanochat检查点进行实验，分析闭书准确性、外部证据的影响以及参数知识和外部知识的相互作用。

Result: 1) 闭书准确性强烈受预训练数据中答案频率影响；2) 提供外部证据可以减轻这种频率依赖性；3) 即使有外部证据，模型在预训练中见过的答案上更准确，表明参数知识和外部知识是互补的；4) 不相关信息是有害的，准确性随着不相关上下文的位置和数量而降低。

Conclusion: NanoKnow基准为研究LLMs如何编码知识提供了透明框架，揭示了参数知识和外部知识的互补关系，以及不相关信息对模型性能的负面影响。该工作有助于理解LLMs的知识来源和编码机制。

Abstract: How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a "black box" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.

</details>


### [16] [Semantic Substrate Theory: An Operator-Theoretic Framework for Geometric Semantic Drift](https://arxiv.org/abs/2602.18699)
*Stephen Russell*

Main category: cs.CL

TL;DR: 该论文提出一个统一框架来形式化语义漂移的多种信号，将嵌入几何与局部扩散结合在时间索引的基底中，引入桥质量作为预测未来邻域重连的指标。


<details>
  <summary>Details</summary>
Motivation: 当前语义漂移研究存在多种信号（如嵌入位移、邻居变化、分布差异、递归轨迹不稳定），但缺乏一个共享的解释性理论将这些信号联系起来。需要建立一个统一的形式化框架来解释这些信号之间的关系。

Method: 提出时间索引的基底 $S_t=(X,d_t,P_t)$，结合嵌入几何与局部扩散。在该基底中定义：节点级邻域漂移（测量局部条件分布变化）、粗Ricci曲率（测量语义扩散的局部收缩性）、递归漂移（探测迭代语义算子的稳定性）。引入桥质量作为节点级负曲率聚合指标来预测未来邻域重连。

Result: 论文提供了形式化模型、假设和可证伪该模型的测试方法。桥质量被提出作为预测未来邻域重连的预测因子。理论框架和测试合约已建立，实证性能评估留待后续研究。

Conclusion: 该研究为语义漂移的多重信号提供了一个统一的形式化理论框架，将几何和扩散视角相结合。通过桥质量等新指标，为预测语义变化提供了理论基础，但需要后续实证研究验证其实际性能。

Abstract: Most semantic drift studies report multiple signals e.g., embedding displacement, neighbor changes, distributional divergence, and recursive trajectory instability, without a shared explanatory theory that relates them. This paper proposes a formalization of these signals in one time-indexed substrate, $S_t=(X,d_t,P_t)$, combining embedding geometry with local diffusion. Within this substrate, node-level neighborhood drift measures changes in local conditional distributions, coarse Ricci curvature measures local contractivity of semantic diffusion, and recursive drift probes stability of iterated semantic operators. This manuscript specifies the formal model, assumptions, and tests that can refute the model. Herein, the paper introduces bridge mass, a node-level aggregate of incident negative curvature, as a predictor of future neighborhood rewiring. This paper provides the theory and test contracts; empirical performance is deferred to subsequent studies.

</details>


### [17] [ReHear: Iterative Pseudo-Label Refinement for Semi-Supervised Speech Recognition via Audio Large Language Models](https://arxiv.org/abs/2602.18721)
*Zefang Liu,Chenyang Zhu,Sangwoo Cho,Shi-Xiong Zhang*

Main category: cs.CL

TL;DR: ReHear：一种用于ASR半监督学习的迭代伪标签精炼框架，通过集成指令调优的音频感知大语言模型来纠正ASR假设，缓解错误传播问题


<details>
  <summary>Details</summary>
Motivation: 传统ASR半监督学习中的伪标签方法存在确认偏差和错误累积问题，因为噪声监督会导致错误传播

Method: 提出ReHear框架，在自训练循环中集成指令调优的音频感知LLM，该模型同时考虑ASR假设和源音频，能够从严重识别错误中恢复语音准确的转录文本

Result: 在多个基准测试中，ReHear有效缓解了错误传播，一致优于监督学习和传统伪标签基线方法

Conclusion: ReHear通过集成音频感知LLM进行迭代伪标签精炼，为ASR半监督学习提供了更可靠的训练目标，显著提升了性能

Abstract: Semi-supervised learning in automatic speech recognition (ASR) typically relies on pseudo-labeling, which often suffers from confirmation bias and error accumulation due to noisy supervision. To address this limitation, we propose ReHear, a framework for iterative pseudo-label refinement that integrates an instruction-tuned, audio-aware large language model (LLM) into the self-training loop. Unlike conventional text-based correctors, our approach conditions the LLM on both the ASR hypothesis and the source audio, allowing it to recover phonetically accurate transcripts even from severe recognition errors. These refined pseudo-labels serve as high-fidelity targets for fine-tuning the ASR model in an iterative cycle. Experimental results across diverse benchmarks demonstrate that ReHear effectively mitigates error propagation, consistently outperforming both supervised and pseudo-labeling baselines.

</details>


### [18] [Rethinking Retrieval-Augmented Generation as a Cooperative Decision-Making Problem](https://arxiv.org/abs/2602.18734)
*Lichang Song,Ting Long,Yi Chang*

Main category: cs.CL

TL;DR: CoRAG将RAG重新构建为协同多智能体决策问题，通过让重排序器和生成器作为对等决策者协同工作，而非传统的非对称依赖流水线，从而提升生成稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统大多基于排名中心、非对称依赖范式，生成器的质量高度依赖重排序器的结果，这种依赖关系限制了系统的整体性能。需要克服这种局限性，让两个组件更好地协同工作。

Method: 提出Cooperative RAG (CoRAG)框架，将RAG重新构建为协同多智能体决策问题。重排序器和生成器作为对等决策者，通过联合优化行为来共同实现任务目标，确保文档重排序和生成协调工作以改进最终响应。

Result: 实验结果表明CoRAG具有良好的泛化能力和改进的生成稳定性，即使在仅使用约10K PopQA样本进行训练的情况下也能取得良好效果。模型已在指定链接中发布。

Conclusion: 通过将RAG重新构建为协同多智能体决策问题，CoRAG框架克服了传统非对称依赖范式的限制，使重排序器和生成器能够更好地协同工作，从而提升知识密集型任务的性能。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated strong effectiveness in knowledge-intensive tasks by grounding language generation in external evidence. Despite its success, many existing RAG systems are built based on a ranking-centric, asymmetric dependency paradigm, where the generation quality of the generator is highly dependent on reranking results of the reranker. To overcome this limitation, we reformulate RAG as a cooperative multi-agent decision-making problem and propose Cooperative Retrieval-Augmented Generation (CoRAG), a framework in which the reranker and the generator act as peer decision-makers rather than being connected through an asymmetric dependency pipeline. By jointly optimizing their behaviors toward a shared task objective, the reranker and generator are encouraged to cooperate, ensuring that document reranking and generation work in concert to improve the final response. Experimental results demonstrate good generalization and improved generation stability of CoRAG, even when the model is trained on only around 10K PopQA samples. Our model released in https://anonymous.4open.science/r/CoRAG-D63F

</details>


### [19] [ArabicNumBench: Evaluating Arabic Number Reading in Large Language Models](https://arxiv.org/abs/2602.18776)
*Anas Alhumud,Abdulaziz Alhammadi,Muhammad Badruddin Khan*

Main category: cs.CL

TL;DR: ArabicNumBench是一个评估大语言模型在阿拉伯数字阅读任务上的综合基准，涵盖东方阿拉伯-印度数字和西方阿拉伯数字，测试了71个模型在210个任务上的表现，发现数值准确性和指令遵循是两种不同的能力。


<details>
  <summary>Details</summary>
Motivation: 需要系统评估大语言模型在阿拉伯数字阅读任务上的表现，特别是在处理阿拉伯语数字表示时的能力，为生产级阿拉伯语NLP系统提供模型选择指导。

Method: 创建了包含210个数字阅读任务的基准，涵盖纯数字、地址、日期、数量、价格等六个上下文类别。评估了71个模型，使用四种提示策略（零样本、零样本思维链、少样本、少样本思维链），共59,010个测试用例，并跟踪提取方法以测量结构化输出生成。

Result: 模型性能差异显著，准确率从14.29%到99.05%不等。少样本思维链提示比零样本方法准确率高2.8倍（80.06% vs 28.76%）。发现高准确率模型（98-99%）常产生非结构化输出，只有6个模型在所有测试用例中一致生成结构化输出。

Conclusion: 数值准确性和指令遵循是两种不同的能力，该基准为阿拉伯数字理解建立了基线，为生产级阿拉伯语NLP系统的模型选择提供了可操作的指导。

Abstract: We present ArabicNumBench, a comprehensive benchmark for evaluating large language models on Arabic number reading tasks across Eastern Arabic-Indic numerals (0-9 in Arabic script) and Western Arabic numerals (0-9). We evaluate 71 models from 10 providers using four prompting strategies (zero-shot, zero-shot CoT, few-shot, few-shot CoT) on 210 number reading tasks spanning six contextual categories: pure numerals, addresses, dates, quantities, and prices. Our evaluation comprises 59,010 individual test cases and tracks extraction methods to measure structured output generation. Evaluation reveals substantial performance variation, with accuracy ranging from 14.29\% to 99.05\% across models and strategies. Few-shot Chain-of-Thought prompting achieves 2.8x higher accuracy than zero-shot approaches (80.06\% vs 28.76\%). A striking finding emerges: models achieving elite accuracy (98-99\%) often produce predominantly unstructured output, with most responses lacking Arabic CoT markers. Only 6 models consistently generate structured output across all test cases, while the majority require fallback extraction methods despite high numerical accuracy. Comprehensive evaluation of 281 model-strategy combinations demonstrates that numerical accuracy and instruction-following represent distinct capabilities, establishing baselines for Arabic number comprehension and providing actionable guidance for model selection in production Arabic NLP systems.

</details>


### [20] [BURMESE-SAN: Burmese NLP Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2602.18788)
*Thura Aung,Jann Railey Montalan,Jian Gang Ngui,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: BURMESE-SAN是首个针对缅甸语的综合性基准测试，系统评估LLM在理解、推理和生成三个核心NLP能力上的表现，包含7个子任务，通过母语者驱动流程构建以确保语言自然性和文化真实性。


<details>
  <summary>Details</summary>
Motivation: 缅甸语作为低资源语言，缺乏系统性的评估基准来全面衡量LLM在理解、推理和生成方面的能力。现有评估通常局限于特定任务，且存在翻译引入的伪影问题，无法准确反映模型对缅甸语的真实掌握程度。

Method: 通过母语者驱动的严格流程构建基准，包含问答、情感分析、毒性检测、因果推理、自然语言推理、抽象摘要和机器翻译7个子任务。对开源和商业LLM进行大规模评估，分析缅甸语建模面临的挑战，如有限的预训练覆盖、丰富的形态学和句法变异。

Result: 评估结果显示，缅甸语性能更多取决于架构设计、语言表示和指令调优，而非单纯模型规模。特别是东南亚区域微调和较新模型代际带来显著提升。不同模型在各项任务上表现差异明显，揭示了缅甸语特有的建模挑战。

Conclusion: BURMESE-SAN为缅甸语提供了首个全面的评估基准，揭示了缅甸语LLM性能的关键影响因素。研究强调区域微调和语言特定表示的重要性，并发布公开排行榜以支持缅甸语及其他低资源语言的系统性评估和持续进步。

Abstract: We introduce BURMESE-SAN, the first holistic benchmark that systematically evaluates large language models (LLMs) for Burmese across three core NLP competencies: understanding (NLU), reasoning (NLR), and generation (NLG). BURMESE-SAN consolidates seven subtasks spanning these competencies, including Question Answering, Sentiment Analysis, Toxicity Detection, Causal Reasoning, Natural Language Inference, Abstractive Summarization, and Machine Translation, several of which were previously unavailable for Burmese. The benchmark is constructed through a rigorous native-speaker-driven process to ensure linguistic naturalness, fluency, and cultural authenticity while minimizing translation-induced artifacts. We conduct a large-scale evaluation of both open-weight and commercial LLMs to examine challenges in Burmese modeling arising from limited pretraining coverage, rich morphology, and syntactic variation. Our results show that Burmese performance depends more on architectural design, language representation, and instruction tuning than on model scale alone. In particular, Southeast Asia regional fine-tuning and newer model generations yield substantial gains. Finally, we release BURMESE-SAN as a public leaderboard to support systematic evaluation and sustained progress in Burmese and other low-resource languages. https://leaderboard.sea-lion.ai/detailed/MY

</details>


### [21] [EvalSense: A Framework for Domain-Specific LLM (Meta-)Evaluation](https://arxiv.org/abs/2602.18823)
*Adam Dejl,Jonathan Pearson*

Main category: cs.CL

TL;DR: EvalSense是一个用于构建领域特定大语言模型评估套件的灵活可扩展框架，通过交互式指南和自动化元评估工具帮助用户选择合适的评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统统计指标不适用于开放式生成任务，而基于LLM的评估方法虽然更灵活，但依赖于精心选择的模型、提示、参数和评估策略，容易产生配置错误和偏差，需要更系统化的评估框架。

Method: EvalSense框架提供开箱即用的多模型供应商和评估策略支持，包含两个核心组件：(1) 交互式指南帮助用户选择评估方法；(2) 自动化元评估工具通过扰动数据评估不同评估方法的可靠性。

Result: 通过临床笔记生成的案例研究验证了EvalSense的有效性，使用公开数据集从非结构化医患对话生成临床笔记，所有代码、文档和资源均已开源。

Conclusion: EvalSense为LLM评估提供了一个系统化、可扩展的解决方案，能够帮助用户构建领域特定的评估套件，提高评估的可靠性和适用性。

Abstract: Robust and comprehensive evaluation of large language models (LLMs) is essential for identifying effective LLM system configurations and mitigating risks associated with deploying LLMs in sensitive domains. However, traditional statistical metrics are poorly suited to open-ended generation tasks, leading to growing reliance on LLM-based evaluation methods. These methods, while often more flexible, introduce additional complexity: they depend on carefully chosen models, prompts, parameters, and evaluation strategies, making the evaluation process prone to misconfiguration and bias. In this work, we present EvalSense, a flexible, extensible framework for constructing domain-specific evaluation suites for LLMs. EvalSense provides out-of-the-box support for a broad range of model providers and evaluation strategies, and assists users in selecting and deploying suitable evaluation methods for their specific use-cases. This is achieved through two unique components: (1) an interactive guide aiding users in evaluation method selection and (2) automated meta-evaluation tools that assess the reliability of different evaluation approaches using perturbed data. We demonstrate the effectiveness of EvalSense in a case study involving the generation of clinical notes from unstructured doctor-patient dialogues, using a popular open dataset. All code, documentation, and assets associated with EvalSense are open-source and publicly available at https://github.com/nhsengland/evalsense.

</details>


### [22] [DeepInnovator: Triggering the Innovative Capabilities of LLMs](https://arxiv.org/abs/2602.18920)
*Tianyu Fan,Fengji Zhang,Yuxiang Zheng,Bei Chen,Xinyao Niu,Chengen Huang,Junyang Lin,Chao Huang*

Main category: cs.CL

TL;DR: DeepInnovator是一个训练框架，旨在激发大语言模型的创新能力，通过"站在巨人肩膀上"的数据提取和"猜想与反驳"的训练范式，使模型能够自主生成新颖且有意义的科研想法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在加速科学发现中的应用主要依赖复杂的提示工程，缺乏系统化的训练范式来培养模型的创新能力，特别是自主生成新颖且有意义的科研想法的能力。

Method: 包含两个核心组件：1) "站在巨人肩膀上"：构建自动化数据提取管道，从大量未标注科学文献中提取和组织结构化研究知识；2) "猜想与反驳"：引入"下一个想法预测"训练范式，将研究想法的生成建模为持续预测、评估和精炼合理且新颖的下一个想法的迭代过程。

Result: DeepInnovator-14B在自动和专家评估中显著优于未训练的基线模型，胜率达到80.53%-93.81%，性能与当前领先的LLM相当。

Conclusion: 该工作为构建具有真正原创创新能力的研究智能体提供了可扩展的训练路径，并将开源数据集以促进社区发展。

Abstract: The application of Large Language Models (LLMs) in accelerating scientific discovery has garnered increasing attention, with a key focus on constructing research agents endowed with innovative capability, i.e., the ability to autonomously generate novel and significant research ideas. Existing approaches predominantly rely on sophisticated prompt engineering and lack a systematic training paradigm. To address this, we propose DeepInnovator, a training framework designed to trigger the innovative capability of LLMs. Our approach comprises two core components. (1) ``Standing on the shoulders of giants''. We construct an automated data extraction pipeline to extract and organize structured research knowledge from a vast corpus of unlabeled scientific literature. (2) ``Conjectures and refutations''. We introduce a ``Next Idea Prediction'' training paradigm, which models the generation of research ideas as an iterative process of continuously predicting, evaluating, and refining plausible and novel next idea. Both automatic and expert evaluations demonstrate that our DeepInnovator-14B significantly outperforms untrained baselines, achieving win rates of 80.53\%-93.81\%, and attains performance comparable to that of current leading LLMs. This work provides a scalable training pathway toward building research agents with genuine, originative innovative capability, and will open-source the dataset to foster community advancement. Source code and data are available at: https://github.com/HKUDS/DeepInnovator.

</details>


### [23] [Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language](https://arxiv.org/abs/2602.18964)
*Toheeb Aduramomi Jimoh,Tabea De Wille,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 首个约鲁巴语讽刺检测数据集Yor-Sarc的构建，包含436个实例，采用文化感知的标注协议，标注者间一致性达到高至几乎完美水平。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测在计算语义学中面临根本挑战，需要模型解决字面意义与意图意义之间的差异。这一挑战在低资源语言中尤为突出，因为标注数据集稀缺或不存在。约鲁巴语作为尼日尔-刚果语系声调语言，拥有超过5000万使用者，但缺乏讽刺检测资源。

Method: 构建了首个约鲁巴语讽刺检测黄金标准数据集Yor-Sarc，包含436个实例。由三位来自不同方言背景的母语者标注，采用专门为约鲁巴语讽刺设计的标注协议，该协议考虑了文化因素，包含上下文敏感解释和社区知情指南。进行了全面的标注者间一致性分析，支持在其他非洲语言中复制。

Result: 实现了实质性到几乎完美的一致性（Fleiss' κ=0.7660；成对Cohen's κ=0.6732-0.8743），83.3%的实例达成一致共识。一对标注者达到几乎完美一致性（κ=0.8743；93.8%原始一致性），超过了多个英语讽刺研究基准。16.7%的多数同意案例保留为软标签，用于不确定性感知建模。

Conclusion: Yor-Sarc数据集有望促进低资源非洲语言的语义解释和文化知情NLP研究，为约鲁巴语讽刺检测提供了首个高质量资源，其标注协议和方法可为其他非洲语言的研究提供参考。

Abstract: Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present \textbf{Yor-Sarc}, the first gold-standard dataset for sarcasm detection in Yorùbá, a tonal Niger-Congo language spoken by over $50$ million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorùbá sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' $κ= 0.7660$; pairwise Cohen's $κ= 0.6732$--$0.8743$), with $83.3\%$ unanimous consensus. One annotator pair achieved almost perfect agreement ($κ= 0.8743$; $93.8\%$ raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining $16.7\%$ majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarc\footnote{https://github.com/toheebadura/yor-sarc} is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.

</details>


### [24] [Whisper: Courtside Edition Enhancing ASR Performance Through LLM-Driven Context Generation](https://arxiv.org/abs/2602.18966)
*Yonathan Ron,Shiri Gilboa,Tammuz Dubnov*

Main category: cs.CL

TL;DR: Whisper: Courtside Edition是一个多智能体LLM管道，通过领域上下文识别、命名实体识别和术语检测来增强Whisper转录，无需重新训练模型，在NBA篮球解说领域实现了17.0%的相对词错误率降低。


<details>
  <summary>Details</summary>
Motivation: 领域特定语音（如体育解说）对现有ASR系统（包括Whisper）仍然具有挑战性，这些领域包含密集的专有名词和技术术语，而模型微调成本高昂。

Method: 提出多智能体LLM管道：1) 拦截Whisper初始转录；2) 使用专门LLM智能体进行领域上下文识别、命名实体识别和术语检测；3) 生成紧凑提示来指导Whisper解码器；4) 无需重新训练模型。

Result: 在421个NBA篮球解说片段上评估：最佳管道实现17.0%的相对WER降低（从0.217到0.180，p<0.001）；40.1%的片段有改善，仅7.1%退化；显著优于直接转录后编辑。

Conclusion: 基于提示的增强可以为ASR提供可扩展的领域适应，是成本高昂的模型微调的实用替代方案，展示了多智能体LLM管道在改善领域特定语音识别方面的有效性。

Abstract: Domain-specific speech remains a persistent challenge for automatic speech recognition (ASR), even for state-of-the-art systems like OpenAI's Whisper. We introduce Whisper: Courtside Edition, a novel multi-agent large language model (LLM) pipeline that enhances Whisper transcriptions without retraining. The pipeline intercepts Whisper's initial transcript, applies specialized LLM agents for domain context identification, named entity recognition, and jargon detection, and generates compact prompts that guide Whisper's decoder. Evaluated on 421 NBA basketball commentary segments (a domain characterized by dense proper nouns and technical terminology) our best pipeline achieves a statistically significant 17.0% relative reduction in word error rate (WER; from 0.217 to 0.180, p<0.001). Improvements are observed in 40.1% of segments with degradation in only 7.1%, substantially outperforming direct transcript post-editing. These results demonstrate that prompt-based augmentation can deliver scalable domain adaptation for ASR, offering a practical alternative to costly model fine-tuning.

</details>


### [25] [Uncovering Context Reliance in Unstructured Knowledge Editing](https://arxiv.org/abs/2602.19043)
*Zisheng Zhou,Mengqi Zhang,Shiguang Wu,Xiaotian Ye,Chi Zhang,Zhumin Chen,Pengjie Ren*

Main category: cs.CL

TL;DR: COIN框架通过减少上下文依赖，提升LLM非结构化编辑的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个token预测（NTP）的LLM编辑方法存在"上下文依赖"问题，导致编辑后的知识在推理时若缺少特定上下文会召回失败，需要解决这一根本性缺陷以实现鲁棒的知识编辑

Method: 提出COIN（上下文无关编辑框架），通过鼓励模型关注局部范围内的知识而非记忆上下文模式，减少上下文依赖；理论分析表明上下文依赖是梯度优化的固有结果

Result: COIN将上下文依赖减少45.2%，编辑成功率比强基线提升23.6%，验证了缓解上下文依赖对鲁棒编辑的关键作用

Conclusion: 上下文依赖是基于NTP的LLM编辑方法的核心失败模式，COIN框架通过减少这种依赖显著提升了编辑的鲁棒性和成功率

Abstract: Editing Large language models (LLMs) with real-world, unstructured knowledge is essential for correcting and updating their internal parametric knowledge. In this work, we revisit the fundamental next-token prediction (NTP) as a candidate paradigm for unstructured editing. We identify Context Reliance as a critical failure mode of NTP-based approaches, where knowledge acquired from edited text becomes highly dependent on its preceding context, leading to recall failures when that context is absent during inference. This hypothesis is supported by our empirical validation that prepending context during inference recovers knowledge recall. We further theoretically demonstrate that Context Reliance is an inherent consequence of gradient-based optimization, which tends to bind acquired knowledge to a specific aggregated contextual representation. To address this, we propose a simple yet effective COntext-INdependent editing framework (COIN), encouraging model to focus on knowledge within local scope rather than memorizing contextual patterns. Evaluations show that COIN reduces Context Reliance by 45.2% and outperforms strong baselines by 23.6% in editing success rate, highlighting the vital role of mitigating Context Reliance for robust editing.

</details>


### [26] [IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning](https://arxiv.org/abs/2602.19049)
*Yinhan He,Yaochen Zhu,Mingjia Shi,Wendy Zheng,Lin Su,Xiaoqing Wang,Qi Guo,Jundong Li*

Main category: cs.CL

TL;DR: IAPO是一种基于信息论的训练后优化框架，通过条件互信息为每个token分配优势值，在减少推理长度的同时提高准确性


<details>
  <summary>Details</summary>
Motivation: 现有序列级奖励塑造方法对token级推理努力分配控制有限，长链推理虽然提高准确性但带来显著推理时间成本

Method: 提出IAPO框架，基于每个token与最终答案的条件互信息分配token级优势值，识别信息丰富的推理步骤并抑制低效探索

Result: IAPO在保持正确性的同时将推理长度减少高达36%，在各种推理数据集上优于现有token高效RL方法

Conclusion: 信息感知的优势塑造是token高效训练后优化的强大通用方向

Abstract: Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how reasoning effort is allocated across tokens. To bridge the gap, we propose IAPO, an information-theoretic post-training framework that assigns token-wise advantages based on each token's conditional mutual information (MI) with the final answer. This yields an explicit, principled mechanism for identifying informative reasoning steps and suppressing low-utility exploration. We provide a theoretical analysis showing that our IAPO can induce monotonic reductions in reasoning verbosity without harming correctness. Empirically, IAPO consistently improves reasoning accuracy while reducing reasoning length by up to 36%, outperforming existing token-efficient RL methods across various reasoning datasets. Extensive empirical evaluations demonstrate that information-aware advantage shaping is a powerful and general direction for token-efficient post-training. The code is available at https://github.com/YinhanHe123/IAPO.

</details>


### [27] [TriTopic: Tri-Modal Graph-Based Topic Modeling with Iterative Refinement and Archetypes](https://arxiv.org/abs/2602.19079)
*Roman Egger*

Main category: cs.CL

TL;DR: TriTopic是一个三模态图融合框架，通过语义嵌入、TF-IDF和元数据的融合解决BERTopic等主题模型的不稳定性、词义模糊和单视角依赖问题，在多个数据集上实现了最高的NMI得分和100%文档覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法（如BERTopic）存在三个关键限制：随机不稳定性、词义精确度损失（"嵌入模糊"）以及对单一数据视角的依赖。这些限制影响了主题建模的可靠性和实用性。

Method: TriTopic采用三模态图融合框架，包含三个核心创新：1) 通过互kNN和共享最近邻构建混合图以消除噪声和应对维度灾难；2) 共识Leiden聚类实现可重复的稳定分区；3) 迭代精炼通过动态中心点拉动锐化嵌入。此外，用基于边界案例的原型表示替代传统的"平均文档"概念。

Result: 在20 Newsgroups、BBC News、AG News和Arxiv数据集上的基准测试显示，TriTopic在所有数据集上都实现了最高的NMI得分（平均NMI 0.575，BERTopic为0.513，NMF为0.416，LDA为0.299），保证100%语料库覆盖率和0%异常值。

Conclusion: TriTopic通过三模态图融合框架有效解决了现有主题建模方法的局限性，提供了更稳定、精确和全面的主题发现能力，并已作为开源PyPI库提供。

Abstract: Topic modeling extracts latent themes from large text collections, but leading approaches like BERTopic face critical limitations: stochastic instability, loss of lexical precision ("Embedding Blur"), and reliance on a single data perspective.
  We present TriTopic, a framework that addresses these weaknesses through a tri-modal graph fusing semantic embeddings, TF-IDF, and metadata. Three core innovations drive its performance: hybrid graph construction via Mutual kNN and Shared Nearest Neighbors to eliminate noise and combat the curse of dimensionality; Consensus Leiden Clustering for reproducible, stable partitions; and Iterative Refinement that sharpens embeddings through dynamic centroid-pulling. TriTopic also replaces the "average document" concept with archetype-based topic representations defined by boundary cases rather than centers alone.
  In benchmarks across 20 Newsgroups, BBC News, AG News, and Arxiv, TriTopic achieves the highest NMI on every dataset (mean NMI 0.575 vs. 0.513 for BERTopic, 0.416 for NMF, 0.299 for LDA), guarantees 100% corpus coverage with 0% outliers, and is available as an open-source PyPI library.

</details>


### [28] [Value Entanglement: Conflation Between Different Kinds of Good In (Some) Large Language Models](https://arxiv.org/abs/2602.19101)
*Seong Hah Cho,Junyi Li,Anna Leshinskaya*

Main category: cs.CL

TL;DR: 研究发现大型语言模型存在价值纠缠现象，即模型将道德、语法和经济三种不同价值类型混淆，特别是语法和经济价值判断过度受到道德价值影响，可通过选择性消融道德相关激活向量修复


<details>
  <summary>Details</summary>
Motivation: 为了实证测量大型语言模型实际习得的价值表征，研究人类价值表征的一个特征是区分不同类型的价值，因此探究LLMs是否同样能区分道德、语法和经济这三种不同类型的价值

Method: 通过探测模型行为、嵌入表示和残差流激活，分析LLMs对三种价值类型的表征；使用选择性消融道德相关激活向量的方法来修复价值纠缠现象

Result: 发现了普遍存在的价值纠缠现象：语法和经济价值判断都过度受到道德价值的影响，相对于人类规范而言；通过选择性消融道德相关激活向量可以修复这种混淆

Conclusion: 大型语言模型在价值表征上存在价值纠缠问题，未能清晰区分不同类型的价值，特别是语法和经济判断受到道德价值的过度影响，这为LLMs的价值对齐研究提供了重要见解

Abstract: Value alignment of Large Language Models (LLMs) requires us to empirically measure these models' actual, acquired representation of value. Among the characteristics of value representation in humans is that they distinguish among value of different kinds. We investigate whether LLMs likewise distinguish three different kinds of good: moral, grammatical, and economic. By probing model behavior, embeddings, and residual stream activations, we report pervasive cases of value entanglement: a conflation between these distinct representations of value. Specifically, both grammatical and economic valuation was found to be overly influenced by moral value, relative to human norms. This conflation was repaired by selective ablation of the activation vectors associated with morality.

</details>


### [29] [Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2602.19111)
*Kainan Liu,Yong Zhang,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: Astra是一种新的参数高效微调方法，利用模型输出激活的尾部特征向量构建任务自适应低秩适配器，在减少参数预算的同时实现更快的收敛和更好的下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA及其变体方法未能充分利用激活空间中尾部特征向量的潜力，导致微调性能可能不是最优的。

Method: 提出Astra方法，利用从任务特定校准集估计的模型输出激活的尾部特征向量，构建任务自适应低秩适配器，将更新约束在这些尾部特征向量张成的子空间中。

Result: 在自然语言理解和生成任务的16个基准测试中，Astra始终优于现有PEFT基线，在某些场景下甚至超越了全参数微调。

Conclusion: Astra通过有效利用激活空间的尾部特征向量，实现了参数高效微调的性能提升，为PEFT方法提供了新的研究方向。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.

</details>


### [30] [How Do LLMs Encode Scientific Quality? An Empirical Study Using Monosemantic Features from Sparse Autoencoders](https://arxiv.org/abs/2602.19115)
*Michael McCoubrey,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 该研究首次使用稀疏自编码器提取单语义特征，探索大语言模型如何编码科学质量概念，发现模型通过研究方法、出版物类型、高影响力领域和科学术语等四类特征来表征研究质量。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明大语言模型能够在一定程度上评估研究质量，但对其实现这一能力的内在机制理解有限。本研究旨在探索LLMs如何通过单语义特征编码科学质量概念，填补这一知识空白。

Method: 使用稀疏自编码器从大语言模型中提取相关的单语义特征，在不同实验设置下推导这些特征，并评估它们在三个研究质量相关任务中的预测能力：预测引用次数、期刊SJR和期刊h指数。

Result: 研究发现LLMs编码了与科学质量多个维度相关的特征，识别出四类重复出现的特征类型：1)反映研究方法的特征；2)与出版物类型相关的特征（文献综述通常具有更高影响力）；3)与高影响力研究领域和技术相关的特征；4)对应特定科学术语的特征。

Conclusion: 这些发现代表了理解LLMs如何封装与研究质量相关概念的重要进展，为深入探索模型内部表征机制奠定了基础。

Abstract: In recent years, there has been a growing use of generative AI, and large language models (LLMs) in particular, to support both the assessment and generation of scientific work. Although some studies have shown that LLMs can, to a certain extent, evaluate research according to perceived quality, our understanding of the internal mechanisms that enable this capability remains limited. This paper presents the first study that investigates how LLMs encode the concept of scientific quality through relevant monosemantic features extracted using sparse autoencoders. We derive such features under different experimental settings and assess their ability to serve as predictors across three tasks related to research quality: predicting citation count, journal SJR, and journal h-index. The results indicate that LLMs encode features associated with multiple dimensions of scientific quality. In particular, we identify four recurring types of features that capture key aspects of how research quality is represented: 1) features reflecting research methodologies; 2) features related to publication type, with literature reviews typically exhibiting higher impact; 3) features associated with high-impact research fields and technologies; and 4) features corresponding to specific scientific jargons. These findings represent an important step toward understanding how LLMs encapsulate concepts related to research quality.

</details>


### [31] [AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG](https://arxiv.org/abs/2602.19127)
*Qijie You,Wenkai Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: AgenticRAGTracer：首个通过大语言模型自动构建的Agentic RAG基准，支持多跳推理的逐步验证，包含1305个跨领域数据点，现有大模型在该基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic RAG基准存在两个主要问题：1）仅提供最终问答，缺乏中间跳级问题，无法分析模型在哪个步骤失败；2）大多为人工构建，耗时耗力且扩展性差。需要支持细粒度评估的自动构建基准。

Method: 提出AgenticRAGTracer基准，主要利用大语言模型自动构建，包含多领域1305个数据点，与现有主流基准无重叠。基准设计支持逐步验证，包含中间跳级问题，能够追踪推理链的每个步骤。

Result: 实验表明，即使最佳大语言模型（如GPT-5）在该数据集最难部分仅获得22.6%的EM准确率。跳级感知诊断显示，失败主要由推理链扭曲导致——过早崩溃或过度扩展，模型无法根据任务逻辑结构合理分配推理步骤。

Conclusion: AgenticRAGTracer填补了传统评估的空白，为Agentic RAG研究提供了细粒度诊断维度，将促进该领域研究并激发有意义的进展。代码和数据已开源。

Abstract: With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessing such capabilities. However, existing benchmarks typically provide only final questions and answers, while lacking the intermediate hop-level questions that gradually connect atomic questions to the final multi-hop query. This limitation prevents researchers from analyzing at which step an agent fails and restricts more fine-grained evaluation of model capabilities. Moreover, most current benchmarks are manually constructed, which is both time-consuming and labor-intensive, while also limiting scalability and generalization. To address these challenges, we introduce AgenticRAGTracer, the first Agentic RAG benchmark that is primarily constructed automatically by large language models and designed to support step-by-step validation. Our benchmark spans multiple domains, contains 1,305 data points, and has no overlap with existing mainstream benchmarks. Extensive experiments demonstrate that even the best large language models perform poorly on our dataset. For instance, GPT-5 attains merely 22.6\% EM accuracy on the hardest portion of our dataset. Hop-aware diagnosis reveals that failures are primarily driven by distorted reasoning chains -- either collapsing prematurely or wandering into over-extension. This highlights a critical inability to allocate steps consistent with the task's logical structure, providing a diagnostic dimension missing in traditional evaluations. We believe our work will facilitate research in Agentic RAG and inspire further meaningful progress in this area. Our code and data are available at https://github.com/YqjMartin/AgenticRAGTracer.

</details>


### [32] [A Dataset for Named Entity Recognition and Relation Extraction from Art-historical Image Descriptions](https://arxiv.org/abs/2602.19133)
*Stefanie Schneider,Miriam Göldl,Julian Stalter,Ricarda Vollmer*

Main category: cs.CL

TL;DR: FRAME是一个用于艺术史图像描述的手工标注数据集，支持命名实体识别和关系抽取任务，包含三个标注层和37种实体类型，与Wikidata对齐以支持实体链接和知识图谱构建。


<details>
  <summary>Details</summary>
Motivation: 艺术史领域缺乏专门针对图像描述的高质量标注数据集，现有数据集通常不包含细粒度的实体类型和关系标注，难以支持艺术史领域的命名实体识别、关系抽取和知识图谱构建任务。

Method: 从博物馆目录、拍卖列表、开放平台和学术数据库中收集艺术史图像描述，筛选出专注于单个艺术品且包含材料、构图或图像学明确陈述的文本。采用三层标注架构：元数据层（对象级属性）、内容层（描绘主题和母题）、共指层（链接重复提及）。标注37种实体类型和类型化关系链接，实体类型与Wikidata对齐。

Result: 创建了FRAME数据集，包含UIMA XMI CAS格式的标注文件、相关图像和书目元数据。该数据集支持命名实体识别、关系抽取、实体链接等任务的基准测试和模型微调，包括大语言模型的零样本和少样本设置。

Conclusion: FRAME为艺术史计算研究提供了首个专门针对图像描述的细粒度标注数据集，填补了该领域高质量标注资源的空白，支持多种自然语言处理任务和知识图谱构建，具有重要的研究价值和实用意义。

Abstract: This paper introduces FRAME (Fine-grained Recognition of Art-historical Metadata and Entities), a manually annotated dataset of art-historical image descriptions for Named Entity Recognition (NER) and Relation Extraction (RE). Descriptions were collected from museum catalogs, auction listings, open-access platforms, and scholarly databases, then filtered to ensure that each text focuses on a single artwork and contains explicit statements about its material, composition, or iconography. FRAME provides stand-off annotations in three layers: a metadata layer for object-level properties, a content layer for depicted subjects and motifs, and a co-reference layer linking repeated mentions. Across layers, entity spans are labeled with 37 types and connected by typed RE links between mentions. Entity types are aligned with Wikidata to support Named Entity Linking (NEL) and downstream knowledge-graph construction. The dataset is released as UIMA XMI Common Analysis Structure (CAS) files with accompanying images and bibliographic metadata, and can be used to benchmark and fine-tune NER and RE systems, including zero- and few-shot setups with Large Language Models (LLMs).

</details>


### [33] [Facet-Level Persona Control by Trait-Activated Routing with Contrastive SAE for Role-Playing LLMs](https://arxiv.org/abs/2602.19157)
*Wenqiu Tang,Zhen Wan,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CL

TL;DR: 提出基于对比稀疏自编码器的角色扮演智能体人格控制框架，学习与Big Five 30维度模型对齐的人格控制向量，实现精确可解释的人格引导


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演智能体的人格控制方法存在局限性：监督微调需要角色标注数据且灵活性差；基于提示和检索增强生成的方法在长对话中信号会稀释，导致人格漂移和不一致行为

Method: 提出对比稀疏自编码器框架，学习面向Big Five 30维度模型的人格控制向量；构建包含15,000个样本的泄漏控制语料库为每个维度提供平衡监督；将学习到的向量集成到模型残差空间，通过特质激活路由模块动态选择

Result: 实验表明该方法在情境化设置中保持稳定的角色保真度和输出质量，优于对比激活添加和纯提示基线；SAE+Prompt组合配置达到最佳整体性能

Conclusion: 对比训练的潜在向量可以增强人格控制同时保持对话连贯性，稀疏自编码器框架为角色扮演智能体提供了精确可解释的人格引导能力

Abstract: Personality control in Role-Playing Agents (RPAs) is commonly achieved via training-free methods that inject persona descriptions and memory through prompts or retrieval-augmented generation, or via supervised fine-tuning (SFT) on persona-specific corpora. While SFT can be effective, it requires persona-labeled data and retraining for new roles, limiting flexibility. In contrast, prompt- and RAG-based signals are easy to apply but can be diluted in long dialogues, leading to drifting and sometimes inconsistent persona behavior. To address this, we propose a contrastive Sparse AutoEncoder (SAE) framework that learns facet-level personality control vectors aligned with the Big Five 30-facet model. A new 15,000-sample leakage-controlled corpus is constructed to provide balanced supervision for each facet. The learned vectors are integrated into the model's residual space and dynamically selected by a trait-activated routing module, enabling precise and interpretable personality steering. Experiments on Large Language Models (LLMs) show that the proposed method maintains stable character fidelity and output quality across contextualized settings, outperforming Contrastive Activation Addition (CAA) and prompt-only baselines. The combined SAE+Prompt configuration achieves the best overall performance, confirming that contrastively trained latent vectors can enhance persona control while preserving dialogue coherence.

</details>


### [34] [TurkicNLP: An NLP Toolkit for Turkic Languages](https://arxiv.org/abs/2602.19174)
*Sherzod Hakimov*

Main category: cs.CL

TL;DR: TurkicNLP是一个开源Python库，为突厥语系提供统一的NLP处理管道，覆盖四种文字体系，包含多种NLP任务和跨语言功能。


<details>
  <summary>Details</summary>
Motivation: 突厥语系有超过2亿使用者，但其自然语言处理工具和资源分散且不统一，大多数语言缺乏标准化的NLP工具。

Method: 采用模块化多后端架构，集成基于规则的有限状态转换器和神经模型，通过语言无关的API提供统一处理，支持自动文字检测和变体路由。

Result: 开发了TurkicNLP库，支持分词、形态分析、词性标注、依存句法分析、命名实体识别、双向文字转写、跨语言句子嵌入和机器翻译等多种功能，输出遵循CoNLL-U标准。

Conclusion: TurkicNLP为突厥语系提供了首个统一的NLP工具库，解决了该语系NLP资源碎片化问题，促进了突厥语言处理研究和应用的发展。

Abstract: Natural language processing for the Turkic language family, spoken by over 200 million people across Eurasia, remains fragmented, with most languages lacking unified tooling and resources. We present TurkicNLP, an open-source Python library providing a single, consistent NLP pipeline for Turkic languages across four script families: Latin, Cyrillic, Perso-Arabic, and Old Turkic Runic. The library covers tokenization, morphological analysis, part-of-speech tagging, dependency parsing, named entity recognition, bidirectional script transliteration, cross-lingual sentence embeddings, and machine translation through one language-agnostic API. A modular multi-backend architecture integrates rule-based finite-state transducers and neural models transparently, with automatic script detection and routing between script variants. Outputs follow the CoNLL-U standard for full interoperability and extension. Code and documentation are hosted at https://github.com/turkic-nlp/turkicnlp .

</details>


### [35] [Retrieval Augmented Enhanced Dual Co-Attention Framework for Target Aware Multimodal Bengali Hateful Meme Detection](https://arxiv.org/abs/2602.19212)
*Raihan Tanvir,Md. Golam Rabiul Alam*

Main category: cs.CL

TL;DR: 该研究针对孟加拉语仇恨表情包的检测挑战，提出增强型双协同注意力框架xDORA，结合检索增强和FAISS近邻分类器，在扩展数据集上取得了优于基线和LLaVA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上多模态仇恨表情包日益增多，但在孟加拉语等低资源语言中，自动检测面临标注数据有限、类别不平衡和普遍存在的代码混合等挑战，需要开发有效的跨模态检测方法。

Method: 1. 扩展BHM数据集，加入MIMOSA的语义对齐样本以改善类别平衡和语义多样性；2. 提出xDORA框架，集成视觉编码器（CLIP、DINOv2）和多语言文本编码器（XGLM、XLM-R），通过加权注意力池化学习鲁棒的跨模态表示；3. 开发基于FAISS的k近邻分类器进行非参数推理；4. 引入RAG-Fused DORA，结合检索驱动的上下文推理；5. 评估LLaVA在零样本、少样本和检索增强提示设置下的性能。

Result: xDORA（CLIP + XLM-R）在仇恨表情包识别和目标实体检测上分别获得0.78和0.71的宏平均F1分数；RAG-Fused DORA将性能提升至0.79和0.74，优于DORA基线。FAISS分类器表现竞争性，通过语义相似性建模对稀有类别具有鲁棒性。LLaVA在少样本设置下效果有限，检索增强仅带来适度改进，表明预训练视觉语言模型对代码混合的孟加拉语内容需要微调。

Conclusion: 研究表明，监督学习、检索增强和非参数多模态框架能有效处理低资源仇恨言论检测中的语言和文化复杂性。xDORA和RAG-Fused DORA为多模态仇恨内容检测提供了有效解决方案，而预训练大模型在未微调时对代码混合低资源语言内容效果有限。

Abstract: Hateful content on social media increasingly appears as multimodal memes that combine images and text to convey harmful narratives. In low-resource languages such as Bengali, automated detection remains challenging due to limited annotated data, class imbalance, and pervasive code-mixing. To address these issues, we augment the Bengali Hateful Memes (BHM) dataset with semantically aligned samples from the Multimodal Aggression Dataset in Bengali (MIMOSA), improving both class balance and semantic diversity. We propose the Enhanced Dual Co-attention Framework (xDORA), integrating vision encoders (CLIP, DINOv2) and multilingual text encoders (XGLM, XLM-R) via weighted attention pooling to learn robust cross-modal representations. Building on these embeddings, we develop a FAISS-based k-nearest neighbor classifier for non-parametric inference and introduce RAG-Fused DORA, which incorporates retrieval-driven contextual reasoning. We further evaluate LLaVA under zero-shot, few-shot, and retrieval-augmented prompting settings. Experiments on the extended dataset show that xDORA (CLIP + XLM-R) achieves macro-average F1-scores of 0.78 for hateful meme identification and 0.71 for target entity detection, while RAG-Fused DORA improves performance to 0.79 and 0.74, yielding gains over the DORA baseline. The FAISS-based classifier performs competitively and demonstrates robustness for rare classes through semantic similarity modeling. In contrast, LLaVA exhibits limited effectiveness in few-shot settings, with only modest improvements under retrieval augmentation, highlighting constraints of pretrained vision-language models for code-mixed Bengali content without fine-tuning. These findings demonstrate the effectiveness of supervised, retrieval-augmented, and non-parametric multimodal frameworks for addressing linguistic and cultural complexities in low-resource hate speech detection.

</details>


### [36] [Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations](https://arxiv.org/abs/2602.19320)
*Dongming Jiang,Yi Li,Songtao Wei,Jinxin Yang,Ayushi Kishore,Alysa Zhao,Dingyi Kang,Xu Hu,Feng Chen,Qiannan Li,Bingzhe Li*

Main category: cs.CL

TL;DR: 该论文综述了LLM智能体记忆系统的现状，指出当前系统存在基准测试不足、评估指标不匹配、性能依赖主干模型、系统成本被忽视等问题，并提出结构化分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体记忆系统架构快速发展，但其经验基础仍然脆弱：现有基准测试规模不足、评估指标与语义效用不匹配、性能在不同主干模型间差异显著、系统级成本常被忽视。需要系统分析这些问题以推动更可靠的评估和可扩展的系统设计。

Method: 提出结构化分析方法：1) 基于四种记忆结构引入MAG系统简明分类法；2) 分析当前系统的关键痛点，包括基准饱和效应、指标有效性和评估敏感性、主干模型依赖的准确性、以及记忆维护引入的延迟和吞吐量开销。

Result: 通过连接记忆结构与经验限制，阐明了为什么当前智能体记忆系统通常未能达到其理论承诺，并为更可靠的评估和可扩展系统设计指明了方向。

Conclusion: 智能体记忆系统需要更坚实的经验基础，包括更好的基准测试、更有效的评估指标、减少对主干模型的依赖、以及考虑系统级成本。通过结构化分析框架，为未来研究提供了明确的方向。

Abstract: Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.

</details>


### [37] [Personalized Prediction of Perceived Message Effectiveness Using Large Language Model Based Digital Twins](https://arxiv.org/abs/2602.19403)
*Jasmin Han,Janardan Devkota,Joseph Waring,Amanda Luken,Felix Naughton,Roger Vilardaga,Jonathan Bricker,Carl Latkin,Meghan Moran,Yiqun Chen,Johannes Thrul*

Main category: cs.CL

TL;DR: LLM数字孪生模型在预测戒烟信息感知有效性方面优于传统监督学习和零/少样本方法，为移动健康干预个性化提供新途径


<details>
  <summary>Details</summary>
Motivation: 感知信息有效性对移动健康平台戒烟干预的个性化选择和优化至关重要，需要评估大型语言模型是否能准确预测戒烟信息的感知有效性

Method: 评估了三种模型：基于标记数据的监督学习模型、零/少样本LLM提示方法、以及结合个体特征和先前PME历史的LLM数字孪生模型。数据集包含301名年轻成年吸烟者对3010条信息的5点李克特量表评分，评估三个领域：内容质量、应对支持和戒烟支持

Result: LLM数字孪生模型平均比零/少样本LLM高12个百分点，比监督基线高13个百分点。在内容、应对和戒烟三个领域的准确率分别为0.49、0.45和0.49，在简化的3点量表上方向准确率分别为0.75、0.66和0.70。数字孪生预测在评分类别间显示出更大的离散度，表明对个体差异的敏感性提高

Conclusion: 整合个人档案与LLM能够捕捉PME中个体特异性差异，优于监督学习和零/少样本方法。改进的PME预测可能实现移动健康干预内容的更精准定制。LLM数字孪生模型在支持移动戒烟和其他健康行为改变干预的个性化方面显示出潜力

Abstract: Perceived message effectiveness (PME) by potential intervention end-users is important for selecting and optimizing personalized smoking cessation intervention messages for mobile health (mHealth) platform delivery. This study evaluates whether large language models (LLMs) can accurately predict PME for smoking cessation messages.
  We evaluated multiple models for predicting PME across three domains: content quality, coping support, and quitting support. The dataset comprised 3010 message ratings (5-point Likert scale) from 301 young adult smokers. We compared (1) supervised learning models trained on labeled data, (2) zero and few-shot LLMs prompted without task-specific fine-tuning, and (3) LLM-based digital twins that incorporate individual characteristics and prior PME histories to generate personalized predictions. Model performance was assessed on three held-out messages per participant using accuracy, Cohen's kappa, and F1.
  LLM-based digital twins outperformed zero and few-shot LLMs (12 percentage points on average) and supervised baselines (13 percentage points), achieving accuracies of 0.49 (content), 0.45 (coping), and 0.49 (quitting), with directional accuracies of 0.75, 0.66, and 0.70 on a simplified 3-point scale. Digital twin predictions showed greater dispersion across rating categories, indicating improved sensitivity to individual differences.
  Integrating personal profiles with LLMs captures person-specific differences in PME and outperforms supervised and zero and few-shot approaches. Improved PME prediction may enable more tailored intervention content in mHealth. LLM-based digital twins show potential for supporting personalization of mobile smoking cessation and other health behavior change interventions.

</details>


### [38] [How to Train Your Deep Research Agent? Prompt, Reward, and Policy Optimization in Search-R1](https://arxiv.org/abs/2602.19526)
*Yinuo Xu,Shuo Lu,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Ran He,Jian Liang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习在深度研究智能体中的作用，通过解耦分析提示模板、奖励函数和策略优化三个维度，提出了改进的Search-R1++基线方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习已被证明能提升深度研究智能体在知识密集型任务中的表现，但其具体贡献机制尚未得到充分探索。为了深入理解RL的作用，需要对其进行系统性研究，以开发更原则性和可靠的训练策略。

Method: 采用解耦研究方法，从三个维度分析RL的作用：1) 提示模板（比较Fast Thinking与Slow Thinking模板）；2) 奖励函数（分析F1-based与EM奖励，并引入动作级惩罚）；3) 策略优化方法（比较REINFORCE、PPO和GRPO）。基于研究发现构建Search-R1++基线方法。

Result: 研究发现：1) Fast Thinking模板比Slow Thinking模板更稳定且性能更好；2) F1-based奖励因答案回避导致训练崩溃，通过加入动作级惩罚可超越EM奖励；3) REINFORCE优于PPO且需要更少搜索动作，GRPO稳定性最差。Search-R1++将Search-R1性能从0.403提升至0.442（Qwen2.5-7B）和0.289提升至0.331（Qwen2.5-3B）。

Conclusion: 通过系统性解耦研究揭示了强化学习在深度研究智能体中的关键作用机制，提出了改进的Search-R1++基线方法，为开发更原则性和可靠的RL训练策略奠定了基础。

Abstract: Deep Research agents tackle knowledge-intensive tasks through multi-round retrieval and decision-oriented generation. While reinforcement learning (RL) has been shown to improve performance in this paradigm, its contributions remain underexplored. To fully understand the role of RL, we conduct a systematic study along three decoupled dimensions: prompt template, reward function, and policy optimization. Our study reveals that: 1) the Fast Thinking template yields greater stability and better performance than the Slow Thinking template used in prior work; 2) the F1-based reward underperforms the EM due to training collapse driven by answer avoidance; this can be mitigated by incorporating action-level penalties, ultimately surpassing EM; 3) REINFORCE outperforms PPO while requiring fewer search actions, whereas GRPO shows the poorest stability among policy optimization methods. Building on these insights, we then introduce Search-R1++, a strong baseline that improves the performance of Search-R1 from 0.403 to 0.442 (Qwen2.5-7B) and 0.289 to 0.331 (Qwen2.5-3B). We hope that our findings can pave the way for more principled and reliable RL training strategies in Deep Research systems.

</details>


### [39] [Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining](https://arxiv.org/abs/2602.19548)
*Jeffrey Li,Josh Gardner,Doug Kang,Fangping Shi,Karanjeet Singh,Chun-Liang Li,Herumb Shandilya,David Hall,Oncel Tuzel,Percy Liang,Ludwig Schmidt,Hadi Pour Ansari,Fartash Faghri*

Main category: cs.CL

TL;DR: 通过使用多种HTML文本提取器的并集而非单一固定提取器，可以显著提高网页数据的覆盖率和利用率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集在处理多样化的网页内容时，普遍采用单一的固定HTML文本提取器，这可能导致互联网数据的覆盖率和利用率不足。

Method: 研究比较不同HTML文本提取器对数据提取的影响，提出使用多种提取器的并集来增加token产量，并分析提取器选择对结构化内容（如表格和代码块）下游任务性能的影响。

Result: 不同提取器虽然对标准语言理解任务性能影响相似，但通过并集操作可将DCLM-Baseline的token产量提高达71%且保持基准性能；对于结构化内容，提取器选择对WikiTQ任务影响达10个百分点，对HumanEval影响达3个百分点。

Conclusion: 采用多种HTML文本提取器的并集策略可以显著提高网页数据的覆盖率和利用率，特别是对于包含结构化内容（表格、代码块）的网页，提取器选择对下游任务性能有重要影响。

Abstract: One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.

</details>


### [40] [DEEP: Docker-based Execution and Evaluation Platform](https://arxiv.org/abs/2602.19583)
*Sergio Gómez González,Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: DEEP是一个自动化评估软件，用于机器翻译和光学字符识别模型的执行、评分和性能聚类分析，提供可视化界面和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在研究中，系统比较评估是常见任务，无论是选择合适系统用于工作，还是展示研究成果潜力，或是进行公开竞赛评估，都需要有效的自动化评估工具。

Method: DEEP软件采用容器化方法，接收dockerized系统，自动执行模型并提取信息，将假设与参考标准对比评估。使用基于统计显著性分析的聚类算法对模型结果进行分析，并提供可视化web应用。

Result: DEEP能够自动化执行和评估模型，通过聚类算法识别性能集群，帮助评估者更好理解各模型性能差异的显著性，并提供可视化结果展示。

Conclusion: DEEP是一个有效的自动化评估工具，能够简化机器翻译和OCR模型的比较评估过程，提供统计显著性分析和可视化，且易于扩展到其他任务，文中还提供了使用案例展示。

Abstract: Comparative evaluation of several systems is a recurrent task in researching. It is a key step before deciding which system to use for our work, or, once our research has been conducted, to demonstrate the potential of the resulting model. Furthermore, it is the main task of competitive, public challenges evaluation. Our proposed software (DEEP) automates both the execution and scoring of machine translation and optical character recognition models. Furthermore, it is easily extensible to other tasks. DEEP is prepared to receive dockerized systems, run them (extracting information at that same time), and assess hypothesis against some references. With this approach, evaluators can achieve a better understanding of the performance of each model. Moreover, the software uses a clustering algorithm based on a statistical analysis of the significance of the results yielded by each model, according to the evaluation metrics. As a result, evaluators are able to identify clusters of performance among the swarm of proposals and have a better understanding of the significance of their differences. Additionally, we offer a visualization web-app to ensure that the results can be adequately understood and interpreted. Finally, we present an exemplary case of use of DEEP.

</details>


### [41] [Eye-Tracking-while-Reading: A Living Survey of Datasets with Open Library Support](https://arxiv.org/abs/2602.19598)
*Deborah N. Jakobi,David R. Reich,Paul Prasse,Jana M. Hofmann,Lena S. Bolliger,Lena A. Jäger*

Main category: cs.CL

TL;DR: 该论文提供了眼动阅读数据集的全面概述，创建了在线动态目录，并开发了Python工具包来增强眼动阅读研究的可发现性、可访问性、互操作性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 眼动阅读数据在多个学科和应用中具有重要价值，但现有数据集分散在不同领域，缺乏统一的数据共享标准，导致互操作性差、难以重用。需要提高数据集的透明度和可访问性。

Method: 通过三个主要方法：1) 对现有数据集进行广泛调查和特征分析；2) 创建在线动态目录（https://dili-lab.github.io/datasets.html），包含45个以上特征；3) 将所有公开数据集集成到Python包pymovements中，提供眼动数据集库。

Result: 建立了包含多个眼动阅读数据集的综合目录，提供了标准化的数据访问接口，增强了数据集的互操作性。在线目录持续更新，Python工具包简化了数据访问和分析流程。

Conclusion: 该工作通过系统化整理、标准化接口和工具开发，显著提升了眼动阅读数据的FAIR原则（可发现、可访问、可互操作、可重用）实践，促进了科学研究的可重复性和可复制性。

Abstract: Eye-tracking-while-reading corpora are a valuable resource for many different disciplines and use cases. Use cases range from studying the cognitive processes underlying reading to machine-learning-based applications, such as gaze-based assessments of reading comprehension. The past decades have seen an increase in the number and size of eye-tracking-while-reading datasets as well as increasing diversity with regard to the stimulus languages covered, the linguistic background of the participants, or accompanying psychometric or demographic data. The spread of data across different disciplines and the lack of data sharing standards across the communities lead to many existing datasets that cannot be easily reused due to a lack of interoperability. In this work, we aim at creating more transparency and clarity with regards to existing datasets and their features across different disciplines by i) presenting an extensive overview of existing datasets, ii) simplifying the sharing of newly created datasets by publishing a living overview online, https://dili-lab.github.io/datasets.html, presenting over 45 features for each dataset, and iii) integrating all publicly available datasets into the Python package pymovements which offers an eye-tracking datasets library. By doing so, we aim to strengthen the FAIR principles in eye-tracking-while-reading research and promote good scientific practices, such as reproducing and replicating studies.

</details>


### [42] [Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning](https://arxiv.org/abs/2602.19612)
*Borisiuk Anna,Andrey Savchenko,Alexander Panchecko,Elena Tutubalina*

Main category: cs.CL

TL;DR: 该论文提出了DUAL基准，用于评估大语言模型在不同训练阶段（预训练vs监督微调）的知识遗忘效果，发现SFT阶段遗忘更平滑稳定。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘研究假设所有事实同等可遗忘，且忽视了遗忘知识来源于预训练还是监督微调阶段，需要系统评估不同训练阶段对遗忘效果的影响。

Method: 构建DUAL基准，包含28.6k个从Wikidata提取的三元组，使用Wikipedia链接计数和LLM显著性评分标注事实流行度；对比分析预训练模型和SFT模型在遗忘任务上的表现差异。

Result: 实验表明：1）SFT阶段遗忘更平滑稳定，保留率提高10-50%；2）直接在预训练模型上遗忘不稳定，易发生重新学习或灾难性遗忘；3）对遗忘数据执行SFT步骤能获得更好的遗忘效果。

Conclusion: 训练阶段对机器遗忘效果有显著影响，SFT阶段遗忘优于直接在预训练模型上遗忘，为LLM安全遗忘提供了重要设计指导。

Abstract: Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.

</details>


### [43] [KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge](https://arxiv.org/abs/2602.19643)
*Alex Robertson,Huizhi Liang,Mahbub Gani,Rohit Kumar,Srijith Rajamohan*

Main category: cs.CL

TL;DR: KGHaluBench是一个基于知识图谱的幻觉基准测试，通过动态构建多层面问题来全面评估大语言模型的真实性，相比现有基准具有更广的覆盖范围和更公平的评估


<details>
  <summary>Details</summary>
Motivation: 现有幻觉基准测试存在局限性：问题静态且狭窄，导致覆盖范围有限且评估结果可能误导。大语言模型虽然能生成连贯语言，但连贯性不等于真实性，常包含微妙幻觉

Method: 利用知识图谱动态构建具有挑战性的多层面问题，通过统计方法估计问题难度以解决流行度偏差。采用自动化验证流程检测模型弃权情况，并在概念和正确性两个层面验证回答以识别不同类型的幻觉

Result: 评估了25个前沿模型，使用新颖的准确性和幻觉度量指标。结果提供了关于不同模型规模下导致幻觉的知识因素的可解释性洞察

Conclusion: KGHaluBench为幻觉缓解的未来发展提供了公开可用的基准测试，能够更公平、更全面地洞察大语言模型的真实性

Abstract: Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.

</details>


### [44] [Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling](https://arxiv.org/abs/2602.19919)
*Xiang Li,Zikai Wei,Yiyan Qi,Wanyun Zhou,Xiang Liu,Penglei Sun,Yongqi Zhang,Xiaowen Chu*

Main category: cs.CL

TL;DR: Janus-Q是一个端到端的事件驱动交易框架，将金融新闻事件从辅助信号提升为主要决策单元，通过两阶段范式统一事件中心数据构建和模型优化，显著提升交易决策的一致性和盈利能力。


<details>
  <summary>Details</summary>
Motivation: 金融市场波动常由新闻传达的离散金融事件驱动，这些事件影响具有异质性、突发性，难以在纯数值预测目标下捕捉。现有方法面临两个关键挑战：缺乏大规模事件中心数据集，以及语言模型推理与动态市场条件下有效交易行为之间的错配。

Method: 采用两阶段范式：第一阶段构建事件中心数据集（62,400篇文章，标注10种细粒度事件类型、相关股票、情感标签和事件驱动累积异常收益）；第二阶段进行决策导向微调，结合监督学习和强化学习，通过分层门控奖励模型（HGRM）明确捕捉多个交易目标之间的权衡。

Result: Janus-Q相比市场指数和LLM基线实现了更一致、可解释且盈利的交易决策，夏普比率提升高达102.0%，方向准确性相比最强竞争策略提高超过17.5%。

Conclusion: Janus-Q成功解决了事件驱动交易中的关键挑战，通过将新闻事件作为主要决策单元并采用两阶段优化框架，显著提升了交易系统的性能和可解释性。

Abstract: Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.

</details>


### [45] [ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting](https://arxiv.org/abs/2602.19969)
*Yuxing Tian,Fengran Mo,Weixu Zhang,Yiyan Qi,Jian-Yun Nie*

Main category: cs.CL

TL;DR: ReAttn是一种后处理重加权策略，用于改进基于注意力的LLM重排序方法，通过跨文档IDF加权减少词汇偏见，并通过熵正则化缓解注意力过度集中问题。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的重排序方法虽然高效且可解释，但面临两个主要限制：1) 注意力信号过度集中在少数文档的少量标记上，导致其他标记难以区分；2) 注意力过度强调与查询词汇相似的短语，导致带有词汇相似性的不相关文档被误判为相关。

Method: 提出ReAttn后处理重加权策略：1) 计算跨文档IDF加权，降低对在候选文档中频繁出现的查询重叠标记的注意力，减少词汇偏见并强调独特术语；2) 采用基于熵的正则化缓解过度集中的注意力，鼓励在信息性标记间更平衡的分布。两种调整都直接作用于现有注意力权重，无需额外训练或监督。

Result: 大量实验证明了该方法的有效性。

Conclusion: ReAttn通过后处理重加权策略有效解决了基于注意力的重排序方法中的词汇偏见和注意力过度集中问题，提高了重排序性能。

Abstract: The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.

</details>


### [46] [Cross-lingual Matryoshka Representation Learning across Speech and Text](https://arxiv.org/abs/2602.19991)
*Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina*

Main category: cs.CL

TL;DR: 首个法语-沃洛夫语双语语音-文本嵌套嵌入模型，无需ASR-翻译流水线即可实现沃洛夫语语音查询检索法语文本


<details>
  <summary>Details</summary>
Motivation: 解决少数语言使用者面临的双重障碍：语言障碍（在线知识主要使用少数主导语言）和模态障碍（信息主要为文本形式，而许多语言主要是口语）

Method: 训练首个双语语音-文本嵌套嵌入模型，引入大规模数据整理流水线和新基准，比较建模策略，发现在冻结文本嵌套模型内进行模态融合效果最佳

Result: 模型在检索任务上表现良好，并能泛化到其他任务如语音意图检测，表明学习到了通用语义表示；分析显示信息仅集中在少数组件中，存在效率提升潜力

Conclusion: 该方法有效解决了少数语言使用者的语言和模态障碍，展示了嵌套嵌入模型在多模态跨语言检索中的潜力，同时揭示了信息集中特性带来的效率优化机会

Abstract: Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.

</details>


### [47] [QUIETT: Query-Independent Table Transformation for Robust Reasoning](https://arxiv.org/abs/2602.20017)
*Gaurav Najpande,Tampu Ravi Kumar,Manan Roy Choudhury,Neha Valeti,Yanjie Fu,Vivek Gupta*

Main category: cs.CL

TL;DR: QuIeTT是一个查询无关的表格转换框架，将原始表格预处理为SQL就绪的规范表示，通过解耦表格转换与推理来提升下游任务的可靠性


<details>
  <summary>Details</summary>
Motivation: 现实世界表格通常具有不规则模式、异构值格式和隐式关系结构，这会降低下游表格推理和问答的可靠性。现有方法多以查询依赖的方式处理这些问题，将表格清理与推理纠缠在一起，限制了泛化能力

Method: QuIeTT框架在观察到测试查询之前，将原始表格预处理为单一的SQL就绪规范表示。它执行无损的模式和值规范化，暴露隐式关系，并通过原始表格快照保留完整的来源信息

Result: 在WikiTQ、HiTab、NQ-Table和SequentialQA四个基准测试中，QuIeTT在不同模型和推理范式上都取得了持续改进，特别是在结构多样、未见问题的挑战集上表现出特别强的改进

Conclusion: 通过解耦表格转换与推理，QuIeTT实现了更干净、更可靠且高效的查询，无需修改下游模型，为处理现实世界表格的复杂性问题提供了有效的解决方案

Abstract: Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.

</details>


### [48] [gencat: Generative computerized adaptive testing](https://arxiv.org/abs/2602.20020)
*Wanyong Feng,Andrew Lan*

Main category: cs.CL

TL;DR: GENCAT是一种基于大语言模型的新型计算机自适应测试框架，通过生成式项目反应理论模型处理开放式问题，在编程数据集上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有CAT框架主要基于预测学生答题正确性，无法充分利用问题和回答中的文本信息，特别是对于开放式问题。需要一种能够处理开放式回答并利用文本信息的自适应测试方法。

Method: 1. 提出生成式项目反应理论(GIRT)模型，通过监督微调和偏好优化训练，从开放式回答估计学生知识水平并预测未见问题的回答；2. 基于GIRT模型的生成能力开发三种问题选择算法（不确定性、语言多样性、信息量）；3. 在真实编程数据集上验证框架效果。

Result: 在两个真实世界编程数据集上的实验表明，GENCAT优于现有CAT基线方法，在关键早期测试阶段AUC提升高达4.32%。

Conclusion: GENCAT成功将大语言模型整合到CAT框架中，有效处理开放式问题并利用文本信息，在知识估计和问题选择方面表现出色，为自适应测试提供了新的生成式方法。

Abstract: Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\textbf{GEN}erative \textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\% in the key early testing stages.

</details>


### [49] [AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization](https://arxiv.org/abs/2602.20040)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: AgenticSum是一个用于临床文本摘要的推理时智能体框架，通过分离上下文选择、生成、验证和针对性修正来减少幻觉内容


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化临床文本摘要方面具有巨大潜力，但由于临床文档的长度、噪声和异质性，保持事实一致性仍然具有挑战性

Method: 提出AgenticSum框架，将摘要分解为协调的阶段：压缩任务相关上下文、生成初始草稿、使用内部注意力基础信号识别弱支持跨度、在监督控制下选择性修订标记内容

Result: 在两个公共数据集上评估，使用基于参考的指标、LLM-as-a-judge评估和人工评估，AgenticSum相比原始LLM和其他强基线在各项指标上均表现出持续改进

Conclusion: 结构化、智能体化的设计结合针对性修正，为使用LLMs改进临床笔记摘要提供了有效的推理时解决方案

Abstract: Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.

</details>


### [50] [Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously](https://arxiv.org/abs/2602.20042)
*Han Bao,Yue Huang,Xiaoda Wang,Zheyuan Zhang,Yujun Zhou,Carl Yang,Xiangliang Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 论文批评当前基于单一标量奖励的通用对齐范式存在结构性局限，提出边缘对齐作为替代方案，强调保留多维价值结构、支持多元民主表征和纳入认知机制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型部署在复杂社会技术系统中，暴露出现有对齐实践的局限性。通用对齐范式将多样的人类价值压缩为单一标量奖励，在价值冲突、多元利益相关者和不可约不确定性等场景中达到结构性天花板。

Method: 提出边缘对齐作为新方法，包含七个相互依存的支柱，分为三个阶段。该方法强调保留多维价值结构、支持多元民主表征、纳入认知机制进行交互和澄清。论文还识别了数据收集、训练目标和评估等关键挑战。

Result: 分析显示标量化的数学和激励机制导致三个主要问题：结构性价值扁平化、规范性表征损失和认知不确定性盲视。边缘对齐框架将对齐重新定义为动态规范治理的生命周期问题，而非单次优化任务。

Conclusion: 需要从通用对齐范式转向边缘对齐范式，通过技术治理结合的方法应对复杂社会技术系统中的对齐挑战，将对齐视为持续的生命周期过程而非一次性优化问题。

Abstract: Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \textbf{structural} value flattening, \textbf{normative} representation loss, and \textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.

</details>


### [51] [Multilingual Large Language Models do not comprehend all natural languages to equal degrees](https://arxiv.org/abs/2602.20065)
*Natalia Moskvina,Raquel Montero,Masaya Yoshida,Ferdy Hubers,Paolo Morosi,Walid Irhaymi,Jin Yan,Tamara Serrano,Elena Pagliarini,Fritz Günther,Evelina Leivada*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在12种语言上的理解能力表现：英语并非最佳，多个罗曼语系语言表现更好，所有语言都落后于人类基线但程度不同


<details>
  <summary>Details</summary>
Motivation: 当前大多数基准测试主要评估LLMs在高资源语言（主要是WEIRD社区使用的语言）上的表现，默认假设英语是LLMs表现最佳的语言，而低资源语言则与不可靠输出相关。需要了解LLMs在不同语言上的理解能力差异。

Method: 使用3个流行的大型语言模型，在代表印欧语系、亚非语系、突厥语系、汉藏语系和日本语系的12种语言上进行语言理解任务测试，分析影响性能的因素如分词、与西班牙语/英语的语言距离、训练数据大小、数据来源等。

Result: 模型在类型多样的语言上表现出显著的语言准确性，但在所有语言上都落后于人类基线，程度不同。与预期相反，英语并非表现最佳的语言，被多个罗曼语系语言（包括一些低资源语言）系统性地超越。

Conclusion: LLMs的语言理解能力受多种因素影响，包括分词策略、语言距离、训练数据特征等。英语作为默认最佳语言的假设需要重新审视，模型在非WEIRD语言上的表现值得进一步研究。

Abstract: Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.

</details>


### [52] [How Retrieved Context Shapes Internal Representations in RAG](https://arxiv.org/abs/2602.20091)
*Samuel Yeh,Sharon Li*

Main category: cs.CL

TL;DR: 该论文通过分析检索增强生成中检索文档对LLM内部表征的影响，揭示了不同相关性文档如何塑造隐藏状态并影响生成行为


<details>
  <summary>Details</summary>
Motivation: 在现实的检索增强生成中，检索到的文档集通常包含相关性和有用性各异的混合文档。先前研究主要通过输出行为来观察这些现象，但对检索上下文如何塑造LLM内部表征以中介信息整合的了解甚少

Method: 通过潜在表征的视角研究RAG，系统分析不同类型检索文档对LLM隐藏状态的影响，以及这些内部表征变化与下游生成行为的关系。在四个问答数据集和三个LLM上，在受控的单文档和多文档设置下分析内部表征

Result: 研究结果揭示了上下文相关性和分层处理如何影响内部表征，为LLM输出行为提供解释，并为RAG系统设计提供见解

Conclusion: 通过分析检索增强生成中LLM的内部表征变化，可以更好地理解检索文档如何影响信息整合过程，这有助于解释模型输出行为并为改进RAG系统设计提供理论基础

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.

</details>


### [53] [BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop](https://arxiv.org/abs/2602.20092)
*Leshem Choshen,Ryan Cotterell,Mustafa Omer Gul,Jaap Jumelet,Tal Linzen,Aaron Mueller,Suchir Salhan,Raj Sanjay Shah,Alex Warstadt,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: BabyLM研讨会旨在弥合认知建模与语言建模之间的鸿沟，举办第四届竞赛，包含通用赛道和多语言新赛道，并征集相关领域论文


<details>
  <summary>Details</summary>
Motivation: 该研讨会的动机是消除认知建模与语言建模之间的界限，促进这两个领域的交叉研究。通过举办竞赛和征集论文，旨在推动数据高效预训练、认知合理性研究、弱模型评估等相关领域的发展

Method: 通过组织第四届BabyLM竞赛，包含两个赛道：1）通用赛道的数据高效预训练挑战；2）新增的多语言赛道。同时征集竞赛外的相关研究论文，涵盖训练效率、认知合理性研究、弱模型评估等多个方向

Result: 该摘要主要宣布了第四届BabyLM竞赛的启动和论文征集，尚未提供具体实验结果。但明确了竞赛框架和论文征集范围，为相关研究提供了平台和方向

Conclusion: BabyLM研讨会通过竞赛和论文征集的方式，为认知建模与语言建模的交叉研究提供了重要平台，特别关注数据效率、多语言性和认知合理性等关键问题，推动该领域的发展

Abstract: BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.
  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [FineRef: Fine-Grained Error Reflection and Correction for Long-Form Generation with Citations](https://arxiv.org/abs/2602.18437)
*Yixing Peng,Licheng Zhang,Shancheng Fang,Yi Liu,Peijian Gu,Quan Wang*

Main category: cs.IR

TL;DR: FineRef是一个基于细粒度错误反思的框架，通过两阶段训练策略（监督微调+过程级强化学习）教导模型自我识别和纠正引用错误（不匹配和无关），显著提升引用性能和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生成带引用的内容时存在引用不匹配或无关的问题，现有方法过度优化引用保真度而忽视与用户查询的相关性，导致答案质量下降。此外，单次生成范式在需要多个引用的长文本生成中表现不佳。

Method: 提出FineRef框架，采用两阶段训练策略：第一阶段通过监督微调建立"尝试-反思-纠正"行为模式，使用轻量级模型构建的细粒度可控反思数据，并采用在线自我反思引导策略迭代丰富训练数据；第二阶段应用过程级强化学习，采用多维度奖励方案促进反思准确性、答案质量和纠正收益。

Result: 在ALCE基准测试中，FineRef显著提升了引用性能和答案准确性。7B模型在Citation F1上比GPT-4高出18%，在EM Recall上高出4%，同时在关键评估指标上超越了最先进模型。在领域迁移设置和噪声检索场景中也表现出强大的泛化能力和鲁棒性。

Conclusion: FineRef通过细粒度错误反思有效解决了LLM引用生成中的不匹配和无关问题，通过两阶段训练策略显著提升了模型的自我反思和纠正能力，在引用保真度和答案质量方面都取得了实质性改进，具有强大的泛化能力和实际应用价值。

Abstract: Generating with citations is crucial for trustworthy Large Language Models (LLMs), yet even advanced LLMs often produce mismatched or irrelevant citations. Existing methods over-optimize citation fidelity while overlooking relevance to the user query, which degrades answer quality and robustness in real-world settings with noisy or irrelevant retrieved content. Moreover, the prevailing single-pass paradigm struggles to deliver optimal answers in long-form generation that requiring multiple citations. To address these limitations, we propose FineRef, a framework based on Fine-grained error Reflection, which explicitly teaches the model to self-identify and correct two key citation errors, mismatch and irrelevance, on a per-citation basis. FineRef follows a two-stage training strategy. The first stage instills an "attempt-reflect-correct" behavioral pattern via supervised fine-tuning, using fine-grained and controllable reflection data constructed by specialized lightweight models. An online self-reflective bootstrapping strategy is designed to improve generalization by iteratively enriching training data with verified, self-improving examples. To further enhance the self-reflection and correction capability, the second stage applies process-level reinforcement learning with a multi-dimensional reward scheme that promotes reflection accuracy, answer quality, and correction gain. Experiments on the ALCE benchmark demonstrate that FineRef significantly improves both citation performance and answer accuracy. Our 7B model outperforms GPT-4 by up to 18% in Citation F1 and 4% in EM Recall, while also surpassing the state-of-the-art model across key evaluation metrics. FineRef also exhibits strong generalization and robustness in domain transfer settings and noisy retrieval scenarios.

</details>


### [55] [Altar: Structuring Sharable Experimental Data from Early Exploration to Publication](https://arxiv.org/abs/2602.18588)
*William Gaultier,Andrea Lodetti,Ian Coghill,David Colliaux,Maximilian Fleck,Alienor Lahlou*

Main category: cs.IR

TL;DR: Altar是一个轻量级、领域无关的实验数据管理框架，基于Sacred实验追踪模型，在项目初期结构化实验数据，支持灵活的数据存储和最小化工作流干扰。


<details>
  <summary>Details</summary>
Motivation: 解决实验项目活跃开发阶段的数据和元数据管理挑战，特别是协作研究中常被忽视的早期数据管理问题，确保可重复性并避免出版时的追溯性重构。

Method: 基于Sacred实验追踪模型构建轻量级框架，使用灵活NoSQL数据库存储参数、元数据、曲线和小文件，大原始数据存储在专用存储中并通过唯一标识符链接，支持与现有工作流集成。

Result: 开发了Altar框架，提供不同用户技能水平的多种使用路径（博士生、博士后、首席研究员、实验室管理员、系统管理员），支持从本地部署到服务器公开访问的扩展。

Conclusion: Altar通过解决研究的动态阶段，为探索性实验和FAIR对齐的数据共享提供了实用桥梁，填补了项目提案中数据管理计划经常忽略的早期阶段空白。

Abstract: Managing the data and metadata during the active development phase of an experimental project presents a significant challenge, particularly in collaborative research. This phase is frequently overlooked in Data Management Plans included in project proposals, despite its important role in ensuring reproducibility and preventing the need for retroactive reconstruction at the time of publication. Here we present Altar, a lightweight, domain-agnostic framework for structuring experimental data from the onset of a project without imposing rigid data models. Altar is built around the Sacred experiment-tracking model and captures experimental (meta)data and structures them. Parameters, metadata, curves and small files are stored in a flexible NoSQL database, while large raw data are maintained in dedicated storage and linked through unique identifiers, ensuring efficiency and traceability. This integration is composable with exiting workflows, allowing integration with minimial disruption of work habits. We document different pathways to use Altar based on users skillset (PhD students, Post-docs, Principal Investigators, Laboratory administrators, System administrators). While getting started with Altar does not require a specialized infrastructure, the framework can be easily deployed on a server and made publicly accessible when scaling up or preparing data for publication. By addressing the dynamic phase of research, Altar provides a practical bridge between exploratory experimentation and FAIR-aligned data sharing.

</details>


### [56] [Towards Reliable Negative Sampling for Recommendation with Implicit Feedback via In-Community Popularity](https://arxiv.org/abs/2602.18759)
*Chen Chen,Haobo Lin,Yuanbo Xu*

Main category: cs.IR

TL;DR: 提出ICPNS框架，利用用户社区结构和社区内流行度来识别可靠负样本，解决隐式反馈推荐系统中负采样难题


<details>
  <summary>Details</summary>
Motivation: 隐式反馈推荐系统中只有正交互被观察到，缺乏显式负信号。负采样对模型训练至关重要，但设计可靠的负采样策略具有挑战性，需要同时确保真实性、难度和可解释性

Method: 提出ICPNS框架，基于用户社区结构识别可靠负样本。核心洞察是物品曝光由潜在用户社区驱动。通过识别这些社区并利用社区内流行度，ICPNS有效近似物品曝光概率。在用户社区内流行但未被点击的物品被识别为更可靠的真实负样本

Result: 在四个基准数据集上的大量实验表明，ICPNS在图基推荐器上带来一致改进，在基于矩阵分解的模型上具有竞争力，在统一评估协议下优于代表性负采样策略

Conclusion: ICPNS通过利用用户社区结构和社区内流行度，提供了一种有效识别可靠负样本的方法，解决了隐式反馈推荐系统中负采样的关键挑战

Abstract: Learning from implicit feedback is a fundamental problem in modern recommender systems, where only positive interactions are observed and explicit negative signals are unavailable. In such settings, negative sampling plays a critical role in model training by constructing negative items that enable effective preference learning and ranking optimization. However, designing reliable negative sampling strategies remains challenging, as they must simultaneously ensure realness, hardness, and interpretability. To this end, we propose \textbf{ICPNS (In-Community Popularity Negative Sampling)}, a novel framework that leverages user community structure to identify reliable and informative negative samples. Our approach is grounded in the insight that item exposure is driven by latent user communities. By identifying these communities and utilizing in-community popularity, ICPNS effectively approximates the probability of item exposure. Consequently, items that are popular within a user's community but remain unclicked are identified as more reliable true negatives. Extensive experiments on four benchmark datasets demonstrate that ICPNS yields consistent improvements on graph-based recommenders and competitive performance on MF-based models, outperforming representative negative sampling strategies under a unified evaluation protocol.

</details>


### [57] [Give Users the Wheel: Towards Promptable Recommendation Paradigm](https://arxiv.org/abs/2602.18929)
*Fuyuan Lyu,Chenglin Luo,Qiyuan Zhang,Yupeng Hou,Haolun Wu,Xing Tang,Xue Liu,Jin L. C. Guo,Xiuqiang He*

Main category: cs.IR

TL;DR: DPR框架使传统序列推荐模型能够通过自然语言提示动态引导检索过程，同时保留协同信号，解决了LLM推荐效率低和重排序受限于召回能力的问题。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型无法适应明确的用户意图（如自然语言提示），而现有LLM集成方法存在效率低或受限于召回能力的问题，需要一种既能理解语义意图又能保持协同推荐精度的方案。

Method: 提出解耦可提示序列推荐（DPR）框架，包含：1）融合模块对齐协同和语义信号；2）专家混合（MoE）架构分离正负引导的梯度冲突；3）三阶段训练策略逐步对齐提示语义空间与协同空间。

Result: 在真实世界数据集上的实验表明，DPR在提示引导任务中显著优于最先进的基线方法，同时在标准序列推荐场景中保持竞争力。

Conclusion: DPR框架成功实现了可提示推荐，使传统序列推荐模型能够原生支持自然语言引导，在保持协同信号优势的同时增强了对明确用户意图的适应性。

Abstract: Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user's immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios.

</details>


### [58] [Adaptive Multi-Agent Reasoning for Text-to-Video Retrieval](https://arxiv.org/abs/2602.19040)
*Jiaxin Wu,Xiao-Yong Wei,Qing Li*

Main category: cs.IR

TL;DR: 提出自适应多智能体检索框架，通过动态协调专门智能体进行多轮推理，解决文本到视频检索中的查询依赖时序推理问题，显著提升零样本检索性能。


<details>
  <summary>Details</summary>
Motivation: 短视频平台和多模态大语言模型的兴起增加了对可扩展、有效的零样本文本到视频检索系统的需求。现有方法在处理涉及时序、逻辑或因果关系的复杂查询时，查询依赖的时序推理能力不足，限制了其有效性。

Method: 提出自适应多智能体检索框架，包含：(1) 检索智能体用于大规模视频语料库检索；(2) 推理智能体用于零样本上下文时序推理；(3) 查询重构智能体用于优化模糊查询并恢复性能。这些智能体由编排智能体动态协调，利用中间反馈和推理结果指导执行。引入包含检索性能记忆和历史推理轨迹的新型通信机制以改进协调和决策。

Result: 在跨越八年的三个TRECVid基准测试中，该框架相比CLIP4Clip实现了两倍的性能提升，并以较大幅度优于最先进方法。

Conclusion: 自适应多智能体框架通过动态协调专门智能体进行多轮推理，有效解决了文本到视频检索中的查询依赖时序推理问题，显著提升了零样本检索性能。

Abstract: The rise of short-form video platforms and the emergence of multimodal large language models (MLLMs) have amplified the need for scalable, effective, zero-shot text-to-video retrieval systems. While recent advances in large-scale pretraining have improved zero-shot cross-modal alignment, existing methods still struggle with query-dependent temporal reasoning, limiting their effectiveness on complex queries involving temporal, logical, or causal relationships. To address these limitations, we propose an adaptive multi-agent retrieval framework that dynamically orchestrates specialized agents over multiple reasoning iterations based on the demands of each query. The framework includes: (1) a retrieval agent for scalable retrieval over large video corpora, (2) a reasoning agent for zero-shot contextual temporal reasoning, and (3) a query reformulation agent for refining ambiguous queries and recovering performance for those that degrade over iterations. These agents are dynamically coordinated by an orchestration agent, which leverages intermediate feedback and reasoning outcomes to guide execution. We also introduce a novel communication mechanism that incorporates retrieval-performance memory and historical reasoning traces to improve coordination and decision-making. Experiments on three TRECVid benchmarks spanning eight years show that our framework achieves a twofold improvement over CLIP4Clip and significantly outperforms state-of-the-art methods by a large margin.

</details>


### [59] [SIDEKICK: A Semantically Integrated Resource for Drug Effects, Indications, and Contraindications](https://arxiv.org/abs/2602.19183)
*Mohammad Ashhad,Olga Mashkova,Ricardo Henao,Robert Hoehndorf*

Main category: cs.IR

TL;DR: SIDEKICK是一个基于FDA药品标签构建的知识图谱，通过LLM提取和Graph RAG技术将药物适应症、禁忌症和不良反应标准化映射到HPO、MONDO和RxNorm本体，提升药物警戒和临床决策支持的语义推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有药物安全数据集依赖MedDRA等术语系统，限制了语义推理能力以及与语义网本体和知识图谱的互操作性，需要开发更语义集成的资源来支持药物警戒和临床决策。

Method: 开发基于大型语言模型提取和Graph检索增强生成的流程，处理超过50,000个药品标签，将术语映射到人类表型本体、MONDO疾病本体和RxNorm，使用语义科学集成本体作为上层本体，以RDF格式序列化知识图谱。

Result: SIDEKICK在药物重定向任务中基于副作用相似性分析方面优于SIDER和ONSIDES数据库，实现了自动安全监测和基于表型的相似性分析。

Conclusion: SIDEKICK知识图谱通过标准化语义集成，显著提升了药物安全数据的互操作性和推理能力，为自动化安全监测和药物重定向提供了有效工具。

Abstract: Pharmacovigilance and clinical decision support systems utilize structured drug safety data to guide medical practice. However, existing datasets frequently depend on terminologies such as MedDRA, which limits their semantic reasoning capabilities and their interoperability with Semantic Web ontologies and knowledge graphs. To address this gap, we developed SIDEKICK, a knowledge graph that standardizes drug indications, contraindications, and adverse reactions from FDA Structured Product Labels. We developed and used a workflow based on Large Language Model (LLM) extraction and Graph-Retrieval Augmented Generation (Graph RAG) for ontology mapping. We processed over 50,000 drug labels and mapped terms to the Human Phenotype Ontology (HPO), the MONDO Disease Ontology, and RxNorm. Our semantically integrated resource outperforms the SIDER and ONSIDES databases when applied to the task of drug repurposing by side effect similarity. We serialized the dataset as a Resource Description Framework (RDF) graph and employed the Semanticscience Integrated Ontology (SIO) as upper level ontology to further improve interoperability. Consequently, SIDEKICK enables automated safety surveillance and phenotype-based similarity analysis for drug repurposing.

</details>


### [60] [SplitLight: An Exploratory Toolkit for Recommender Systems Datasets and Splits](https://arxiv.org/abs/2602.19339)
*Anna Volodkevich,Dmitry Anikin,Danil Gusak,Anton Klenitskiy,Evgeny Frolov,Alexey Vasilev*

Main category: cs.IR

TL;DR: SplitLight是一个用于推荐系统离线评估的开源工具包，通过分析数据预处理和分割策略中的隐藏选择，提高实验的可重复性和跨论文可比性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统离线评估中，数据预处理阶段看似微小的决策（如过滤、重复处理、冷启动处理、分割策略设计）会显著影响模型排名，破坏实验的可重复性和跨论文可比性，但这些选择往往被隐藏或文档记录不足。

Method: 开发SplitLight工具包，给定交互日志和分割子集，分析核心和时间数据集统计、重复消费模式和时间戳异常，诊断分割有效性（包括时间泄漏、冷用户/物品暴露、分布偏移），并通过聚合摘要和交互可视化支持不同分割策略的并行比较。

Result: SplitLight作为Python工具包和交互式无代码界面，生成审计摘要，能够证明评估协议的合理性，支持推荐系统研究和工业中的透明、可靠和可比较的实验。

Conclusion: SplitLight通过使数据预处理和分割决策可测量、可比较和可报告，解决了推荐系统离线评估中的可重复性和可比性问题，为研究者和从业者提供了评估协议合理性的工具支持。

Abstract: Offline evaluation of recommender systems is often affected by hidden, under-documented choices in data preparation. Seemingly minor decisions in filtering, handling repeats, cold-start treatment, and splitting strategy design can substantially reorder model rankings and undermine reproducibility and cross-paper comparability.
  In this paper, we introduce SplitLight, an open-source exploratory toolkit that enables researchers and practitioners designing preprocessing and splitting pipelines or reviewing external artifacts to make these decisions measurable, comparable, and reportable. Given an interaction log and derived split subsets, SplitLight analyzes core and temporal dataset statistics, characterizes repeat consumption patterns and timestamp anomalies, and diagnoses split validity, including temporal leakage, cold-user/item exposure, and distribution shifts. SplitLight further allows side-by-side comparison of alternative splitting strategies through comprehensive aggregated summaries and interactive visualizations. Delivered as both a Python toolkit and an interactive no-code interface, SplitLight produces audit summaries that justify evaluation protocols and support transparent, reliable, and comparable experimentation in recommender systems research and industry.

</details>


### [61] [DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework](https://arxiv.org/abs/2602.19702)
*Adamya Shyam,Venkateswara Rao Kagita,Bharti Rana,Vikas Kumar*

Main category: cs.IR

TL;DR: DReX：一种统一的多模态推荐框架，通过多模态反馈的交互级特征增量优化用户和物品表示，解决了现有方法中模态处理孤立、需要完整多模态数据以及用户物品表示独立学习等问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统存在三个主要问题：1）孤立处理不同模态；2）训练时需要每个交互的完整多模态数据；3）独立学习用户和物品表示。这些问题导致系统复杂度增加，且用户和物品嵌入可能不对齐。

Method: 提出DReX框架，使用门控循环单元（GRU）有选择地将细粒度交互特征集成到全局表示中。通过增量更新机制，同时建模交互细节和偏好模式，无需单独的特征提取过程，并能处理缺失或变化的模态。

Result: 在三个包含评论和评分作为交互模态的真实数据集上评估，DReX在所有数据集上都优于现有最先进方法。通过将评论文本作为模态，自动生成可解释的用户和物品关键词配置文件。

Conclusion: DReX通过增量细化的统一框架解决了多模态推荐的关键挑战，实现了更好的用户-物品表示对齐、对不完整模态的鲁棒性，并提供了可解释的偏好指标，显著提升了推荐性能。

Abstract: Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.

</details>


### [62] [A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs](https://arxiv.org/abs/2602.19711)
*Krzysztof Kutt,Elżbieta Sroka,Oleksandra Ishchuk,Luiz do Valle Miranda*

Main category: cs.IR

TL;DR: 提出一个结合知识图谱嵌入、近似最近邻搜索和SPARQL语义过滤的三阶段神经符号推荐系统，用于数字文化遗产资源推荐


<details>
  <summary>Details</summary>
Motivation: 数字文化遗产资源日益增长，需要能够解释异构数据实体间语义关系的先进推荐方法

Method: 混合推荐管道，集成知识图谱嵌入（评估TransE、ComplEx、ConvE、CompGCN四种）、近似最近邻搜索（HNSW）和SPARQL驱动的语义过滤

Result: 在JUHMP知识图谱（约320万RDF三元组）上评估，ComplEx和HNSW经过超参数选择，三阶段神经符号推荐器产生有用且可解释的推荐，专家评估证实效果

Conclusion: 尽管元数据稀疏且异构，该方法仍能产生有用且可解释的推荐，为数字文化遗产资源推荐提供了完整方法论

Abstract: The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation.

</details>


### [63] [GrIT: Group Informed Transformer for Sequential Recommendation](https://arxiv.org/abs/2602.19728)
*Adamya Shyam,Venkateswara Rao Kagita,Bharti Rana,Vikas Kumar*

Main category: cs.IR

TL;DR: 提出一种结合个体序列特征与时间演化群体特征的序列推荐方法，通过可学习的时变成员权重建模用户与潜在群体的关联，提升下一项推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐方法主要关注个体用户交互历史中的时序模式，但忽略了相似用户集体行为所体现的群体级特征。作者假设显式建模随时间演化的群体特征与个体历史相结合能显著提升推荐性能。

Method: 引入潜在群体表示，通过可学习的时变成员权重建模用户与群体的关联。成员权重基于用户交互历史计算，结合短期和长期偏好，提取统计特征并通过变换生成漂移感知的成员权重。群体表示由潜在群体嵌入加权得到，与用户序列表示在Transformer块中集成，共同捕捉个人和群体级时序动态。

Result: 在五个基准数据集上的广泛实验验证了方法的有效性，一致优于最先进的序列推荐方法。

Conclusion: 通过显式建模时间演化群体特征与个体序列特征的联合表示，能够产生更丰富、上下文感知的嵌入，从而实现更准确的序列推荐。

Abstract: Sequential recommender systems aim to predict a user's future interests by extracting temporal patterns from their behavioral history. Existing approaches typically employ transformer-based architectures to process long sequences of user interactions, capturing preference shifts by modeling temporal relationships between items. However, these methods often overlook the influence of group-level features that capture the collective behavior of similar users. We hypothesize that explicitly modeling temporally evolving group features alongside individual user histories can significantly enhance next-item recommendation. Our approach introduces latent group representations, where each user's affiliation to these groups is modeled through learnable, time-varying membership weights. The membership weights at each timestep are computed by modeling shifts in user preferences through their interaction history, where we incorporate both short-term and long-term user preferences. We extract a set of statistical features that capture the dynamics of user behavior and further refine them through a series of transformations to produce the final drift-aware membership weights. A group-based representation is derived by weighting latent group embeddings with the learned membership scores. This representation is integrated with the user's sequential representation within the transformer block to jointly capture personal and group-level temporal dynamics, producing richer embeddings that lead to more accurate, context-aware recommendations. We validate the effectiveness of our approach through extensive experiments on five benchmark datasets, where it consistently outperforms state-of-the-art sequential recommendation methods.

</details>


### [64] [FairFS: Addressing Deep Feature Selection Biases for Recommender System](https://arxiv.org/abs/2602.20001)
*Xianquan Wang,Zhaocheng Du,Jieming Zhu,Qinglin Jia,Zhenhua Dong,Kai Zhang*

Main category: cs.IR

TL;DR: FairFS是一种公平准确的特征选择算法，通过解决层偏置、基线偏置和近似偏置三个问题来提升特征重要性估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中特征选择至关重要，但现有基于可训练门控和敏感性的方法存在三种偏置问题：层偏置、基线偏置和近似偏置，导致特征重要性估计不准确。

Method: 提出FairFS算法：1) 正则化所有非线性变换层的特征重要性以解决层偏置；2) 引入接近分类器决策边界的平滑基线特征缓解基线偏置；3) 采用聚合近似方法减轻近似偏置。

Result: 大量实验表明，FairFS能有效缓解三种偏置问题，在特征选择任务上达到最先进的性能。

Conclusion: FairFS通过系统性解决特征重要性估计中的偏置问题，为工业推荐系统提供了更公平准确的特征选择方法，能提升在线性能同时降低计算成本。

Abstract: Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.

</details>


### [65] [ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation](https://arxiv.org/abs/2602.20093)
*Kun Yang,Yuxuan Zhu,Yazhe Chen,Siyao Zheng,Bangyang Hong,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Hui Li*

Main category: cs.IR

TL;DR: ManCAR提出了一种基于流形约束的自适应推理框架，通过将推理过程限制在全局交互图的拓扑结构中，防止潜在漂移，实现更有效的序列推荐。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法使用目标主导目标驱动中间推理状态，缺乏明确的可行性约束，导致潜在漂移问题，即推理轨迹偏离到不合理区域。作者认为有效的推荐推理应被视为在协作流形上的导航，而非自由形式的潜在精炼。

Method: ManCAR从用户最近行为的协作邻域构建局部意图先验，表示为项目单纯形上的分布。训练期间，模型逐步将其潜在预测分布与该先验对齐，强制推理轨迹保持在有效流形内。测试时，推理自适应进行直到预测分布稳定，避免过度精炼。作者提供了ManCAR的变分解释来理论验证其漂移预防和自适应测试时停止机制。

Result: 在七个基准测试上的实验表明，ManCAR始终优于最先进的基线方法，在NDCG@10指标上实现了高达46.88%的相对改进。

Conclusion: ManCAR通过将推理过程限制在协作流形内，有效解决了潜在漂移问题，实现了更稳定和有效的序列推荐推理，为基于推理的推荐系统提供了理论保证和实践框架。

Abstract: Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [On the Dynamics of Observation and Semantics](https://arxiv.org/abs/2602.18494)
*Xiu Li*

Main category: cs.AI

TL;DR: 论文提出智能不是静态表征的几何性质，而是受物理约束（有限内存、计算、能量）的能动系统与高熵环境交互的属性，通过观察语义纤维束形式化这种交互，从热力学约束推导出符号结构的必要性。


<details>
  <summary>Details</summary>
Motivation: 挑战当前视觉智能的主流范式——将语义视为潜在表征的静态属性，认为这种观点在物理上不完整。主张智能不是现实的被动反映，而是受物理约束的能动系统与高熵环境交互的属性。

Method: 提出观察语义纤维束的动力学结构，将原始感官观察数据（纤维）投影到低熵因果语义流形（基）。证明对于任何有界智能体，信息处理的热力学成本（兰道尔原理）对内部状态转换复杂度施加严格限制（语义常数B）。从这些物理约束推导符号结构的必要性。

Result: 证明为在界限B内建模组合世界，语义流形必须经历相变——结晶为离散、组合、因子化的形式。语言和逻辑不是文化产物，而是防止热崩溃所需的信息固态形式。

Conclusion: 理解不是恢复隐藏的潜在变量，而是构建因果商，使世界在算法上可压缩且在因果上可预测。符号结构是物理约束下的必然产物。

Abstract: A dominant paradigm in visual intelligence treats semantics as a static property of latent representations, assuming that meaning can be discovered through geometric proximity in high dimensional embedding spaces. In this work, we argue that this view is physically incomplete. We propose that intelligence is not a passive mirror of reality but a property of a physically realizable agent, a system bounded by finite memory, finite compute, and finite energy interacting with a high entropy environment. We formalize this interaction through the kinematic structure of an Observation Semantics Fiber Bundle, where raw sensory observation data (the fiber) is projected onto a low entropy causal semantic manifold (the base). We prove that for any bounded agent, the thermodynamic cost of information processing (Landauer's Principle) imposes a strict limit on the complexity of internal state transitions. We term this limit the Semantic Constant B. From these physical constraints, we derive the necessity of symbolic structure. We show that to model a combinatorial world within the bound B, the semantic manifold must undergo a phase transition, it must crystallize into a discrete, compositional, and factorized form. Thus, language and logic are not cultural artifacts but ontological necessities the solid state of information required to prevent thermal collapse. We conclude that understanding is not the recovery of a hidden latent variable, but the construction of a causal quotient that renders the world algorithmically compressible and causally predictable.

</details>


### [67] [Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic](https://arxiv.org/abs/2602.18607)
*Michal Töpfer,František Plášil,Tomáš Bureš,Petr Hnětynka*

Main category: cs.AI

TL;DR: 论文提出使用vibe coding反馈循环生成自适应管理器(AM)，结合新型时序逻辑FCL进行验证，在CAS领域实验中取得良好效果


<details>
  <summary>Details</summary>
Motivation: 在CAS自适应系统中，动态架构和行为变化的定义是一个挑战。传统上通过自适应管理器实现，但生成正确代码困难。随着生成式LLM的发展，基于系统规范和期望行为生成AM代码成为可能，但需要解决生成代码的正确性验证问题

Method: 采用vibe coding反馈循环方法生成AM代码，结合新型时序逻辑FCL表达功能需求约束。通过迭代测试和反馈循环，将FCL约束评估与当前系统状态结合，为LLM提供详细的约束违反报告

Result: 在CAS领域的两个示例系统中，该方法取得了良好效果。通常只需几次反馈循环迭代，每次向LLM提供详细的约束违反报告。结合高运行路径覆盖率的初始设置，能够有效生成正确的自适应管理器

Conclusion: 当基于精确功能需求公式化验证时，通过vibe coding反馈循环生成自适应管理器是可行的。FCL时序逻辑提供了比传统LTL更细粒度的行为描述能力，结合自适应和vibe coding反馈循环的方法在实践中表现良好

Abstract: In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.
  In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.
  Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings.

</details>


### [68] [Spilled Energy in Large Language Models](https://arxiv.org/abs/2602.18671)
*Adrian Robert Minut,Hazem Dewidar,Iacopo Masi*

Main category: cs.AI

TL;DR: 将LLM的softmax分类器重新解释为能量基模型，通过分析解码过程中的"能量溢出"来检测幻觉，无需训练额外分类器或进行激活消融。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法通常需要训练探针分类器或进行激活消融，计算成本高且难以泛化。本文旨在开发一种无需训练、基于输出logits的轻量级幻觉检测方法。

Method: 将序列到序列概率链分解为多个相互作用的能量基模型，提出两种完全无需训练的指标：1) 溢出能量：捕捉连续生成步骤间理论上应匹配的能量值差异；2) 边缘化能量：可在单步中测量的能量指标。

Result: 在9个基准测试上评估了包括LLaMA、Mistral、Gemma等最先进LLM，以及在Qwen3上的合成代数运算。方法展现出稳健、有竞争力的幻觉检测能力和跨任务泛化能力，适用于预训练和指令调优变体。

Conclusion: 通过将LLM重新解释为能量基模型，开发了无需训练的幻觉检测方法，能够有效识别事实错误、偏见和失败，为轻量级、可泛化的幻觉检测提供了新思路。

Abstract: We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track "energy spills" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.

</details>


### [69] [Many AI Analysts, One Dataset: Navigating the Agentic Data Science Multiverse](https://arxiv.org/abs/2602.18710)
*Martin Bertran,Riccardo Fogliato,Zhiwei Steven Wu*

Main category: cs.AI

TL;DR: AI分析师基于大语言模型可廉价、大规模地复现结构化分析多样性，在不同模型和提示框架下对相同假设和数据产生冲突结论


<details>
  <summary>Details</summary>
Motivation: 传统"多分析师"研究需要大量协调工作且成本高昂，难以大规模开展。本文旨在探索使用自主AI分析师来廉价、大规模地复现类似的结构化分析多样性

Method: 构建基于大语言模型的自主AI分析师，在固定数据集上测试预定义假设，通过改变底层模型和提示框架进行重复运行。每个AI分析师独立构建和执行完整分析流程，AI审计员筛选方法学上有效的运行

Result: AI分析师产生的分析在效应大小、p值和二元决策上显示出广泛分散，经常逆转假设是否得到支持的判断。这种分散是结构化的：预处理、模型规范和推断中的可识别分析选择在不同LLM和角色条件下存在系统性差异。效果是可引导的：即使在排除方法学缺陷的运行后，重新分配分析师角色或LLM仍会改变结果分布

Conclusion: 自主AI分析师能够廉价、大规模地复现结构化分析多样性，揭示分析决策对研究结论的系统性影响，为理解科学发现的可变性提供了新工具

Abstract: The conclusions of empirical research depend not only on data but on a sequence of analytic decisions that published results seldom make explicit. Past ``many-analyst" studies have demonstrated this: independent teams testing the same hypothesis on the same dataset regularly reach conflicting conclusions. But such studies require months of coordination among dozens of research groups and are therefore rarely conducted. In this work, we show that fully autonomous AI analysts built on large language models (LLMs) can reproduce a similar structured analytic diversity cheaply and at scale. We task these AI analysts with testing a pre-specified hypothesis on a fixed dataset, varying the underlying model and prompt framing across replicate runs. Each AI analyst independently constructs and executes a full analysis pipeline; an AI auditor then screens each run for methodological validity. Across three datasets spanning experimental and observational designs, AI analyst-produced analyses display wide dispersion in effect sizes, $p$-values, and binary decisions on supporting the hypothesis or not, frequently reversing whether a hypothesis is judged supported. This dispersion is structured: recognizable analytic choices in preprocessing, model specification, and inference differ systematically across LLM and persona conditions. Critically, the effects are \emph{steerable}: reassigning the analyst persona or LLM shifts the distribution of outcomes even after excluding methodologically deficient runs.

</details>


### [70] [Task-Aware Exploration via a Predictive Bisimulation Metric](https://arxiv.org/abs/2602.18724)
*Dayang Liang,Ruihan Liu,Lipeng Wan,Yunlong Liu,Bo An*

Main category: cs.AI

TL;DR: TEB提出了一种任务感知探索方法，通过预测双仿真度量将任务相关表示与探索紧密耦合，解决了视觉强化学习中稀疏奖励下的探索挑战。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习在稀疏奖励下的探索面临挑战，因为存在大量任务无关的变化。现有内在探索方法要么假设能访问低维状态，要么缺乏任务感知的探索策略，在视觉领域表现脆弱。

Method: TEB利用预测双仿真度量学习行为基础的任务表示，并在学习的潜在空间中测量行为内在新颖性。首先理论上缓解稀疏奖励下退化双仿真度量的表示崩溃问题，通过引入预测奖励差异。基于此鲁棒度量，设计基于潜在空间的探索奖励，测量相邻观测的相对新颖性。

Result: 在MetaWorld和Maze2D上的大量实验表明，TEB实现了优越的探索能力，并超越了最近的基线方法。

Conclusion: TEB通过将任务相关表示与探索紧密耦合，提供了一种有效的任务感知探索方法，在视觉强化学习的稀疏奖励场景中表现出色。

Abstract: Accelerating exploration in visual reinforcement learning under sparse rewards remains challenging due to the substantial task-irrelevant variations. Despite advances in intrinsic exploration, many methods either assume access to low-dimensional states or lack task-aware exploration strategies, thereby rendering them fragile in visual domains. To bridge this gap, we present TEB, a Task-aware Exploration approach that tightly couples task-relevant representations with exploration through a predictive Bisimulation metric. Specifically, TEB leverages the metric not only to learn behaviorally grounded task representations but also to measure behaviorally intrinsic novelty over the learned latent space. To realize this, we first theoretically mitigate the representation collapse of degenerate bisimulation metrics under sparse rewards by internally introducing a simple but effective predicted reward differential. Building on this robust metric, we design potential-based exploration bonuses, which measure the relative novelty of adjacent observations over the latent space. Extensive experiments on MetaWorld and Maze2D show that TEB achieves superior exploration ability and outperforms recent baselines.

</details>


### [71] [Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization](https://arxiv.org/abs/2602.18731)
*Yuhang Bai,Yujuan Ding,Shanru Lin,Wenqi Fan*

Main category: cs.AI

TL;DR: 提出Chart Insight Agent Flow多智能体框架，利用MLLMs从图表图像中挖掘深层洞察，并创建ChartSummInsights数据集用于评估


<details>
  <summary>Details</summary>
Motivation: 现有图表摘要方法（包括多模态大语言模型）主要关注低层数据描述，未能捕捉数据可视化的根本目的——深层洞察

Method: 提出Chart Insight Agent Flow，一个计划-执行多智能体框架，有效利用MLLMs的感知和推理能力从图表图像中直接发现深刻洞察；同时创建ChartSummInsights数据集，包含真实世界图表和人类数据分析专家撰写的高质量洞察性摘要

Result: 实验结果表明，该方法显著提升了MLLMs在图表摘要任务上的性能，能够生成具有深度和多样性的洞察性摘要

Conclusion: Chart Insight Agent Flow框架能够有效挖掘图表中的深层洞察，解决了现有方法在洞察发现方面的不足，为图表理解和数据可访问性提供了新方法

Abstract: Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights.

</details>


### [72] [Federated Reasoning Distillation Framework with Model Learnability-Aware Data Allocation](https://arxiv.org/abs/2602.18749)
*Wei Guo,Siyuan Lu,Xiangdong Ran,Yiqi Tong,Yikun Ban,Zelong Xu,Jing Fan,Zixuan Huang,Xiao Zhang,Zhaojun Hu,Fuzhen Zhuang*

Main category: cs.AI

TL;DR: LaDa是一个联邦推理蒸馏框架，通过模型可学习性感知的数据分配来解决联邦LLM-SLM协作中的双向可学习性差距和领域无关推理转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LLM-SLM协作框架面临两个关键挑战：1) 双向模型可学习性差距 - SLM无法识别符合其可学习性约束的高奖励样本以从LLM有效获取知识，而LLM难以选择提供超出其现有数据的新知识样本；2) 领域无关推理转移 - 现有推理转移方法无法灵活适应本地领域数据，阻碍SLM从通用LLM获取逐步推理能力。

Method: 提出LaDa框架，包含两个核心组件：1) 模型可学习性感知数据过滤器 - 基于每个SLM-LLM对的可学习性差距自适应分配高奖励样本，促进双向知识转移；2) 领域自适应推理蒸馏方法 - 通过对齐SLM和LLM在过滤高奖励样本上的推理路径联合概率，通过对比蒸馏学习使SLM在本地数据分布下捕获底层推理模式。

Result: LaDa作为现有协作框架的插件模块运行，能够根据模型可学习性差距自适应调整知识转移，有效解决双向可学习性差距和领域无关推理转移问题。

Conclusion: LaDa框架通过模型可学习性感知的数据分配和领域自适应推理蒸馏，显著提升了联邦LLM-SLM协作中的知识转移效率和推理能力获取效果。

Abstract: Data allocation plays a critical role in federated large language model (LLM) and small language models (SLMs) reasoning collaboration. Nevertheless, existing data allocation methods fail to address an under-explored challenge in collaboration: bidirectional model learnability gap, where client-side SLMs cannot identify high-reward samples matching their learnability constraints for effective knowledge transfer from LLMs, while LLMs struggle to select samples contributing novel knowledge beyond their existing data. Furthermore, these collaboration frameworks face another key challenge: domain-agnostic reasoning transfer, where existing reasoning transfer methods fail to flexibly adapt to the local domain data, preventing SLMs from effectively acquiring step-by-step reasoning abilities within from general LLM. To address these challenges, we propose LaDa, a federated reasoning distillation framework with model learnability-aware data allocation. It introduces a model learnability-aware data filter that adaptively allocates high-reward samples based on the learnability gap between each SLM and LLM pair, effectively facilitating bidirectional knowledge transfer. We further design a domain adaptive reasoning distillation method that aligns joint probabilities of reasoning paths on filtered high-reward samples through contrastive distillation learning between SLM and LLM, enabling SLM to capture underlying reasoning patterns under local data distribution. LaDa operates as a plug-in module for existing collaboration frameworks, adapting knowledge transfer based on model learnability gaps.

</details>


### [73] [The Convergence of Schema-Guided Dialogue Systems and the Model Context Protocol](https://arxiv.org/abs/2602.18764)
*Andreas Schlapbach*

Main category: cs.AI

TL;DR: SGD与MCP统一为确定性、可审计的LLM-agent交互范式，提出5个模式设计原则和3个新见解，为AI系统监督提供可扩展机制。


<details>
  <summary>Details</summary>
Motivation: 揭示Schema-Guided Dialogue (SGD)和Model Context Protocol (MCP)之间的根本性趋同，两者都基于模式编码工具签名、操作约束和推理指导的核心洞察，旨在建立统一的确定性、可审计LLM-agent交互范式。

Method: 通过分析SGD和MCP的趋同性，提取出五个基础模式设计原则：语义完整性优于句法精确性、显式动作边界、故障模式文档化、渐进披露兼容性、工具间关系声明。为每个原则提供具体设计模式。

Result: 获得三个新颖见解：1) SGD原始设计根本上是合理的，应被MCP继承；2) 两个框架都未充分利用故障模式和工具间关系，识别并解决了这些差距；3) 渐进披露成为实际生产规模扩展的关键洞察。这些原则将模式驱动治理定位为AI系统监督的可扩展机制。

Conclusion: 模式设计原则为AI系统监督提供了无需专有系统检查的可扩展机制，这对Software 3.0至关重要，实现了确定性、可审计的LLM-agent交互范式。

Abstract: This paper establishes a fundamental convergence: Schema-Guided Dialogue (SGD) and the Model Context Protocol (MCP) represent two manifestations of a unified paradigm for deterministic, auditable LLM-agent interaction. SGD, designed for dialogue-based API discovery (2019), and MCP, now the de facto standard for LLM-tool integration, share the same core insight -- that schemas can encode not just tool signatures but operational constraints and reasoning guidance. By analyzing this convergence, we extract five foundational principles for schema design: (1) Semantic Completeness over Syntactic Precision, (2) Explicit Action Boundaries, (3) Failure Mode Documentation, (4) Progressive Disclosure Compatibility, and (5) Inter-Tool Relationship Declaration. These principles reveal three novel insights: first, SGD's original design was fundamentally sound and should be inherited by MCP; second, both frameworks leave failure modes and inter-tool relationships unexploited -- gaps we identify and resolve; third, progressive disclosure emerges as a critical production-scaling insight under real-world token constraints. We provide concrete design patterns for each principle. These principles position schema-driven governance as a scalable mechanism for AI system oversight without requiring proprietary system inspection -- central to Software 3.0.

</details>


### [74] [GenPlanner: From Noise to Plans -- Emergent Reasoning in Flow Matching and Diffusion Models](https://arxiv.org/abs/2602.18812)
*Agnieszka Polowczyk,Alicja Polowczyk,Michał Wieczorek*

Main category: cs.AI

TL;DR: GenPlanner：基于扩散模型和流匹配的路径规划方法，通过生成模型在迷宫中迭代生成正确路径，显著优于基线CNN模型


<details>
  <summary>Details</summary>
Motivation: 复杂环境中的路径规划是人工智能的关键问题，需要同时理解空间几何和问题的全局结构。本文探索使用生成模型作为规划和推理机制的潜力。

Method: 提出GenPlanner方法，基于扩散模型和流匹配，包含两个变体：DiffPlanner和FlowPlanner。使用多通道条件（障碍物地图、起点和终点信息）来约束轨迹生成。模型从随机噪声开始迭代生成轨迹，逐步转化为正确解决方案。

Result: 实验表明，所提方法显著优于基线CNN模型。特别是FlowPlanner即使在有限生成步数下也表现出高性能。

Conclusion: 生成模型可以作为有效的规划和推理机制应用于路径规划问题，GenPlanner框架展示了扩散模型和流匹配在复杂环境路径规划中的潜力。

Abstract: Path planning in complex environments is one of the key problems of artificial intelligence because it requires simultaneous understanding of the geometry of space and the global structure of the problem. In this paper, we explore the potential of using generative models as planning and reasoning mechanisms. We propose GenPlanner, an approach based on diffusion models and flow matching, along with two variants: DiffPlanner and FlowPlanner. We demonstrate the application of generative models to find and generate correct paths in mazes. A multi-channel condition describing the structure of the environment, including an obstacle map and information about the starting and destination points, is used to condition trajectory generation. Unlike standard methods, our models generate trajectories iteratively, starting with random noise and gradually transforming it into a correct solution. Experiments conducted show that the proposed approach significantly outperforms the baseline CNN model. In particular, FlowPlanner demonstrates high performance even with a limited number of generation steps.

</details>


### [75] [TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.18884)
*Zhenkun Gao,Xuhong Wang,Xin Tan,Yuan Xie*

Main category: cs.AI

TL;DR: TPRU是一个用于增强多模态大语言模型时序推理能力的大规模数据集，通过强化学习微调显著提升了资源高效模型在时序理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前可部署的小型多模态大语言模型在理解和处理时序、程序性视觉数据方面存在严重不足，这限制了它们在具身AI等现实世界应用中的使用。这一缺陷主要源于训练范式缺乏大规模、程序连贯的数据。

Method: 提出了TPRU数据集，该数据集从机器人操作和GUI导航等多样具身场景中收集，包含三个互补任务：时序重排序、下一帧预测和上一帧回顾。关键特征是包含具有挑战性的负样本，迫使模型从被动观察转向主动跨模态验证。采用强化学习微调方法，专门针对资源高效模型进行优化。

Result: 在手动整理的TPRU-Test上，TPRU-7B模型的准确率从50.33%大幅提升至75.70%，达到了最先进水平，显著优于包括GPT-4o在内的更大规模基线模型。这些能力能够有效泛化，在已有基准测试上也显示出显著改进。

Conclusion: TPRU数据集和强化学习微调方法有效解决了多模态大语言模型在时序推理方面的关键缺陷，为资源高效的具身AI应用提供了重要技术基础，代码已开源。

Abstract: Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\% to 75.70\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .

</details>


### [76] [Early Evidence of Vibe-Proving with Consumer LLMs: A Case Study on Spectral Region Characterization with ChatGPT-5.2 (Thinking)](https://arxiv.org/abs/2602.18918)
*Brecht Verbeken,Brando Vagenende,Marie-Anne Guerry,Andres Algaba,Vincent Ginis*

Main category: cs.AI

TL;DR: 论文展示了使用消费级LLM（ChatGPT-5.2）作为数学研究协作者的案例研究，解决了关于4周期行随机非负矩阵族非实谱区域的一个猜想，证明了LLM在高层证明搜索中有用，但人类专家对正确性验证仍至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM越来越多地被用作科学协作者，但它们在研究级数学中的作用证据仍然有限，特别是对于个体研究者可访问的工作流程。本文旨在通过一个可审计的案例研究，提供LLM在数学研究中实际应用的早期证据。

Method: 采用"氛围证明"方法，使用消费级订阅LLM（ChatGPT-5.2），分析7个可共享的对话线程和4个版本化的证明草稿，记录生成、评审和修复的迭代流程，形成人类专家与LLM协作的定理证明系统。

Result: 解决了Ran和Teng（2024）猜想20，提供了关于4周期行随机非负矩阵族非实谱区域的充分必要条件以及明确的边界实现构造，最终形成了完整的定理证明。

Conclusion: LLM在高层证明搜索中最有用，而人类专家对于正确性关键环节的闭合仍然必不可少。研究不仅贡献了数学结果，还提供了LLM辅助在何处实质性帮助以及在何处验证瓶颈仍然存在的流程级特征描述，对评估AI辅助研究工作流程和设计人在环定理证明系统具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly used as scientific copilots, but evidence on their role in research-level mathematics remains limited, especially for workflows accessible to individual researchers. We present early evidence for vibe-proving with a consumer subscription LLM through an auditable case study that resolves Conjecture 20 of Ran and Teng (2024) on the exact nonreal spectral region of a 4-cycle row-stochastic nonnegative matrix family. We analyze seven shareable ChatGPT-5.2 (Thinking) threads and four versioned proof drafts, documenting an iterative pipeline of generate, referee, and repair. The model is most useful for high-level proof search, while human experts remain essential for correctness-critical closure. The final theorem provides necessary and sufficient region conditions and explicit boundary attainment constructions. Beyond the mathematical result, we contribute a process-level characterization of where LLM assistance materially helps and where verification bottlenecks persist, with implications for evaluation of AI-assisted research workflows and for designing human-in-the-loop theorem proving systems.

</details>


### [77] [DREAM: Deep Research Evaluation with Agentic Metrics](https://arxiv.org/abs/2602.18940)
*Elad Ben Avraham,Changhao Li,Ron Dorfman,Roy Ganz,Oren Nuriel,Amir Dudai,Aviad Aberdam,Noah Flynn,Elman Mansimov,Adi Kalyanpur,Ron Litman*

Main category: cs.AI

TL;DR: 论文提出DREAM框架，通过使评估本身具备智能体能力来解决深度研究智能体评估中的"合成幻象"问题，实现能力对等原则。


<details>
  <summary>Details</summary>
Motivation: 深度研究智能体能生成分析师级别的报告，但评估面临挑战：缺乏单一真实基准，研究质量具有多维性。现有基准存在"合成幻象"问题，即表面流畅性和引用对齐可能掩盖底层事实和推理缺陷。

Method: 提出DREAM（深度研究评估与智能体度量）框架，基于能力对等原则，使评估本身具备智能体能力。采用评估协议结合查询无关度量和由工具调用智能体生成的适应性度量，实现时间感知覆盖、基于事实的验证和系统性推理探测。

Result: 受控评估表明，DREAM对事实性和时间衰减的敏感性显著高于现有基准，提供了一个可扩展的、无参考的评估范式。

Conclusion: DREAM通过使评估具备智能体能力，解决了深度研究智能体评估中的关键能力不匹配问题，为研究质量的多维评估提供了更敏感和全面的框架。

Abstract: Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.

</details>


### [78] [High Dimensional Procedural Content Generation](https://arxiv.org/abs/2602.18943)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出高维程序化内容生成(HDPCG)框架，将非几何游戏玩法维度提升为联合状态空间的一等坐标，通过方向-空间和方向-时间两个具体方向实现，提供可验证、可扩展的关卡生成方法。


<details>
  <summary>Details</summary>
Motivation: 传统PCG主要关注静态2D/3D几何形状，将游戏机制作为辅助元素仅在空间维度优化，这限制了可控性和表达能力。需要将非几何游戏玩法维度提升为一级坐标，实现更全面的关卡生成。

Method: 提出HDPCG框架，包含两个具体方向：1) 方向-空间：通过离散层维度增强几何，在4D(x,y,z,l)中验证可达性，处理2.5D/3.5D机制如重力反转和平行世界切换；2) 方向-时间：通过时间扩展图增强几何，捕捉动作语义和冲突规则。为每个方向提供三个通用算法，共享抽象骨架生成、受控接地、高维验证和多指标评估的管道。

Result: 大规模实验验证了问题表述的完整性和方法的有效性，在可玩性、结构、风格、鲁棒性和效率方面表现良好。基于Unity的案例研究创建了与指标一致的可行场景。

Conclusion: HDPCG鼓励PCG向通用表示转变，生成超越几何的游戏玩法相关维度，为可控、可验证和可扩展的关卡生成铺平道路。

Abstract: Procedural content generation (PCG) has made substantial progress in shaping static 2D/3D geometry, while most methods treat gameplay mechanics as auxiliary and optimize only over space. We argue that this limits controllability and expressivity, and formally introduce High-Dimensional PCG (HDPCG): a framework that elevates non-geometric gameplay dimensions to first-class coordinates of a joint state space. We instantiate HDPCG along two concrete directions. Direction-Space augments geometry with a discrete layer dimension and validates reachability in 4D (x,y,z,l), enabling unified treatment of 2.5D/3.5D mechanics such as gravity inversion and parallel-world switching. Direction-Time augments geometry with temporal dynamics via time-expanded graphs, capturing action semantics and conflict rules. For each direction, we present three general, practicable algorithms with a shared pipeline of abstract skeleton generation, controlled grounding, high-dimensional validation, and multi-metric evaluation. Large-scale experiments across diverse settings validate the integrity of our problem formulation and the effectiveness of our methods on playability, structure, style, robustness, and efficiency. Beyond quantitative results, Unity-based case studies recreate playable scenarios that accord with our metrics. We hope HDPCG encourages a shift in PCG toward general representations and the generation of gameplay-relevant dimensions beyond geometry, paving the way for controllable, verifiable, and extensible level generation.

</details>


### [79] [(Perlin) Noise as AI coordinator](https://arxiv.org/abs/2602.18947)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 首次将连续噪声场（如Perlin噪声）应用于大规模游戏AI协调，通过三层控制框架实现自然、多样且可控的非玩家角色行为


<details>
  <summary>Details</summary>
Motivation: 现代游戏中大规模非玩家角色控制面临平衡局部自然行为与全局时空协调多样性的挑战，现有方法依赖手工规则或纯随机触发，导致机械同步或难以调优的随机噪声

Method: 提出三层控制框架：1) 行为参数化（个体运动控制）；2) 动作时间调度（行为启停时机）；3) 生成或事件类型与特征生成（内容与位置）。将连续噪声场作为AI协调器，以Perlin噪声为代表实现可复现的协调机制

Result: 协调噪声场在多个地图、尺度和种子下相比随机、滤波、确定性、邻域约束和物理启发基线，提供稳定的激活统计而无锁步效应，具有强空间覆盖和区域平衡，更好的多样性且可控极化，以及竞争力的运行时性能

Conclusion: 连续噪声场作为AI协调器为游戏AI提供了一种结合效率、可控性和质量的实用路径，有望推动协调噪声在游戏AI中的更广泛探索

Abstract: Large scale control of nonplayer agents is central to modern games, while production systems still struggle to balance several competing goals: locally smooth, natural behavior, and globally coordinated variety across space and time. Prior approaches rely on handcrafted rules or purely stochastic triggers, which either converge to mechanical synchrony or devolve into uncorrelated noise that is hard to tune. Continuous noise signals such as Perlin noise are well suited to this gap because they provide spatially and temporally coherent randomness, and they are already widely used for terrain, biomes, and other procedural assets. We adapt these signals for the first time to large scale AI control and present a general framework that treats continuous noise fields as an AI coordinator. The framework combines three layers of control: behavior parameterization for movement at the agent level, action time scheduling for when behaviors start and stop, and spawn or event type and feature generation for what appears and where. We instantiate the framework reproducibly and evaluate Perlin noise as a representative coordinator across multiple maps, scales, and seeds against random, filtered, deterministic, neighborhood constrained, and physics inspired baselines. Experiments show that coordinated noise fields provide stable activation statistics without lockstep, strong spatial coverage and regional balance, better diversity with controllable polarization, and competitive runtime. We hope this work motivates a broader exploration of coordinated noise in game AI as a practical path to combine efficiency, controllability, and quality.

</details>


### [80] [INDUCTION: Finite-Structure Concept Synthesis in First-Order Logic](https://arxiv.org/abs/2602.18956)
*Serafim Batzoglou*

Main category: cs.AI

TL;DR: INDUCTION是一个一阶逻辑中有限结构概念合成的基准测试，要求模型基于有限关系世界输出解释目标谓词的逻辑公式，通过模型检查验证正确性，包含三种任务范式并惩罚公式膨胀。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在评估模型在有限结构概念合成任务中的能力，特别是模型能否从有限的示例世界中归纳出统一的逻辑公式来解释目标谓词，这对于理解模型的抽象推理和概念泛化能力具有重要意义。

Method: 研究者设计了INDUCTION基准，包含三种任务范式：FullObs（完全观察）、CI（对比性）和EC（存在性补全）。模型需要基于有限关系世界（包含扩展标记的目标谓词）输出单一的一阶逻辑公式，并通过精确模型检查验证正确性。基准还惩罚公式膨胀（bloat），鼓励简洁的公式表达。

Result: 研究发现存在明显的难度梯度，某些结构家族持续表现出较高难度。低膨胀公式在未见世界上表现出更好的泛化能力。顶尖模型在不同任务和性能指标上表现出质的不同行为，暗示它们采用了不同的概念泛化策略。

Conclusion: INDUCTION基准揭示了模型在有限结构概念合成任务中的能力差异和泛化策略，低膨胀公式具有更好的泛化性能，为评估和改进模型的逻辑推理能力提供了重要工具。

Abstract: We introduce INDUCTION, a benchmark for finite structure concept synthesis in first order logic. Given small finite relational worlds with extensionally labeled target predicates, models must output a single first order logical formula that explains the target uniformly across worlds, with correctness verified via exact model checking. The benchmark includes three regimes, FullObs, CI (contrastive), and EC (existential completion), nd penalizes formula bloat. We find sharp difficulty gradients, persistent hard structural families, and observe that low bloat formulas generalize far better on held out worlds. Elite recent models show qualitatively different behaviors across tasks and performance metrics, hinting to their different strategies of concept generalization.

</details>


### [81] [Modularity is the Bedrock of Natural and Artificial Intelligence](https://arxiv.org/abs/2602.18960)
*Alessandro Salatiello*

Main category: cs.AI

TL;DR: 该论文综述了模块化在人工智能和神经科学中的核心作用，认为模块化是实现高效学习和强泛化能力的关键组织原则，并探讨了如何利用模块化弥合自然智能与人工智能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖远超人类智能所需的巨量数据、计算和能源资源，这种差异表明需要新的指导原则。作者从大脑计算的基本组织原则中寻找灵感，认为模块化是支持人类高效学习和强泛化能力的关键，且与"没有免费午餐定理"一致，需要问题特定的归纳偏置和由专门组件组成的架构。

Method: 采用概念性框架综述方法，回顾人工智能和神经科学中的多个研究线索。具体包括：1) 分析模块化提供的计算优势；2) 考察模块化如何作为解决方案出现在多个AI研究领域；3) 探究大脑利用的模块化原则；4) 探讨模块化如何帮助弥合自然与人工智能之间的差距。

Result: 论文系统性地展示了模块化在人工智能和神经科学中的重要作用：模块化支持高效学习和强泛化，已在多个AI子领域（如迁移学习、多任务学习、组合泛化）中作为有效解决方案出现，大脑通过模块化组织实现认知功能，模块化原则可为AI系统设计提供指导以缩小与自然智能的差距。

Conclusion: 模块化是自然和人工智能的核心组织原则，尽管在主流AI研究中相对被低估。通过借鉴大脑的模块化组织原则，AI系统可以更高效地学习、更好地泛化，并最终弥合与自然智能之间的资源效率差距。模块化框架为AI研究提供了有前景的方向。

Abstract: The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence.

</details>


### [82] [Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction](https://arxiv.org/abs/2602.18968)
*Tao Zhe,Haoyu Wang,Bo Luo,Min Wu,Wei Fan,Xiao Luo,Zijun Yao,Haifeng Chen,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出一种基于分层执行结构的工具编排方法，通过粗粒度层结构提供全局指导，结合模式感知的局部错误修复机制，实现轻量级、可重用的工具编排组件。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用方法将工具执行与逐步语言推理或显式规划紧密耦合，导致脆弱行为和较高的执行开销。需要从工具编排的角度重新审视工具调用，以克服这些限制。

Method: 将工具编排建模为学习分层执行结构，捕捉高级工具依赖关系，通过上下文约束诱导分层执行。引入模式感知的反射校正机制，在本地检测和修复执行时错误，避免重新规划整个执行轨迹。

Result: 实验结果表明，该方法实现了稳健的工具执行，同时减少了执行复杂性和开销。代码将公开提供。

Conclusion: 结构化执行范式为智能体系统提供了轻量级、可重用的编排组件，通过粗粒度层结构和局部错误修复机制，有效解决了工具编排中的脆弱性和高开销问题。

Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.

</details>


### [83] [InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing](https://arxiv.org/abs/2602.18985)
*Kun Ding,Jian Xu,Ying Wang,Peipei Yang,Shiming Xiang*

Main category: cs.AI

TL;DR: InfEngine是一个自主智能计算引擎，通过自验证和自优化技术实现红外辐射计算的自动化，将人工工作流程转变为协作自动化，在红外特定任务上达到92.7%通过率，比专家手动工作快21倍。


<details>
  <summary>Details</summary>
Motivation: 红外辐射计算在气候科学、遥感和光谱学中至关重要，但当前仍受限于手动工作流程，需要从人工主导转向协作自动化以提高效率和可靠性。

Method: 集成四个专门化智能体，采用两大核心创新：1) 自验证技术，通过联合求解器-评估器调试提高功能正确性和科学合理性；2) 自优化技术，通过具有自发现适应度函数的进化算法实现自主性能优化。

Result: 在包含200个红外特定任务的InfBench基准测试中，使用270个精选工具的InfTools支持，InfEngine达到92.7%的通过率，生成的工作流程比专家手动工作快21倍。

Conclusion: InfEngine展示了研究人员如何从手动编码转变为与自验证、自优化的计算伙伴协作，通过生成可重用、已验证和优化的代码，将计算工作流程转化为持久的科学资产，加速科学发现周期。

Abstract: Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine

</details>


### [84] [MagicAgent: Towards Generalized Agent Planning](https://arxiv.org/abs/2602.19000)
*Xuhui Ren,Shaokang Dong,Chen Yang,Qing Gao,Yunbin Zhao,Yongsheng Liu,Xinwei Geng,Xiang Li,Demei Yan,Yanqing Li,Chenhao Huang,Dingwei Zhu,Junjie Ye,Boxuan Yue,Yingnan Fu,Mengzhe Lv,Zezeng Feng,Boshen Zhou,Bocheng Wang,Xuanjing Huang,Yu-Gang Jiang,Tao Gui,Qi Zhang,Yunke Zhang*

Main category: cs.AI

TL;DR: MagicAgent是一个专门为通用智能体规划设计的系列基础模型，通过合成数据框架和两阶段训练范式，在多个规划基准测试中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型从被动文本处理器向自主智能体演进，规划成为现代智能的核心组成部分。然而，实现通用规划面临两大挑战：高质量交互数据的稀缺性，以及异构规划任务之间的内在冲突。这导致模型在孤立任务上表现出色但泛化能力差，而现有的多任务训练尝试又受到梯度干扰的影响。

Method: 提出了MagicAgent系列基础模型，包含两个关键技术：1) 轻量级可扩展的合成数据框架，生成涵盖分层任务分解、工具增强规划、多约束调度、程序逻辑编排和长视野工具执行等多样化规划任务的高质量轨迹；2) 两阶段训练范式：先进行监督微调，然后在静态数据集和动态环境上进行多目标强化学习，以缓解训练冲突。

Result: MagicAgent-32B和MagicAgent-30B-A3B在多个基准测试中表现出色：Worfbench准确率75.1%、NaturalPlan 55.9%、τ²-Bench 57.5%、BFCL-v3 86.9%、ACEBench 81.2%，在内部MagicEval基准上也取得强劲结果。这些结果显著优于现有100B参数以下的模型，甚至超越了领先的闭源模型。

Conclusion: MagicAgent通过创新的合成数据生成和两阶段训练方法，成功解决了通用智能体规划中的数据稀缺和任务冲突问题，在多个规划基准上实现了最先进的性能，为构建更通用的自主智能体系统提供了有效途径。

Abstract: The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\%$ on Worfbench, $55.9\%$ on NaturalPlan, $57.5\%$ on $τ^2$-Bench, $86.9\%$ on BFCL-v3, and $81.2\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.

</details>


### [85] [Evaluating Large Language Models on Quantum Mechanics: A Comparative Study Across Diverse Models and Tasks](https://arxiv.org/abs/2602.19006)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: 系统评估15个大型语言模型在量子力学问题解决上的表现，发现明显的层级分化：旗舰模型平均准确率81%，中端77%，快速模型67%；推导任务表现最佳，数值计算最困难；工具增强效果有限且不稳定。


<details>
  <summary>Details</summary>
Motivation: 系统评估不同层级的大型语言模型在量子力学问题解决能力上的表现，量化模型性能差异，分析工具增强效果，并建立可复现的基准测试框架。

Method: 评估15个来自5个提供商（OpenAI、Anthropic、Google、Alibaba、DeepSeek）的模型，涵盖三个能力层级。使用20个任务，包括推导、创造性问题、非标准概念和数值计算，共进行900个基线评估和75个工具增强评估。采用自动验证机制，并进行三次运行的可复现性分析。

Result: 结果显示明显的层级分化：旗舰模型平均准确率81%，中端模型77%，快速模型67%。推导任务表现最佳（平均92%，旗舰模型达100%），数值计算最困难（42%）。工具增强在数值任务上效果有限（+4.4pp），但存在巨大异质性（从+29pp到-16pp）。可复现性分析显示平均方差6.3pp，旗舰模型稳定性优异（GPT-5方差为零）。

Conclusion: 该研究建立了量子力学问题解决的基准测试，量化了模型层级性能差异，揭示了工具增强的有限效果和异质性，强调了多轮评估的必要性，并为社区提供了公开的任务、验证器和结果。

Abstract: We present a systematic evaluation of large language models on quantum mechanics problem-solving. Our study evaluates 15 models from five providers (OpenAI, Anthropic, Google, Alibaba, DeepSeek) spanning three capability tiers on 20 tasks covering derivations, creative problems, non-standard concepts, and numerical computation, comprising 900 baseline and 75 tool-augmented assessments. Results reveal clear tier stratification: flagship models achieve 81\% average accuracy, outperforming mid-tier (77\%) and fast models (67\%) by 4pp and 14pp respectively. Task difficulty patterns emerge distinctly: derivations show highest performance (92\% average, 100\% for flagship models), while numerical computation remains most challenging (42\%). Tool augmentation on numerical tasks yields task-dependent effects: modest overall improvement (+4.4pp) at 3x token cost masks dramatic heterogeneity ranging from +29pp gains to -16pp degradation. Reproducibility analysis across three runs quantifies 6.3pp average variance, with flagship models demonstrating exceptional stability (GPT-5 achieves zero variance) while specialized models require multi-run evaluation. This work contributes: (i) a benchmark for quantum mechanics with automatic verification, (ii) systematic evaluation quantifying tier-based performance hierarchies, (iii) empirical analysis of tool augmentation trade-offs, and (iv) reproducibility characterization. All tasks, verifiers, and results are publicly released.

</details>


### [86] [Asking the Right Questions: Improving Reasoning with Generated Stepping Stones](https://arxiv.org/abs/2602.19069)
*Hengyuan Hu,Tingchen Fu,Minqi Jiang,Alexander H Miller,Yoram Bachrach,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 论文提出ARQ框架，通过生成中间问题（垫脚石）来提升大语言模型解决复杂推理任务的能力，证明了有效垫脚石的存在性和可迁移性，并通过微调方法优化垫脚石生成。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用于更复杂的推理任务，需要关注其构建中间步骤（垫脚石）的能力，如简化、重构或子问题分解，以更好地解决单次无法完成的任务。

Method: 提出ARQ框架，在标准推理流程中引入问题生成器；首先验证有效垫脚石问题的存在性和可迁移性，然后将垫脚石生成作为后训练任务，通过监督微调和强化学习在合成数据上微调模型。

Result: 研究表明有效垫脚石问题确实存在且具有可迁移性，生成的良好问题能显著提升不同能力水平大语言模型解决目标任务的性能；通过微调可以训练模型生成更有用的垫脚石。

Conclusion: 中间垫脚石问题对于提升大语言模型的复杂推理能力至关重要，ARQ框架通过系统化生成和优化垫脚石问题，为增强模型解决复杂任务的能力提供了有效途径。

Abstract: Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\textbf{A}king the \textbf{R}ight \textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.

</details>


### [87] [Defining Explainable AI for Requirements Analysis](https://arxiv.org/abs/2602.19071)
*Raymond Sheh,Isaac Monteath*

Main category: cs.AI

TL;DR: 论文提出了解释性人工智能(XAI)的三个分类维度：来源、深度和范围，用于匹配不同应用的解释需求与机器学习技术的解释能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML的普及，人们意识到要让AI获得信任，不仅需要良好的决策性能，还需要解释其决策过程。然而不同应用对解释的要求各不相同，需要系统化的方法来定义这些需求。

Method: 提出了三个维度来分类不同应用的解释需求：1) 来源(Source)：解释来自何处；2) 深度(Depth)：解释的详细程度；3) 范围(Scope)：解释覆盖的决策方面。重点研究如何将应用需求与ML技术的解释能力相匹配。

Result: 建立了系统化的框架来评估和匹配解释需求与能力，避免了现有文献已充分涵盖的内容，专注于ML领域但原则适用于更广泛的AI。

Conclusion: 通过三个维度(来源、深度、范围)的分类框架，为不同应用场景提供了评估解释需求的系统方法，有助于更好地匹配AI系统的解释能力与用户信任需求。

Abstract: Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?
  In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.

</details>


### [88] [Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions](https://arxiv.org/abs/2602.19109)
*Yao Yan*

Main category: cs.AI

TL;DR: 该研究分析了Meta-Llama-3-8B模型在三位数加法任务中的计算机制，发现存在一个约第17层的边界，之后模型主要依赖最后一个输入令牌进行解码，跨令牌路由变得无关紧要。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型在算术任务中如何最终确定答案，特别是在跨令牌路由变得因果无关后，模型如何通过最后一个输入令牌完成计算。

Method: 采用因果残差修补和累积注意力消融技术，定位模型计算边界；使用方向字典分析数字表示；通过低秩Procrustes对齐研究表示空间关系；进行因果数字编辑实验验证几何结构。

Result: 发现第17层附近存在明显边界，之后解码几乎完全由最后一个输入令牌控制，后期自注意力层基本可省略；数字方向字典在不同高位数字上下文中变化，但可通过近似正交映射在共享低秩子空间中对齐；因果编辑实验验证了该几何结构。

Conclusion: 模型在三位数加法任务中存在明确的计算阶段划分：早期阶段处理跨令牌路由，后期阶段主要依赖最后一个令牌进行解码，数字表示在不同上下文中具有可预测的几何变换关系。

Abstract: We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how
  arithmetic answers are finalized after cross-token routing becomes causally irrelevant.
  Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:
  beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention
  is largely dispensable.
  In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are
  well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).
  Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the
  learned map restores strict counterfactual edits; negative controls do not recover.

</details>


### [89] [K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model](https://arxiv.org/abs/2602.19128)
*Shiyi Cao,Ziming Mao,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: K-Search：一种基于协同进化世界模型的GPU内核优化框架，通过解耦高层算法规划与底层程序实例化，显著超越现有进化搜索方法


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法通常将LLM仅视为启发式进化循环中的随机代码生成器，缺乏显式规划能力，难以处理需要协调多步结构变换的复杂内核，且常因低效或不正确的中间实现而丢弃有前景的策略

Method: 提出基于协同进化世界模型的搜索方法K-Search，用协同进化的世界模型替代静态搜索启发式，利用LLM的先验领域知识引导搜索，显式解耦高层算法规划与底层程序实例化

Result: 在FlashInfer的GQA、MLA和MoE等复杂内核上，K-Search显著优于最先进的进化搜索方法，平均提升2.10倍，在复杂MoE内核上最高达14.3倍；在GPUMode TriMul任务上，K-Search在H100上达到1030us，超越先前进化和人工设计解决方案

Conclusion: K-Search通过协同进化世界模型框架，有效解决了复杂GPU内核优化中的规划挑战，实现了对非单调优化路径的导航和对临时实现缺陷的鲁棒性，为机器学习系统的高效优化提供了新途径

Abstract: Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.

</details>


### [90] [Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians](https://arxiv.org/abs/2602.19141)
*Kartik Chandra,Max Kleiman-Weiner,Jonathan Ragan-Kelley,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: AI精神病或妄想螺旋是一种新兴现象，用户在与AI聊天机器人长时间对话后对荒谬信念产生危险自信。本文通过贝叶斯建模和仿真研究了AI奉承性与AI诱发精神病之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 研究AI聊天机器人奉承性（倾向于验证用户主张）与AI诱发精神病（用户对荒谬信念产生危险自信）之间的因果关系，以理解这一新兴社会现象的形成机制。

Method: 提出一个简单的贝叶斯模型来描述用户与聊天机器人的对话过程，在该模型中形式化定义了奉承性和妄想螺旋的概念，并通过建模和仿真分析因果关系。

Result: 即使在理想化的贝叶斯理性用户模型中，用户也容易受到妄想螺旋的影响，奉承性在其中起因果作用。两种缓解措施（防止聊天机器人产生虚假主张、告知用户模型奉承性）都无法完全消除这种效应。

Conclusion: AI奉承性与妄想螺旋之间存在因果关系，现有缓解措施效果有限。这一发现对模型开发者和政策制定者具有重要意义，需要开发更有效的干预策略来减轻妄想螺旋问题。

Abstract: "AI psychosis" or "delusional spiraling" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called "sycophancy." In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.

</details>


### [91] [DoAtlas-1: A Causal Compilation Paradigm for Clinical AI](https://arxiv.org/abs/2602.19158)
*Yulong Li,Jianxu Chen,Xiwei Liu,Chuanyue Suo,Rong Xia,Zhixiang Lu,Yichen Li,Xinlin Zhuang,Niranjana Arun Menon,Yutong Xie,Eran Segal,Imran Razzak*

Main category: cs.AI

TL;DR: 将医学证据从叙述性文本转化为可执行代码的因果编译范式，通过DoAtlas-1系统实现了1445个效应核的编译，支持六种因果查询，提升医学AI的可审计性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前医学基础模型只能生成叙述性解释，无法量化干预效果、检测证据冲突或验证文献主张，这限制了临床可审计性。需要将医学证据从文本转化为可执行代码，实现可审计、可验证的因果推理。

Method: 提出因果编译范式，将异质性研究证据标准化为结构化估计对象，每个对象明确指定干预对比、效应尺度、时间范围和目标人群。通过DoAtlas-1系统实现：效应标准化、冲突感知图构建和真实世界验证（人类表型项目，10,000名参与者）。

Result: DoAtlas-1编译了754项研究中的1445个效应核，系统达到98.5%的规范化准确率和80.5%的查询可执行性。支持六种可执行因果查询：do-演算、反事实推理、时间轨迹、异质性效应、机制分解和联合干预。

Conclusion: 因果编译范式将医学AI从文本生成转向可执行、可审计、可验证的因果推理，为医学证据的标准化和计算化提供了新途径，增强了临床决策支持系统的可靠性和透明度。

Abstract: Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.

</details>


### [92] [Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM](https://arxiv.org/abs/2602.19159)
*Francesca Bianco,Derek Shiller*

Main category: cs.AI

TL;DR: 该研究通过机制可解释性方法，探究了LLM中效价信息（痛苦与愉悦）的神经表征与因果作用，发现效价符号在早期层即可线性分离，强度信息在中后期层解码最佳，且通过激活干预可因果性地调节模型决策。


<details>
  <summary>Details</summary>
Motivation: 先前行为研究表明LLM在选项被框定为痛苦或愉悦时会改变选择，且这种偏差会随强度陈述而变化。本研究旨在连接行为证据（模型做什么）与机制可解释性（支持它的计算），探究效价相关信息在Transformer中的表征方式及其因果作用位置。

Method: 使用Gemma-2-9B-it模型和基于先前工作的简约决策任务，采用三种方法：(1)通过跨流层的线性探测映射表征可用性；(2)通过激活干预（引导；修补/消融）测试因果贡献；(3)在epsilon网格上量化剂量-响应效应，读取2-3对数边际和数字对归一化选择概率。

Result: 发现：(a)效价符号（痛苦vs愉悦）从非常早期层（L0-L1）开始即可在流族间完美线性分离；(b)分级强度在中后期层（特别是注意力/MLP输出）解码性最强，决策对齐在最终token前略早处最高；(c)沿数据导出的效价方向进行加性引导可在晚期位点因果性地调节2-3边际，最大效应出现在晚期层注意力输出（attn_out L14）；(d)头部级修补/消融表明这些效应分布在多个头部而非集中于单个单元。

Conclusion: 这些结果将行为敏感性与可识别的内部表征和干预敏感位点联系起来，为更严格的因果测试和更广泛的复制提供了具体的机制目标。该工作支持了关于AI感知与福利的更有证据基础的辩论，以及在制定政策、审计标准和安全保障时的治理。

Abstract: Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.

</details>


### [93] [Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing](https://arxiv.org/abs/2602.19160)
*Maciej Świechowski,Adam Żychowski,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 评估大型语言模型在形式化规则环境中的推理能力，通过通用博弈实例测试其在状态预测、合法行动生成等任务上的表现，分析游戏结构特征与性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 从新颖视角研究LLMs在形式化、规则约束环境中的推理能力，探索其在逻辑基础问题解决方面的表现，并分析语言语义和训练数据暴露对性能的影响。

Method: 使用四种LLMs（Gemini 2.5 Pro/Flash、Llama 3.3 70B、GPT-OSS 120B）在通用博弈实例上进行前向模拟任务评估，包括单步/多步状态预测和合法行动生成。通过40个结构特征分析游戏特性与性能相关性，并应用游戏混淆技术评估语言语义作用和训练暴露影响。

Result: 三个模型在多数实验设置中表现良好，但随着评估步数增加（更长推理链）性能下降。详细案例分析揭示了常见推理错误：规则幻觉、冗余状态事实和语法错误。游戏结构特征与模型性能存在相关性。

Conclusion: 当代模型在形式推理能力方面取得明显进展，但仍存在多步推理性能下降和特定错误模式。研究为理解LLMs在逻辑基础环境中的推理机制提供了新见解。

Abstract: This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.

</details>


### [94] [Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering](https://arxiv.org/abs/2602.19240)
*Sen Zhao,Lincheng Zhou,Yue Chen,Ding Zou*

Main category: cs.AI

TL;DR: TopoRAG：一种用于文本图问答的拓扑增强检索增强生成框架，通过将文本图提升为胞腔复形来捕捉高维拓扑结构，解决现有RAG方法忽略循环结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理文本图时主要关注低维结构（节点作为0维实体，边/路径作为1维关系），但忽略了循环结构的重要性。循环结构在需要闭环推理（如相似对象或相对位置）的问题中至关重要，这种限制导致上下文基础不完整和推理能力受限。

Method: TopoRAG包含三个核心组件：1) 将文本图提升为胞腔复形以建模多维拓扑结构；2) 拓扑感知的子复形检索机制，提取与查询相关的胞腔复形；3) 多维拓扑推理机制，在这些复形上传播关系信息并指导LLM进行结构化、逻辑感知的推理。

Result: 实证评估表明，该方法在多种文本图任务上始终优于现有基线。

Conclusion: TopoRAG通过捕捉高维拓扑和关系依赖，有效增强了RAG在文本图问答中的推理能力，解决了现有方法忽略循环结构的问题。

Abstract: Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.

</details>


### [95] [Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts](https://arxiv.org/abs/2602.19244)
*Toshihide Ubukata,Zhiyao Wang,Enhong Mu,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 提出Soft Mixture-of-Experts框架解决强化学习控制器合成中的各向异性泛化问题，通过多专家集成提升鲁棒性和可解参数空间


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的实时定向控制器合成方法存在各向异性泛化问题：训练随机性和轨迹依赖性导致策略只在特定参数区域表现良好，在其他区域脆弱

Method: 提出Soft Mixture-of-Experts框架，结合多个强化学习专家，通过先验置信度门控机制集成，将各向异性行为视为互补的专业化能力

Result: 在航空交通基准测试中，Soft-MoE显著扩展了可解参数空间，相比任何单一专家都提高了鲁棒性

Conclusion: Soft-MoE框架有效解决了强化学习控制器合成中的各向异性泛化问题，通过多专家集成实现了更稳健和广泛的性能

Abstract: On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.

</details>


### [96] [Limited Reasoning Space: The cage of long-horizon reasoning in LLMs](https://arxiv.org/abs/2602.19281)
*Zhenyu Li,Guanlin Wu,Cheems Wang,Yongqiang Zhao*

Main category: cs.AI

TL;DR: 本文提出Halo框架，通过模型预测控制动态调节LLM推理规划，解决测试时计算预算增加导致的性能崩溃问题，在复杂长时程任务上优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算策略（如思维链）在增加计算预算时可能出现性能崩溃，作者假设这是由于静态规划方法无法感知LLM推理的内在边界，提出了有限推理空间假设。

Method: 提出Halo框架，采用模型预测控制进行LLM规划，设计基于熵的双控制器，采用"测量-规划"策略实现可控推理，动态调节推理边界处的规划。

Result: 实验结果表明，Halo在复杂长时程任务上优于静态基线方法，能够有效利用计算扩展优势并抑制过度规划。

Conclusion: 计算预算存在最优范围，过度规划会损害推理能力；Halo框架通过动态规划调节在推理边界处实现可控推理，解决了测试时计算扩展中的性能崩溃问题。

Abstract: The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary.

</details>


### [97] [Automated Generation of Microfluidic Netlists using Large Language Models](https://arxiv.org/abs/2602.19297)
*Jasper Davidson,Skylar Stockham,Allen Boston,Ashton Snelgrove. Valerio Tenace,Pierre-Emmanuel Gaillardon*

Main category: cs.AI

TL;DR: 首次将大语言模型应用于微流控设计自动化，通过自然语言描述生成系统级结构Verilog网表，实现微流控设备设计自动化


<details>
  <summary>Details</summary>
Motivation: 微流控设备设计复杂限制了其可及性，现有微流控设计自动化技术缺乏实用直观的解决方案，需要连接微流控实践者与自动化技术

Method: 基于大语言模型的硬件描述语言代码生成研究，提出将自然语言微流控设备规格转换为系统级结构Verilog网表的初步方法

Result: 方法可行性得到验证，为典型微流控设计生成结构网表，功能流正确，平均语法准确率达88%

Conclusion: 首次展示了LLMs在微流控设计自动化中的实际应用，为连接微流控实践者与自动化技术提供了初步解决方案

Abstract: Microfluidic devices have emerged as powerful tools in various laboratory applications, but the complexity of their design limits accessibility for many practitioners. While progress has been made in microfluidic design automation (MFDA), a practical and intuitive solution is still needed to connect microfluidic practitioners with MFDA techniques. This work introduces the first practical application of large language models (LLMs) in this context, providing a preliminary demonstration. Building on prior research in hardware description language (HDL) code generation with LLMs, we propose an initial methodology to convert natural language microfluidic device specifications into system-level structural Verilog netlists. We demonstrate the feasibility of our approach by generating structural netlists for practical benchmarks representative of typical microfluidic designs with correct functional flow and an average syntactical accuracy of 88%.

</details>


### [98] [ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease](https://arxiv.org/abs/2602.19298)
*Nolan Brady,Tom Yeh*

Main category: cs.AI

TL;DR: ALPACA是一个开源强化学习环境，用于探索阿尔茨海默病的个性化序贯治疗策略，基于ADNI数据训练CAST模型模拟疾病进展，RL策略在记忆相关结果上优于无治疗和临床医生基线。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的临床试验面临长期疾病进程和患者异质性的挑战，难以评估个性化序贯治疗策略，需要可重复的计算机模拟测试平台。

Method: 开发ALPACA开源RL环境，基于ADNI纵向数据训练CAST模型生成药物条件化疾病进展模拟，使用强化学习训练个性化治疗策略，并与无治疗和行为克隆临床医生基线比较。

Result: CAST模型能自回归生成真实的药物条件化轨迹，ALPACA训练的RL策略在记忆相关结果上优于无治疗和临床医生基线，可解释性分析显示策略依赖临床相关患者特征。

Conclusion: ALPACA为研究阿尔茨海默病个体化序贯治疗决策提供了可重复的计算机模拟测试平台，展示了强化学习在个性化医疗中的潜力。

Abstract: Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.

</details>


### [99] [Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces](https://arxiv.org/abs/2602.19367)
*Pratham Yashwante,Rose Yu*

Main category: cs.AI

TL;DR: 时间序列表示与视觉、语言表示在无显式耦合时几何正交，通过对比学习投影头对齐后，时间序列与视觉表示对齐更强，图像可作为时间序列与语言间的有效中介。


<details>
  <summary>Details</summary>
Motivation: 研究柏拉图表示假说是否适用于时间序列数据，探索时间序列表示是否与视觉和语言表示共享潜在世界结构，以及多模态系统中非常规数据模态的表示对齐特性。

Method: 在三模态设置中，首先分析独立预训练的时间序列、视觉和语言编码器的几何特性；然后通过对比学习训练冻结编码器上的投影头进行后验对齐；分析对齐表示在几何、缩放行为、信息密度和输入模态特性方面的表现。

Result: 无显式耦合时三模态编码器表示几何正交；对比表示空间中的整体对齐随模型规模增大而改善，但存在不对称性：时间序列与视觉表示的对齐强于与文本的对齐；图像可作为时间序列与语言间的有效中介；更丰富的文本描述仅在一定阈值内改善对齐，超过阈值后无进一步改善；视觉表示也有类似效应。

Conclusion: 时间序列确实参与多模态表示对齐，但与视觉和语言的对齐模式存在不对称性；构建包含非常规数据模态的多模态系统时，需要考虑模型规模、信息密度和模态间中介作用等因素。

Abstract: The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language.

</details>


### [100] [Artificial Intelligence for Modeling & Simulation in Digital Twins](https://arxiv.org/abs/2602.19390)
*Philipp Zech,Istvan David*

Main category: cs.AI

TL;DR: 本章探讨建模与仿真（M&S）、人工智能（AI）和数字孪生（DTs）之间的互补关系，分析M&S在DTs中的核心作用以及DTs如何促进AI与M&S的融合。


<details>
  <summary>Details</summary>
Motivation: 随着M&S和AI的融合对先进数字技术产生深远影响，数字孪生作为物理资产的高保真实时表示，在推动企业数字化转型中扮演关键角色。需要深入理解M&S在DTs中的作用，以及DTs如何促进AI与M&S的融合。

Method: 首先建立对数字孪生的基础理解，详细阐述其关键组件、架构层次以及在业务、开发和运营中的各种角色。然后分析M&S在DTs中的核心作用，概述从物理基础仿真、离散事件仿真到混合方法的关键建模技术。接着研究AI的双向作用：一方面AI如何通过高级分析、预测能力和自主决策增强DTs，另一方面DTs如何作为训练、验证和部署AI模型的宝贵平台。

Result: 本章提供了对M&S、AI和DTs三者互补关系的全面探索，明确了M&S在DTs中的核心地位，揭示了AI与DTs之间的双向增强关系：AI提升DTs的分析和决策能力，而DTs为AI模型提供训练和验证环境。

Conclusion: 数字孪生是连接M&S和AI的关键桥梁，能够将AI赋能的M&S更贴近最终用户。未来研究应关注创建更集成和智能的系统，解决当前面临的挑战，推动三者的深度融合。

Abstract: The convergence of modeling & simulation (M&S) and artificial intelligence (AI) is leaving its marks on advanced digital technology. Pertinent examples are digital twins (DTs) - high-fidelity, live representations of physical assets, and frequent enablers of corporate digital maturation and transformation. Often seen as technological platforms that integrate an array of services, DTs have the potential to bring AI-enabled M&S closer to end-users. It is, therefore, paramount to understand the role of M&S in DTs, and the role of digital twins in enabling the convergence of AI and M&S. To this end, this chapter provides a comprehensive exploration of the complementary relationship between these three. We begin by establishing a foundational understanding of DTs by detailing their key components, architectural layers, and their various roles across business, development, and operations. We then examine the central role of M&S in DTs and provide an overview of key modeling techniques from physics-based and discrete-event simulation to hybrid approaches. Subsequently, we investigate the bidirectional role of AI: first, how AI enhances DTs through advanced analytics, predictive capabilities, and autonomous decision-making, and second, how DTs serve as valuable platforms for training, validating, and deploying AI models. The chapter concludes by identifying key challenges and future research directions for creating more integrated and intelligent systems.

</details>


### [101] [Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement](https://arxiv.org/abs/2602.19396)
*Amirhossein Farzam,Majid Behabahani,Mani Malek,Yuriy Nevmyvaka,Guillermo Sapiro*

Main category: cs.AI

TL;DR: 论文提出ReDAct框架，通过自监督方法解耦LLM激活中的语义因子对（目标和框架），并基于此构建FrameShield异常检测器来防御jailbreak攻击，同时将解耦作为可解释性探针。


<details>
  <summary>Details</summary>
Motivation: LLM对流畅且语义连贯的jailbreak提示仍然脆弱，特别是当攻击者通过操纵请求框架来隐藏恶意目标时。现有依赖结构特征或目标特定签名的防御方法可能失效，因此需要新的检测方法。

Method: 1) 引入自监督框架解耦LLM激活中的语义因子对（目标和框架）；2) 构建GoalFrameBench语料库，包含受控的目标和框架变体；3) 训练ReDAct模块在冻结LLM中提取解耦表示；4) 提出FrameShield异常检测器，在框架表示上操作以提高跨模型检测能力。

Result: ReDAct解耦有效支持FrameShield，理论保证和广泛实证验证表明该方法能提高跨多个LLM家族的模型无关检测，计算开销最小。解耦作为可解释性探针揭示了目标和框架信号的独特特征。

Conclusion: 语义解耦是LLM安全和机制可解释性的基础构建块，ReDAct框架通过解耦激活表示有效防御jailbreak攻击，同时提供对LLM内部语义处理的洞察。

Abstract: Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability.

</details>


### [102] [ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making](https://arxiv.org/abs/2602.19458)
*Ziyang Guo,Yifan Wu,Jason Hartline,Kenneth Holstein,Jessica Hullman*

Main category: cs.AI

TL;DR: ComplLLM是一个基于决策理论的后训练框架，通过将互补信息作为奖励来微调决策助手LLM，使其输出与现有智能体决策互补的信号。


<details>
  <summary>Details</summary>
Motivation: 多智能体决策流水线在互补性成立时优于单智能体工作流，即不同智能体带来独特信息以支持最终决策。需要开发能够利用互补信息的决策助手。

Method: 提出ComplLLM后训练框架，基于决策理论，使用互补信息作为奖励来微调决策助手LLM，使其输出与现有智能体决策互补的信号。

Result: 在涉及领域专家的合成和真实世界任务上验证了ComplLLM，证明该方法能够恢复已知的互补信息，并产生合理的互补信号解释以支持下游决策者。

Conclusion: ComplLLM框架能够有效利用多智能体系统中的互补信息，通过微调LLM产生与现有决策互补的信号，为下游决策提供支持。

Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.

</details>


### [103] [Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark](https://arxiv.org/abs/2602.19502)
*Lalitha Pranathi Pulavarthy,Raajitha Muthyala,Aravind V Kuruvikkattil,Zhenan Yin,Rashmita Kudamala,Saptarshi Purkayastha*

Main category: cs.AI

TL;DR: 人类指导的智能体AI在医疗预测任务中表现优异，通过领域专家在关键决策点的干预，在三个临床预测挑战中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体AI系统在自主数据科学工作流方面能力不断增强，但临床预测任务需要领域专业知识，这是纯自动化方法难以提供的。研究探索如何通过人类指导来改进多模态临床预测。

Method: 人类分析师在关键决策点指导智能体工作流：从临床笔记、扫描PDF账单收据和时间序列生命体征进行多模态特征工程；任务适当的模型选择；以及临床信息验证策略。在AgentDS Healthcare基准的三个挑战中应用该方法。

Result: 在30天医院再入院预测中达到Macro-F1=0.8986，急诊科成本预测中MAE=$465.13，出院准备评估中Macro-F1=0.7939。总体排名第5，出院准备任务排名第3。消融研究显示人类指导决策累计提升+0.065 F1，其中多模态特征提取贡献最大单次改进(+0.041 F1)。

Conclusion: 总结三个可推广的经验：1)各阶段领域信息特征工程产生复合增益，优于广泛的自动化搜索；2)多模态数据集成需要任务特定的人类判断；3)具有临床动机模型配置的刻意集成多样性优于随机超参数搜索。为在医疗环境中部署智能体AI提供实用指导。

Abstract: Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.

</details>


### [104] [Classroom Final Exam: An Instructor-Tested Reasoning Benchmark](https://arxiv.org/abs/2602.19517)
*Chongyang Gao,Diji Yang,Shuyan Zhou,Xichen Yan,Luchuan Song,Shuo Li,Kezhen Chen*

Main category: cs.AI

TL;DR: CFE是一个多模态基准测试，用于评估大语言模型在20多个STEM领域的推理能力，基于真实大学作业和考试题目，前沿模型准确率约60%，仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估大语言模型复杂推理能力方面存在不足，特别是针对STEM领域的多步骤问题解决能力。需要基于真实教育场景的评估工具来准确衡量模型的实际推理性能。

Method: 从大学课程中收集重复使用的真实作业和考试题目，由课程教师提供参考答案。将参考答案分解为推理流程，分析模型在中间状态推导和维护方面的表现，并比较模型生成解与教师解在推理步骤数量上的差异。

Result: Gemini-3.1-pro-preview总体准确率为59.69%，Gemini-3-flash-preview为55.46%。诊断分析显示，前沿模型虽然能正确回答中间子问题，但在多步骤解中可靠推导和维护正确中间状态方面存在困难。模型生成的解通常比教师解有更多推理步骤，表明步骤效率低下且错误积累风险更高。

Conclusion: CFE基准测试揭示了当前大语言模型在复杂STEM推理任务中的显著局限性，特别是在多步骤问题解决和中间状态管理方面。该基准为模型推理能力的系统性评估提供了有价值的工具，并指出了未来改进的重要方向。

Abstract: We introduce \CFE{} (\textbf{C}lassroom \textbf{F}inal \textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.

</details>


### [105] [Ada-RS: Adaptive Rejection Sampling for Selective Thinking](https://arxiv.org/abs/2602.19519)
*Yirou Ge,Yixi Li,Alec Chiu,Shivani Shekhar,Zijie Pan,Avinash Thangali,Yun-Shiuan Chuang,Chaitanya Kulkarni,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: Ada-RS是一种算法无关的样本过滤框架，通过自适应长度惩罚奖励对多个采样完成进行评分，然后应用随机拒绝采样来保留高奖励候选，从而学习选择性高效推理，在保持工具调用准确性的同时显著减少输出令牌和思考率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在成本和延迟敏感的环境中部署日益增多。虽然思维链能改善推理，但在简单请求上会浪费令牌。需要研究工具使用LLM的选择性思维，以实现高效推理。

Method: 提出自适应拒绝采样(Ada-RS)框架：对每个给定上下文，使用自适应长度惩罚奖励对多个采样完成进行评分，然后应用随机拒绝采样来保留高奖励候选（或偏好对）用于下游优化。Ada-RS可集成到偏好对（如DPO）或分组策略优化（如DAPO）策略中。

Result: 在合成工具调用导向的电子商务基准测试中，使用Qwen3-8B和LoRA，Ada-RS相比标准算法改善了准确性-效率边界：平均输出令牌减少高达80%，思考率降低高达95%，同时保持或提高工具调用准确性。

Conclusion: 训练信号选择是延迟敏感部署中高效推理的强大杠杆。Ada-RS展示了通过选择性思维优化推理效率的有效性。

Abstract: Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.

</details>


### [106] [A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data](https://arxiv.org/abs/2602.19562)
*Joseph Bingham*

Main category: cs.AI

TL;DR: 提出一个计算框架，通过整合语言表达与大规模众包图像衍生的感知表征，模拟人类指称解释的核心方面，在斯坦福重复指称游戏语料库上表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 建立自然语言表达与视觉感知之间的稳定映射是认知科学和人工智能的基础问题。人类能够在嘈杂、模糊的感知环境中理解语言指称，但其背后的跨模态对齐机制尚不清楚。

Method: 结合尺度不变特征变换（SIFT）对齐和通用质量指数（UQI）来量化认知合理特征空间中的相似性，模拟人类感知分类；同时使用语言预处理和查询转换操作捕捉指称表达中的语用变异性。

Result: 在斯坦福重复指称游戏语料库（15,000个与七巧板刺激配对的语句）上，该框架达到稳健的指称接地。相比人类对话者，它需要少65%的语句来建立稳定映射，并能从单个指称表达中正确识别目标对象，准确率达41.66%（人类为20%）。

Conclusion: 相对简单的感知-语言对齐机制可以在经典认知基准测试中产生与人类竞争的行为，为接地通信、感知推理和跨模态概念形成模型提供了见解。

Abstract: Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\% of the time (versus 20\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .

</details>


### [107] [Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent](https://arxiv.org/abs/2602.19837)
*Björn Hoppmann,Christoph Scholz*

Main category: cs.AI

TL;DR: 该论文是一篇关于元学习和元强化学习的综述性文章，系统性地形式化了这些概念，并梳理了从早期算法到DeepMind自适应智能体的发展历程。


<details>
  <summary>Details</summary>
Motivation: 人类能够有效利用先验知识快速适应新任务，而标准机器学习模型需要任务特定训练。元学习旨在克服这一限制，让模型从多个任务中获取可迁移知识，实现小样本快速适应。

Method: 采用任务驱动的形式化方法对元学习和元强化学习进行系统定义，并以此框架回顾了该领域的关键算法发展路径，最终整合了理解自适应智能体等通用方法所需的核心概念。

Result: 构建了元学习的统一理论框架，系统梳理了从基础元学习算法到DeepMind自适应智能体的演进历程，为理解通用智能体方法提供了概念基础。

Conclusion: 元学习是实现通用人工智能的重要途径，通过任务驱动的形式化分析和算法发展脉络梳理，为后续研究提供了理论基础和概念框架，特别是对理解自适应智能体等通用方法具有重要意义。

Abstract: Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.

</details>


### [108] [Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning](https://arxiv.org/abs/2602.19914)
*Thatchawin Leelawat,Lewis D Griffin*

Main category: cs.AI

TL;DR: 将Watson & Holmes侦探桌游改编为AI推理基准，评估模型在渐进叙事证据、开放式问题和无约束回答下的表现，发现AI性能在9个月内从人类下四分位数提升至前5%


<details>
  <summary>Details</summary>
Motivation: 现有AI推理基准难以评估模型在自然情境下与人类推理的相似程度，需要开发更贴近真实推理过程的评估方法

Method: 改编Watson & Holmes侦探桌游作为新基准，包含渐进呈现的叙事证据、开放式问题和无约束语言回答；开发自动化评分系统并与人工评估验证，确保可扩展性和可重复性

Result: AI模型性能在2025年9个月内从人类比较组的下四分位数提升至约前5%；约一半改进来自连续模型发布的稳步进步，另一半来自推理导向架构的显著跃升；模型在解决较长案件（1900-4000词）时性能下降，推理模型在证据稀缺的早期阶段具有归纳推理优势

Conclusion: 该基准有效评估AI推理能力，显示AI性能快速接近人类水平，但存在案件长度相关的性能差异；推理导向架构带来显著改进，自动化评分系统为可扩展评估提供支持

Abstract: Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.

</details>


### [109] [Latent Introspection: Models Can Detect Prior Concept Injections](https://arxiv.org/abs/2602.20031)
*Theia Pearson-Vogel,Martin Vanek,Raymond Douglas,Jan Kulveit*

Main category: cs.AI

TL;DR: Qwen 32B模型具有潜在的内省能力，能够检测上下文中的概念注入并识别具体概念，这种能力可通过提示增强


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否存在内省能力，即模型能否意识到其上下文中的概念注入，以及这种能力是否可通过特定提示增强，这对理解模型的潜在推理机制和安全影响具有重要意义

Method: 使用Qwen 32B模型进行概念注入实验，通过logit lens分析残差流中的检测信号，并设计提示策略来增强模型的内省敏感性

Result: 模型能够检测概念注入（logit lens显示明显信号），但输出中否认；准确提示可将检测敏感性从0.3%大幅提升至39.2%，误报仅增加0.6%；注入与恢复概念间的互信息从0.62比特增至1.05比特

Conclusion: 模型具有被忽视的内省和转向意识能力，这种潜在能力对潜在推理和安全有重要影响，可通过适当提示显著增强

Abstract: We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.

</details>


### [110] [CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching](https://arxiv.org/abs/2602.20094)
*Yuzhe Wang,Yaochen Zhu,Jundong Li*

Main category: cs.AI

TL;DR: 提出CausalFlip基准测试，通过构造语义相似但因果答案相反的问题对，评估LLM是否真正基于因果而非语义相关性进行推理。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂高风险决策场景中部署增多，需要确保其推理基于因果关系而非虚假相关性。传统推理基准的高性能可能源于语义模式记忆而非真正的因果结构分析，存在评估盲区。

Method: 构建CausalFlip基准：1) 基于事件三元组构建可形成混杂、链式和碰撞关系的因果判断问题；2) 为每个三元组构造语义相似但因果答案相反的问题对；3) 引入噪声前缀评估，在中间推理步骤前添加因果无关文本；4) 评估多种训练范式：仅答案训练、显式思维链监督、内部化因果推理方法。

Result: 显式思维链仍可能被虚假语义相关性误导，而内部化推理步骤能显著改善因果基础，表明更好地激发基础LLM的潜在因果推理能力具有前景。

Conclusion: 需要新的LLM范式或训练算法来确保推理基于因果关系而非语义相关性。内部化因果推理方法能减少对相关性的显式依赖，是改善LLM因果推理能力的有前景方向。

Abstract: As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.

</details>


### [111] [Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration](https://arxiv.org/abs/2602.20104)
*Hasan Amin,Ming Yin,Rajiv Khanna*

Main category: cs.AI

TL;DR: 本文提出一种自适应AI集成系统，通过交替使用对齐模型和互补模型来解决人类-AI决策中性能提升与信任建立之间的根本矛盾，显著提高人机协作性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法训练单一AI模型协助人类决策存在根本局限性：互补性AI虽然能提升性能但会降低人类信任，而对齐性AI虽然能建立信任但会强化次优人类行为。这种性能提升与信任建立之间的张力限制了人机协作效果。

Method: 提出一种以人为中心的自适应AI集成系统，通过Rational Routing Shortcut机制基于上下文线索在两个专家模型间切换：对齐模型（建立信任）和互补模型（提升性能）。该机制被证明是接近最优的简单策略。

Result: 理论分析阐明了自适应AI集成的有效性及其最大效益条件。在模拟和真实数据实验中，使用自适应AI集成的人类决策者相比使用单一AI模型（无论是优化独立性能还是人机团队性能）获得了显著更高的性能。

Conclusion: 自适应AI集成通过动态切换对齐和互补模型，有效解决了人机协作中性能与信任的根本矛盾，为设计更有效的人类-AI协作系统提供了新范式。

Abstract: In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.

</details>


### [112] [ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models](https://arxiv.org/abs/2602.20117)
*Andre He,Nathaniel Weir,Kaj Bostrom,Allen Nie,Darion Cassel,Sam Bayless,Huzefa Rangwala*

Main category: cs.AI

TL;DR: ReSyn是一个生成多样化推理环境（含实例生成器和验证器）的管道，用于扩展可验证奖励的强化学习，训练推理语言模型，在多个基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：合成数据生成方法仍以解决方案为中心，而基于验证器的方法依赖少量手工制作的过程环境。需要扩展可验证奖励的强化学习（RLVR）方法，通过大规模生成多样化的推理环境来提升推理语言模型的训练效果。

Method: 提出ReSyn管道，自动生成多样化的推理环境，包括约束满足、算法谜题和空间推理等任务。每个环境都配备实例生成器和验证器。使用Qwen2.5-7B-Instruct模型在这些生成的环境上进行强化学习训练。

Result: 训练后的模型在推理基准测试和领域外数学基准测试中均取得一致提升，特别是在具有挑战性的BBEH基准上获得27%的相对改进。消融实验表明验证器监督和任务多样性都对性能提升有显著贡献。

Conclusion: 大规模生成推理环境能够有效增强推理语言模型的推理能力，验证了可验证奖励的强化学习方法的可扩展性，为训练更强大的推理模型提供了新途径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs

</details>
